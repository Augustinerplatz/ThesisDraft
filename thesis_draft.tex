\documentclass[honours]{UNSWthesis}
\linespread{1}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{latexsym,amsmath}
\usepackage{graphicx}

%% define some macros
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\G}{\mathcal{G}}
%\newcommand{\H}{\mathcal{H}}
\newcommand{\g}{\mathfrak{g}}


%% new environments

\newcounter{Item}[section]
%\newenvironment{proof}{\noindent {\bf Proof.}\ }{\qed}
\newenvironment{Definition}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Definition \thesection.\theItem.}\ }
                           {\medskip}
\newenvironment{Notation}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Notation \thesection.\theItem.}\ }
                           {\medskip}
\newenvironment{Theorem}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Theorem \thesection.\theItem.}\ %
                            \begingroup \sl}
                           {\endgroup\medskip}
\newenvironment{Proposition}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Proposition \thesection.\theItem.}\ %
                            \begingroup \sl}
                           {\endgroup\medskip}
\newenvironment{Corollary}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Corollary \thesection.\theItem.}\ %
                            \begingroup \sl}
                           {\endgroup\medskip}
\newenvironment{Lemma}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Lemma \thesection.\theItem.}\ %
                            \begingroup \sl}
                           {\endgroup\medskip}
\newenvironment{Conjecture}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Conjecture \thesection.\theItem.}\ %
                            \begingroup \sl}
                           {\endgroup\medskip}
\newenvironment{Example}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Example \thesection.\theItem.}\ }
                           {\qed}
\newenvironment{Remark}{\smallskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Remark \thesection.\theItem.}\ }
                           {\qed}
\newenvironment{Question}{\smallskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Question \thesection.\theItem.}\ }
                           {\par}
\newenvironment{theoremlist}{\begin{list}{}
                        {\setlength{\parsep}{0pt}
                        \setlength{\topsep}{\smallskipamount}} }
                        {\end{list}}

\title{Rigidity of Coset-preserving Maps on Nilpotent Lie Groups}

\authornameonly{Richard Tierney}

\author{\Authornameonly\\{\bigskip}Supervisor: Professor Michael Cowling}

\begin{document}
\maketitle

\prefacesection{Acknowledgements}
{\noindent}Many thanks go to Michael Cowling, Georgia Tsambos, Andrew Ardill...

\prefacesection{Introduction}

Lie Groups are to be found in many different applications of mathematics as well as being central to many of the ideas in quantum physics. It is therefore very useful to know whether different Lie groups linked by a certain map are similar in structure. This is the idea of a rigidity theorem. If there is a map between two nilpotent Lie groups that preserves the cosets of subgroups (structures arising from the group law on a Lie group), just how similar are the two Lie groups? In this paper, the extra assumption of bijectivity will help to show the kind of similarity that exists. If a map is bijective then no information is 'lost' about the structures of the domain and codomain, when either the map or its inverse is applied. 
This thesis is an attempt to demonstrate rigorously the extension of the rigidity theorem for the Heisenberg Group to all the nilpotent Lie groups. The proof involves arguments from a geometrical as well as algebraic perspective. 

\chapter{Lie Groups, Lie Algebras and Homomorphisms}
This chapter outlines the basics of the theory of Lie Groups and their relationship to Lie Algebras. It gives some examples of matrix Lie groups including the Heisenberg Group, and explains the use of the Baker-Campbell-Hausdorff Formula in transferring between a Lie Group and its corresponding Lie Algebra. 

\section{Introduction}
The theory of Lie Groups has evolved from the combination of several different disciplines in mathematics. Lie Group Theory combines the following ingredients: Group Theory from Algebra, Manifold Theory from Differential Geometry, and basic ideas from Topology. All of the Lie Groups mentioned in this paper will be matrix Lie groups. That is, they are matrix groups that are also $C^2$-manifolds. The idea of a rigidity theory in this context is to use structures that arise from the group law within the manifold to show that a weak similarity of these matrix Lie groups implies a strong similarity between them. This paper will prove the theorem for nilpotent matrix Lie groups (and solvable matrix Lie groups??).

\section{Lie Groups}
\begin{Definition}\label{Lie Group}
A Lie group $\G $ is a $\mathrm{C}^{\infty}$ manifold which is also a group, for which the group operations of multiplication and taking inverses are continuous. ie the maps

\begin{eqnarray*}
\sigma : & \G \times \G & \longrightarrow \G  \\
& (g,h) & \longmapsto gh
\end{eqnarray*}
and

\begin{eqnarray*}
\tau : & \G  & \longrightarrow \G  \\
& g & \longmapsto g^{-1}
\end{eqnarray*}

are continuous with respect to the topology on $\G$.
\end{Definition}

\begin{Example}\label{Rn}
$(\R^{n}, +)$ is a Lie group with one single coordinate patch defined by the identity map on $\R^{n}$. The usual addition, and inversion given by multiplication by $(-1)$ are continuous maps.

\end{Example}

\begin{Example}\label{Circle}
The circle $\mathrm{S}^{1} = \{ e^{i\theta}\; \big| 0 \leq \theta < 2\pi \}$ is a Lie group. As a manifold, $\mathrm{S}^{1}$ has a single coordinate patch given by:

\begin{eqnarray*}
p_{1} : & \mathrm{S}^{1}  & \longrightarrow \R  \\
& e^{i\theta} & \longmapsto \theta
\end{eqnarray*}

The group multiplication on $\mathrm{S}^{1}$ is given by 
\[ (e^{i\theta}, e^{i\phi}) \longmapsto e^{i\theta}e^{i\phi} = e^{i(\theta + \phi)} \]
\end{Example}

An important aspect of this particular coordinate patch (even though in this case only one is required), is that it is 
also the tangent space of the manifold at the identity element of the group. [add picture]. In general this object is 
very useful for the study of Lie groups and takes on a structure of its own called a Lie algebra. 

%\begin{figure}
%\includegraphics{imagename.eps}
%\caption{Lovely caption}
%\label{fig:somethin-you-will-remember}
%\end{figure}

\begin{Definition}\label{Matrix Lie Group}
A matrix Lie group is any subgroup $\G$ of $\mathrm{GL}(n,\C)$ with the property that if there is a sequence $A_{m}$ of
matrices in $\G$ that converges to some matrix $A \in \mathrm{M}(n,\C)$ then either $A \in \G $ or $A$ is not invertible.
This is equivalent to $G$ being a closed subset of $\mathrm{GL}(n,\C)$.
\end{Definition}

\subsection*{Examples}
\[\mathrm{SL}(n,\R), \mathrm{O}(n), \mathrm{SO}(n), \mathrm{SU}(2) \]


\begin{Example}\label{Heisenberg Group}.
The Heisenberg group, defined as follows, is a Lie group. This example forms the cornerstone of the result in this thesis.
\paragraph
{\noindent}The Heisenberg group is the set of real three-tuples with non-commutative multiplication given by
\[
(x,y,z). (x', y', z') := (x+x', y+y', z+z' + xy')
\]
Inversion is given by 
\[ (x,y,z)^{-1}= (-x, -y, -z+xy) \]
As a matrix Lie group, the Heisenberg group can be represented as the group of all $ 3\times 3 $ upper triangular real unipotent
matrices:
\[
 \G= \left\{ \begin{bmatrix} 1 & x & z \\ 0 & 1 & y \\ 0 & 0 & 1 \end{bmatrix} \bigg| x,y, z \in \R \right\} 
\]
\end{Example}

Many of the important examples of matrix Lie groups are noncommutative groups. This makes the study of their structure in many 
cases fairly complex. However, as with the example of the circle, studying the Lie group can be made simpler by studying what is called its Lie algebra. The structure of the Lie algebra determines the local structure of its Lie group.


\section{Lie Algebras}
\begin{Definition}\label{Lie Algebra}
A Lie algebra is a vector space $\mathcal{A}$ endowed with a map
\[ [\cdotp,\cdotp]:\; \mathcal{A} \times \mathcal{A} \longrightarrow \mathcal{A} \]
with the following properties:
\begin{enumerate}
 \item $[\cdotp, \cdotp]$ is bilinear
 \item $[X,Y]=-[Y,X]$ for all $X$, $Y$ $\in \mathcal{A}$
 \item $[X,[Y,Z]]+[Y,[Z,X]]+[Z,[X,Y]]=0$ for all $X$, $Y$, $Z$ $\in \mathcal{A}$
\end{enumerate}
This map is often called the \emph{Lie bracket} or the \emph{bracket operation}.
\end{Definition}

The Lie algebra $\g$ of a Lie group $\G$ is the tangent space to $G$ at the identity $e$. This can be seen clearly in 
the example of the circle. The circle is parametrised as before by a the curve
\[ g: [0,2\pi) \longrightarrow \C \]
\[ g(\theta)= e^{i\theta}\]
Differentiating gives
\[g' (\theta)= ie^{i\theta}\]
At the identity, $1=e^{0}$,
\[ g' (0) = i \]
so the tangent space $s$ is the real vector space with basis $\{i\}$. 
In this case the operation $[\cdotp, \cdotp]$ is trivial, as with the Lie algebra of any abelian Lie group.
Without going into rigorous detail, it can be seen that the group operation on $S^1$ corresponds to addition of 
vectors in $s$. If corresponding elements are taken very close to the identity the results of these two operations 
come very close together. (more detail of manifold theory???)
(picture required)

In the case of matrix Lie groups, tangent space at the identity takes a very clear form. The relationship between a 
matrix Lie group and its Lie algebra is determined by the exponential mapping on matrices. 

\begin{Definition}\label{Lie Algebra of a Matrix Lie Group}
The Lie algebra of an $n \times n$ matrix Lie group $\G$ is
\[
\g = \{ X \in M_{n}(\C) \big| \exp{X} \in \G \}
\]
The bracket operation on $\g$, given by $[X,Y]=XY-YX$, satisfies the axioms for $\g$ to be a Lie algebra. 
\end{Definition}

(Propose that this definition really does correspond to the tangent bundle at the identity... proof??)

Note that the exponential map from $\g$ to $\G$ is injective but not necessarily surjective. The image of $\g$
under the exponential map is precisely the connected component of $\G$ containing the identity $e$. (proof). In the case that $\G$ is connected, the inverse of the map $\exp : \g \longrightarrow \G $ is called the matrix logarithm map, $\log : \G \longrightarrow \g $.

\begin{Proposition}\label{Lie algebra of Heisenberg group}
The Lie algebra of the Heisenberg group, $$\G = \left\{ \begin{bmatrix} 1 & x & z \\ 0 & 1 & y \\ 0 & 0 & 1 \end{bmatrix} \bigg| x,y, z \in \R \right\}$$ is 
\[
\g= \left\{ \begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix} \bigg| x,y,z \in \R \right\}
\]
and the exponential map $\exp$: $\g \longrightarrow \G$ is a bijection.
\end{Proposition}

\begin{proof}
It will be shown that $im(\exp) \in \G$, $\G \in im(\exp)$ and that $\exp$ is injective. 
Let $x$, $y$ and $z$ be real numbers and note the following: 
\[
\begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix}^2=\begin{bmatrix} 0 & 0 & xy \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}
\]
\[
\begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix}^3=\begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}
\]
Now,
\begin{eqnarray*}
 \exp \begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix} &=& \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} + \begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix}+ \frac{1}{2}\begin{bmatrix} 0 & 0 & xy \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} \\
&=& \begin{bmatrix} 1 & x & z+\frac{1}{2}xy \\ 0 & 1 & y \\ 0 & 0 & 1 \end{bmatrix} \\
&\in & \G
\end{eqnarray*} so $ im(\exp) \in \G$.
Now suppose $a,b,c \in \R$ so $\begin{bmatrix} 1 & a & c \\ 0 & 1 & b \\ 0 & 0 & 1 \end{bmatrix} \in \G$. Then 
\[ 
\exp  \begin{bmatrix} 0 & a & c-\frac{1}{2}ab \\ 0 & 0 & b \\ 0 & 0 & 0 \end{bmatrix} = \begin{bmatrix} 1 & a & c \\ 0 & 1 & b \\ 0 & 0 & 1 \end{bmatrix}
\]
so $\G \in im(\exp)$.
\bigskip
Suppose that $A=\begin{bmatrix} 0 & a & c \\ 0 & 0 & b \\ 0 & 0 & 0 \end{bmatrix}$ and $A'=\begin{bmatrix} 0 & a' & c' \\ 0 & 0 & b' \\ 0 & 0 & 0 \end{bmatrix}$ are in $\g$ and that $\exp{A}=\exp{A'}$. Then 
\[
\begin{bmatrix} 0 & a & c+\frac{1}{2}ab \\ 0 & 0 & b \\ 0 & 0 & 0 \end{bmatrix} =\begin{bmatrix} 0 & a' & c' +\frac{1}{2}a' b' \\ 0 & 0 & b' \\ 0 & 0 & 0 \end{bmatrix}
\] 
and hence
\begin{eqnarray*}
a &=& a' \\
b &=& b' \\
c &=& c'
\end{eqnarray*}
so $A=A'$.
\end{proof}

\subsection*{Notation}
For the purpose of simplifying the notation of matrices in the Heisenberg group and in its Lie algebra, vector notation will sometimes be used. As defined above, an element of the Heisenberg group $\G$ will be denoted either by $ \begin{bmatrix} 1 & a & c \\ 0 & 1 & b \\ 0 & 0 & 1 \end{bmatrix} $ or equivalently $[a,b,c]$. Similarly, elements of the Lie algebra of the Heisenberg group will be denoted either by $\begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix}$ or $(x,y,z)$, or in some cases $\begin{bmatrix} x \\ y \\ z \end{bmatrix}$.

In this notation, the bracket operation on two elements of $\g$ is given by 
\begin{eqnarray*}
[(x,y,z), (x',y',z')] &=& \begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & x' & z' \\ 0 & 0 & y' \\ 0 & 0 & 0 \end{bmatrix} - \begin{bmatrix} 0 & x' & z' \\ 0 & 0 & y' \\ 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix} \\
&=& \begin{bmatrix} 0 & 0 & xy' \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} -\begin{bmatrix} 0 & 0 & x'y \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} \\
&=& (0,0,xy'-x'y)
\end{eqnarray*}

\section{Subgroups and Subalgebras}

\begin{Definition}\label{Lie subgroup}
Let $\G$ be a Lie group. A subset $\mathcal{H}$ of $\G$ is called a Lie subgroup if it is a subgroup and also a Lie group.
\end{Definition}

\begin{Definition}\label{Lie subalgebra}

N.B. In this paper, the term "subalgebra" will be interchangeable with "Lie subalgebra"
\end{Definition}

\section{Homomorphisms}

\begin{Definition}\label{Lie Group Homomorphism}
Let $\G$ and $\mathcal{H}$ be Lie groups and $f:\G \longrightarrow \mathcal{H} $ be a function. $f$ is called a Lie group homomorphism if and only if 
\[ f(xy)=f(x)f(y) \; \; \; \text{(i.e. $f$ is a group homomorphism)} \]
and $f$ is continuous with respect to the topologies on $\G$ and $\mathcal{H}$. 

\end{Definition}

\begin{Example}\label{canonical quotient of centre}
Let $\G$ be the Heisenberg group. Let 
\begin{eqnarray*}
\pi : &\G& \longrightarrow (\R^{2},+) \\
&[a,b,c]& \longmapsto (a,b)
\end{eqnarray*}

Then 

\begin{eqnarray*}
\pi([a,b,c][a',b',c']) &=& \pi[a+a',b+b',c+c'+ab'] \\
&=&(a+a',b+b') \\
&=& (a,b)+(a',b') \\
&=& \pi[a,b,c] + \pi[a',b',c']
\end{eqnarray*}

So $\pi$ is a group homomorphism. Also, since the topology on $\G$ is inherited from the topology on $\R^{3}$, $\pi$ acts topologically as a projection from $\R^{3}$ onto $\R^{2}$ so is continuous. Hence $\pi$ is a Lie group homomorphism.
\end{Example}

\begin{Definition}\label{Lie Algebra Homomorphism}
Let $\g$ and $\mathfrak{h}$ be Lie algebras. A linear transformation $T: \g \longrightarrow \mathfrak{h}$ is a Lie algebra homomorphism if in addition to the linearity properties, $T$ preserves the Lie bracket, i.e. 
\[
T[A,B] = [T(A), T(B)] \; \;\;\;\;\;\;\;\; \forall A,B \in \g
\]
\end{Definition}

\begin{Theorem}\label{Homomorphisms Correspondence}
Let $\G$ and $\mathcal{H}$ be matrix Lie groups with corresponding Lie algebras $\g$ and $\mathfrak{h}$ respectively. Suppose that $\Phi: \G \longrightarrow \mathcal{H}$ is a Lie group homomorphism. Then there exists a unique Lie algebra homomorphism $\phi: \g \longrightarrow \mathfrak{h}$ such that 
\[
\Phi(e^{X}) = e^{\phi(X)} \;\;\;\;\;\;\; \forall X \in \g
\]

Now let $\G$ and $\mathcal{H}$ be matrix Lie groups, $\g$ and $\mathfrak{h}$ be their Lie algebras and let $\G$ be connected. Then if $\phi: \g \longrightarrow \mathfrak{h}$ is a Lie algebra homomorphism, there exists a unique Lie group homomorphism $\Phi: \G \longrightarrow \mathcal{H}$ defined by
\[
\Phi(A) = e^{\phi(\log{A})} \;\;\;\;\;\;\; \forall A \in \G
\]
\end{Theorem}

\begin{Example}
Let $\G$ be the Heisenberg group and $\g$ its Lie algebra. Let $\mathcal{H}=\mathrm{GL}(1,\R)$. The Lie algebra of $\mathcal{H}$ is $\mathfrak{h}=\mathrm{M}_{1}(\R)$. Define 
\begin{eqnarray*}
\Phi :& \G &\longrightarrow  \mathcal{H} \\
&[x,y,z] &\longmapsto  e^{x}
\end{eqnarray*}
This is a Lie group homomorphism. It is continuous since the map $x \mapsto e^{x}$ is continuous and 

\begin{eqnarray*}
\Phi ([x,y,z][x',y',z']) &=& \Phi [x+x',y+y', z+z'+xy'] \\
&=& e^{x+x'} \\
&=& e^{x}e^{x'} \\
&=& \Phi [x,y,z] \Phi [x',y',z']
\end{eqnarray*}

Now
\[
\Phi [x,y,z] = \Phi (\exp{(x,y,z-\frac{1}{2}xy))}=e^{x}
\]
So there is a unique map $\phi: \g \longrightarrow \mathfrak{h}$ defined by $\phi(x,y,z)=x$ such that
\[
\Phi(e^{X}) = e^{\phi(X)} \;\;\;\;\;\;\;\; \forall X \in \g
\]
 It is a linear map of vector spaces since it projects onto one of the dimensions. Also for $(x,y,z)$ and $(x',y',z')$ in $\g$, 
\[ \phi [(x,y,z), (x',y',z')]=\phi (0,0,xy'-x'y)=0
\]
and
\begin{eqnarray*}
[\phi(x,y,z),\phi(x',y',z')]=[x,x']=xx'-x'x &=& 0 \\
&=&  \phi [(x,y,z), (x',y',z')]
\end{eqnarray*}
Hence $\phi$ is a Lie algebra homomorphism.
\end{Example}

\section{The Baker-Campbell-Hausdorff Formula}

Since we wish to simplify the study of Lie group homomorphisms by studying properties of the corresponding Lie algebra homomorphisms, we need a way to relate the operations on these objects directly. In the case of the unit circle $\mathrm{S}^{1}$, the properties of the exponential function on real numbers give the simple correspondence between addition on the real line, and multiplication of elements of the unit circle in the complex plane. For noncommutative groups this simple relationship does not hold. However, there is a more complicated way of relating the structure of Lie algebras to their corresponding Lie groups. 

\paragraph{Heisenberg Group}
The Lie algebra of the Heisenberg group has noncommutative multiplication, however for any $X$, $Y$ $\in \g$, $X$ and $Y$ each commutes with $[X,Y]$. In other words
\[
[X,[X,Y]]=[Y,[X,Y]]=0
\]
\begin{proof}
Let $X=(x,y,z)$ and $Y=(x',y',z')$ be elements of $\g$. 
\begin{eqnarray*}
[X,[X,Y]] &=& [X, (0,0,xy'-x'y)] \\
&=&[(x,y,z),(0,0,xy'-x'y)] \\
&=& 0
\end{eqnarray*}

and
\begin{eqnarray*}
[Y,[X,Y]] &=& [Y, (0,0,xy'-x'y)] \\
&=&[(x',y',z'),(0,0,xy'-x'y)] \\
&=& 0
\end{eqnarray*}
\end{proof}

We require some tools in order to explain the formula for the Heisenberg group. 

\begin{Definition}\label{Two Adjoint Mappings}
Let $X$ and $Y$ be elements of $\mathrm{M}_{n}(\C)$. Define the following maps: 
\begin{eqnarray*}
\mathrm{Ad}_{Y}:& \mathrm{M}_{n}(\C) & \longrightarrow \mathrm{M}_{n}(\C) \\
&X& \longmapsto YXY^{-1}
\end{eqnarray*}

and
\begin{eqnarray*}
\mathrm{ad}_{Y}:& \mathrm{M}_{n}(\C) & \longrightarrow \mathrm{M}_{n}(\C) \\
&X& \longmapsto [Y,X]=YX - XY
\end{eqnarray*}

\end{Definition}

\begin{Definition}
Define also the operator $e^{\mathrm{ad}_{Y}}$ given by the following power series:
\[
e^{\mathrm{ad}_{Y}}(X)= \mathrm{id}(X) + \mathrm{ad}_{Y}(X) + \frac{1}{2!}( \mathrm{ad}_{Y}^{2})(X) + \frac{1}{3!}( \mathrm{ad}_{Y}^{3})(X) + \cdots
\]
\end{Definition}

\begin{Lemma}
\[
\mathrm{Ad}_{e^{Y}}(X)=e^{\mathrm{ad}_{Y}}(X)
\]
\end{Lemma}

\begin{Theorem}
Let $\G$ be a matrix Lie group with Lie algebra $\g$. If the commutivity property above holds in $\g$ then for $X$ and $Y$ in $\g$, 
\[
e^{X}e^{Y}=e^{X+Y+\frac{1}{2}[X,Y]}
\]
\end{Theorem}

\begin{proof}
To prove the formula, it will be shown that two apparently differential functions are both the unique solution to a certain differential equation, and therefore must be identical. 
Let $X$ and $Y$ be elements of $\g$. Consider the differential equations in the real variable $t$ defined by 
\begin{eqnarray*}
A(t)&=& e^{tX}e^{tY}e^{-\frac{t^{2}}{2}[X,Y]} \\
B(t) &=& e^{t(X+Y)}
\end{eqnarray*}

\begin{Lemma}\label{commute matrices}
Let $M$ and $N$ be $n \times n$ matrices. If $M$ commutes with $N$, then $M$ also commutes with $e^{N}$. 
\end{Lemma}
 

\begin{proof}
Suppose $MN^{k}=N^{k}M$ for some positive integer $k$. Then 
\begin{eqnarray*}
MN^{k+1} &=& MN^{k}N \\
&=& N^{k}MN \\
&=& N^{k}NM \\
&=& N^{k+1}M
\end{eqnarray*}
So since $MN=NM$, the mathematical induction hypothesis indicates that $MN^{K}=N^{K}M$ for all positive integers $K$.
Now, 
\begin{eqnarray*}
Me^{N} &=& M(I+N+\frac{N^{2}}{2!}+\frac{N^3}{3!}+\cdots) \\
&=& M+MN+\frac{1}{2!}MN^{2}+\frac{1}{3!}MN^{3} + \cdots \\
&=& M+NM+\frac{1}{2!}N^{2}M+\frac{1}{3!}N^{3}M + \cdots \\
&=& (I+N+\frac{N^{2}}{2!}+\frac{N^3}{3!}+\cdots)M \\
&=& e^{N}M
\end{eqnarray*}
\end{proof}
\begin{Lemma}
\[
Xe^{tY}=e^{tY}(X+t[X,Y])
\]
\end{Lemma}
\begin{proof}
\begin{eqnarray*}
Xe^{tY} &=& e^{tY}e^{-tY}Xe^{tY} \\
&=& e^{tY}\mathrm{Ad}_{e^{-tY}}(X) \\
&=& e^{tY}e^{-t\mathrm{ad}_{Y}}(X) \\
&=& e^{tY}(X-t[Y,X]+\frac{t^{2}}{2}[Y,[Y,X]]-\frac{t^{3}}{3!}[Y,[Y,[Y,X]]]+\cdots \\
&=& e^{tY}(X - t[Y,X])\\
&=& e^{tY}(X + t[X,Y])
\end{eqnarray*}
since $[Y,[Y,X]]=0$.
\end{proof}

$A(t)$ and $B(t)$ are both now differentiated as follows:

\begin{eqnarray*}
A'(t) &=& e^{tX}Xe^{tY}e^{-\frac{t^2}{2}[X,Y]}+e^{tX}e^{tY}Ye^{-\frac{t^2}{2}[X,Y]}+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(-t[X,Y]) \\
&=& e^{tX}Xe^{tY}e^{-\frac{t^2}{2}[X,Y]}+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}Y+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(-t[X,Y]) \\
& & (\text{since $Y$ commutes with $[X,Y]$ and by Lemma 1.5.5})\\
&=&e^{tX}e^{tY}(X+t[X,Y])e^{-\frac{t^2}{2}[X,Y]}+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}Y+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(-t[X,Y]) \\
&=& e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(X+t[X,Y])+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}Y+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(-t[X,Y]) \\
& & (\text{since $X$ and $[X,Y]$ each commute with $[X,Y]$})\\
&=& e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(X+t[X,Y]+Y-t[X,Y]) \\
&=& e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(X+Y) \\
&=& A(t)(X+Y)
\end{eqnarray*}

\begin{eqnarray*}
B'(t) &=& e^{t(X+Y)}(X+Y) \\
&=& B(t)(X+Y)
\end{eqnarray*}

It is also necessary for the above differential equations to have the same initial conditions if the uniqueness theorem is to be applied. 
\[
A(0)=e^{0}e^{0}e^{0}=I^{3}=I
\]
and 
\[
B(0)=e^{0}=I
\]
So $A(t)$ and $B(t)$ satisfy the same differential equation with the same initial conditions. By the basic uniqueness theorems for differential equations this implies $A(t)=B(t), \forall t \in \R$. Substituting the value $t=1$ gives 
\begin{eqnarray*}
e^{X}e^{Y}e^{-\frac{1}{2}[X,Y]} &=&e^{X+Y} \\
e^{X}e^{Y} &=& e^{X+Y}e^{\frac{1}{2}[X,Y]} \\
&=& e^{X+Y+\frac{1}{2}[X,Y]} \\
& & (\text{since $(X+Y)$ commutes with $\frac{1}{2}[X,Y]$})
\end{eqnarray*}
This completes the proof of the Baker-Campbell-Hausdorff Formula for the Heisenberg group. 
\end{proof}

There is a more general formula for any noncommutative Lie group $\G$, however there is an infinite series of commutator terms, say $S$, such that for all $X$ and $Y$ in $\G$,
\[
e^{X}e^{Y}=e^{X+Y+S}
\]
The series formula will be stated but not proved in this thesis. A full proof is given in ...

\chapter{Case for the Heisenberg Group}
For this chapter the following notation will be used:
\begin{Notation}
$$\G=\text{the Heisenberg group}$$
$$\g=\text{the Lie algebra of the Heisenberg group}$$
\end{Notation}

\section{Aim}
The aim of this chapter is to prove a rigidity theorem for coset-preserving bijective maps on the Heisenberg group. The ideas and methods used in this theorem are foundational for the proof of the theorem for higher dimensional Lie groups.

\begin{Theorem}\label{preserve closed cosets}
Suppose that $\Phi: \G \longrightarrow \G$ is bijective and sends every coset of any Lie subgroup of $\G$ to some coset of some Lie subgroup of $\G$. Then the induced map $\phi: \g \longrightarrow \g$ has the form
\[
\phi(x,y,z)=A
\begin{bmatrix}
x \\ y \\ z
\end{bmatrix}+\bf{b}
\]
where $A=\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & g
\end{bmatrix}$ and $\bf{b} \in \g$
\end{Theorem}

\begin{Theorem}
Suppose that $\Phi: \G \longrightarrow \G$ is bijective and sends every coset of \emph{any} subgroup of $\G$ to some coset of some subgroup of $\G$. Then $\Phi=\tau_{o}\varphi$ where $\varphi:\G \longrightarrow \G$ is either an isomorphism or an anti-isomorphism and $\tau: \G \longrightarrow \G$ is a translation. That is, for any $x \in \G$
\[
\tau(x)=xy
\]
where $y$ is a fixed element of $\G$
\end{Theorem}

\section{Proof that it is an affine map}
\begin{Lemma}
Suppose $\phi: \R^{2} \longrightarrow \R^{2}$ sends $0$ to $0$, sends lines to lines and is bijective. Then $\phi$ is linear. 
\end{Lemma}

\begin{proof}
Construct a map based on the following assumptions. Let $\phi': \R^{2} \longrightarrow \R^{2}$ be linear and
\begin{eqnarray*}
\phi'(\phi(1,0))&=& (1,0)\\
\phi'(\phi(0,1)) &=& (0,1)
\end{eqnarray*}
This map $\phi'$ is well-defined and unique since bijectivity of $\phi$ implies $\phi(1,0) \neq \phi(0,1)$ so every element of $\R^{2}$ can be written as a linear combination of $\phi(1,0)$ and $\phi(0,1)$. 
Now define a map $\psi: \R^{2} \longrightarrow \R^{2}$ by $\psi= \phi'_{o}\phi$
Note that $\psi$ is bijective so sends parallel lines to parallel lines. In particular $\psi$ sends the x-axis to itself and also the y-axis to itself.
A geometrical construction will demonstrate that
\begin{equation}
\psi(x_{1}+x_{2},0)=\psi(x_{1},0)+\psi(x_{2},0)
\end{equation}

and 
\begin{equation}
\psi(x_{1}x_{2},0)=P_{x}(\psi(x_{1},0))P_{x}(\psi(x_{2},0))
\end{equation}


where 
\begin{eqnarray*}
P: &R^{2}& \longrightarrow \R \\
&(x,y)& \longmapsto x
\end{eqnarray*}

Proof of the addition property:
Consider the diagram. Let $P=(1,0)$ and $Q=(0,1)$. $A$ is the point $(x_{1},0)$ and $B$ is the point $(x_{2},0)$. Then construct lines $L_{1}$, $L_{2}$, $L_{3}$ and $L_{4}$ as shown so that $$A'=(0,x_{1})$$ $$B'=(x_{2}, x_{1})$$ and $$C=(x_{1}+x_{2},0)$$. Now the map $\psi$ sends the x and y axes to themselves, sends parallel lines to parallel lines and $\psi(P)=P$ and $\psi(Q)=Q$. So the parallelogram $AA'B'C$ is mapped to a new parallelogram $\psi(A)\psi(A')\psi(B')\psi(C)$ where the sloping sides must have gradient $-1$, since they are parallel to $PQ$. Suppose $\psi(A)=(x_{1}',0)$ and $\psi(B)=(x_{2}',0)$. Then by the properties of the parallelogram, 
\[
\psi(A')=(0,x_{1}')
\]
\[
\psi(B')=(x_{2}',x_{1}')
\]
and 
\[
\psi(C)=(x_{1}'+x_{2}',0)
\]
Hence $\psi(x_{1}+x_{2},0)=\psi(x_{1},0)+\psi(x_{2},0)$.

Now consider this diagram showing the construction of $C=(x_{1}x_{2},0)$ by starting with $A=(x_{1},0)$ and $B=(x_{2},0)$. Construct the line of gradient $-1$ through $A$ so that $A'=(0,x_{1})$. Then construct a horizontal through $A'$ and a vertical through $P$ to meet in $D=(1,x_{1})$. Construct a line $L$ through $O$ and $C$, $y=x_{1}x$ and a vertical line through $B$ to meet in $E=(x_{2}, x_{1}x{2})$. Then construct a horizontal line through $E$ to meet the y-axis in $B'=(0,x_{1}x_{2})$. Construct a line of gradient $-1$ through $B'$ which meets the x-axis at $C$. 

Now consider the same diagram under the map $\psi$. Let $\psi(A)=(x_{1}',0)$ and $\psi(B)=(x_{2}',0) $. Lines of gradient $-1$ are mapped to lines of gradient $-1$ since $P$ and $Q$ are fixed. So 
\[
\psi(A')=(0,x_{1}')
\]
Horizontals are mapped to horizontals and verticals are mapped to verticals so
\[
\psi(D)=(1,x_{1}')
\]
Hence the line $L$ is mapped to the line $\psi(L)$ which is given by $y=x_{1}'x$. Again, the vertical line through $B$ is mapped to a vertical line through $\psi(B)$ so 
\[
E=(x_{2}',x_{1}'x_{2}')
\]
and the by the preservation of horizontals, 
\[
\psi(B)=(0,x_{1}'x_{2}')
\]
and by the preservation of lines of gradient $-1$


\[
\psi(C)=(x_{1}'x_{2}',0)
\]

Hence 
\begin{equation*}
\psi(x_{1}x_{2},0)=P_{x}(\psi(x_{1},0))P_{x}(\psi(x_{2},0))
\end{equation*}

Consider the induced map $f:\R \longrightarrow \R$ defined by
\[
f(x)=P_{x}(\psi(x,0))
\]
Then by the equations (how do I refer to their labels???) above, $f$ is bijective and has the following properties:
\begin{eqnarray*}
f(0) &=& 0 \\
f(1) &=& 1 \\
f(x+y)&=& f(x)+f(y) \\
f(xy) &=& f(x)f(y)
\end{eqnarray*} 
Now let $n \in \Z$. Then 
\begin{eqnarray*}
f(n) &=& f(1+1+ \cdots +1) \\
&=&f(1)+f(1) + \cdots +f(1) \\
&=& 1+1+ \cdots +1 \\
&=& n
\end{eqnarray*}
Also 
\begin{eqnarray*}
1 &=&f(1) \\
 &=& f(n.\frac{1}{n}) \\
&=& f(n)f(\frac{1}{n}) \\
&=& nf(\frac{1}{n})
\end{eqnarray*}
Hence 
\[
f(\frac{1}{n})=\frac{1}{n}
\]
Now let $m$ and $n$ be integers. So 
\begin{eqnarray*}
f(\frac{m}{n})&=&f(m.\frac{1}{n})\\
&=& f(m)f(\frac{1}{n}) \\
&=& m\frac{1}{n} \\
&=& \frac{m}{n}
\end{eqnarray*}
So $f$ restricted to $\Q$ is the identity. In fact, $f$ is continuous because [...] require a proof. Hence since every real number is a limit of a sequence of rational numbers, and $f$ is continuous, $f$ is the identity map $\R \longrightarrow \R$. 

The same properties can be shown for the map $\psi$ on the y-axis. And then since every point in the plane is a projection of point on the x and y axes by horizontal and vertical lines, $\psi(x,y)=(x,y)$ for all $(x,y) \in \R^{2}$. Hence $\phi'_{o}\phi=\psi=\text{id}$ so $\phi=\phi'^{-1}$, which is a linear map. 
\end{proof}

\begin{Definition}\label{affine map}
A map $\Phi: \R^{n} \longrightarrow \R^{n}$ is \emph{affine} if for all $\mathbf{v_{1}},\mathbf{v_{2}} \in \R^{n}$ and all $\lambda \in \R$ 
\[
\Phi [ \lambda \mathbf{v_{1}}+(1-\lambda) \mathbf{v_{2}}]= \lambda \Phi (\mathbf{v_{1}})+(1-\lambda) \Phi(\mathbf{v_{2}})
\]
\end{Definition}

\begin{Proposition}
Let $\Phi: \R^{3} \longrightarrow \R^{3}$ be such that $\Phi$ is bijective, sends lines to lines and vertical planes to vertical planes. Then $\Phi$ is an affine map. 
\end{Proposition}  

\begin{proof}
Let there be a line $L$ in $\R^{3}$ given by $$\lambda \mathbf{v_{1}} + (1-\lambda) \mathbf{v_{2}} $$ where the vector $\mathbf{v_{1}}-\mathbf{v_{2}} \notin \text{span} \{ \mathbf{e_{3}} \}$. 
Now define a vertical plane $\Pi: \lambda \mathbf{v_{1}} + (1-\lambda) \mathbf{v_{2}} + \mu\mathbf{e_{3}}$. $Phi$ sends vertical planes to vertical planes so define the plane $\Sigma = \Phi(\Pi)$. Then 
\[
\mathbf{v_{1}} \in \Pi
\]
\[
\mathbf{v_{2}} \in \Pi
\]
so 
\[
\Phi(\mathbf{v_{1}}) \in \Sigma
\]
and 
\[
\Phi(\mathbf{v_{2}}) \in \Sigma
\]
Hence the line joining these to points is also in $\Sigma$, in other words
\[
\gamma \Phi(\mathbf{v_{1}}) + (1-\gamma)\Phi(\mathbf{v_{2}}) \in \Sigma
\]
Also $\Sigma$ is vertical, and since $\mathbf{v_{1}}-\mathbf{v_{2}} \notin \text{span}\{ \mathbf{e_{3}}\}$, the plane $\Sigma$ is defined by 
\[
\Sigma = \{ \gamma \Phi(\mathbf{v_{1}}) + (1-\gamma)\Phi(\mathbf{v_{2}}) + \delta\mathbf{e_{3}} \; \;|\;\; \gamma,\delta \in \R \}
\]
Now each plane $\Pi$ and $\Sigma$ can be given the structure of a two dimensional vector space in the following sense:
Let $O_{\Pi}=\mathbf{v_{2}}$ and $O_{\Sigma}= \Phi(\mathbf{v_{2}})$. Then elements of $\Pi$ can all be expressed as 
\[
(\alpha_{1}, \alpha_{2})_{\Pi}:= \alpha_{1}\mathbf{v_{1}}+(1-\alpha_{1})\mathbf{v_{2}} + \alpha_{2}\mathbf{e_{3}}
\]
so that $O_{\Pi}=(0,0)$. 
Elements of $\Sigma$ can similarly be expressed as
\[
(\beta_{1}, \beta_{2})_{\Sigma}= \beta_{1}\Phi(\mathbf{v_{1}})+(1-\beta_{1})\Phi(\mathbf{v_{2}}) + \beta_{2}\mathbf{e_{3}}
\]
Thus $\Pi$ and $\Sigma$ have the structure of two-dimensional vector spaces, isomorphic to $\R^2$. Then by the Lemma 2.2.1, since $\Phi(O_{\Pi}=O_{\Sigma}$, and $\Phi$ is bijective and sends lines to lines, $\Phi:\Pi \longrightarrow \Sigma$ is linear. In other words, 
\[
\Phi[(\alpha_{1},\alpha_{2})_{\Pi}+\lambda(\alpha_{1}',\alpha_{2}')_{\Pi}]=\Phi(\alpha_{1},\alpha_{2})_{\Pi} + \lambda \Phi(\alpha_{1}',\alpha_{2}')_{\Pi}
\]
Note also that 
\begin{eqnarray*}
\Phi(1,0)_{\Pi}&=&\Phi(\mathbf{v_{1}}) \\
&=& (1,0)_{\Sigma}
\end{eqnarray*}
So:
\begin{eqnarray*}
\lambda(\mathbf{v_{1}})+(1-\lambda)(\mathbf{v_{2}}) &=& (\lambda,0)_{\Pi} \\
&=&\lambda(1,0)_{\Pi} \\
&=& \Phi[\lambda(1,0)_{\Pi}] \\
&=& \lambda\Phi(1,0)_{\Pi} \\
&=& \lambda(1,0)_{\Sigma} \\
&=&(\lambda,0)_{\Sigma} \\
&=& \lambda\Phi(\mathbf{v_{1}})+(1-\lambda)\Phi(\mathbf{v_{2}})
\end{eqnarray*}
Hence $\Phi$ is affine on $\R^{3}$ (except for the case when v1-v2 is vertical).
\end{proof}

\begin{Proposition}
Let $\Phi$ be an affine map from $\R^{n}$ to $\R^{n}$. So $\Phi$ preserves affine combinations. Then the map $\phi$ defined by 
\[
\phi(x)=\Phi(x)-\Phi(0)
\]
is a linear map.
\end{Proposition}

\begin{proof}
Let $\lambda \in \R$ and $x$ and $y \in \R^{n}$. Then 
\begin{eqnarray*}
\phi(\lambda x) &=& \Phi(\lambda x) - \Phi(0) \\
&=& \Phi(\lambda x +(1-\lambda)0)-\Phi(0) \\
&=& \lambda \Phi(x)+(1-\lambda)\Phi(0) -\Phi(0) \\
&=& \lambda \Phi(x)-\lambda\Phi(0) \\
&=& \lambda (\Phi(x)-\Phi(0)) \\
&=& \lambda \phi(x) 
\end{eqnarray*}

and

\begin{eqnarray*}
\phi(x+y) &=& \Phi(x+y)-\Phi(0) \\
&=& \Phi(\lambda x' + (1-\lambda)y') -\Phi(0) \\
&=& \lambda \Phi( x') + (1-\lambda)\Phi(y') -\Phi(0) \\
&=& \lambda \Phi(\frac{1}{\lambda} x) + (1-\lambda)\Phi(\frac{1}{1-\lambda} y') -\Phi(0) \\
&=& \lambda (\Phi(\frac{1}{\lambda} x) -\Phi(0)) + (1-\lambda)(\Phi(\frac{1}{1-\lambda} y')-\Phi(0))  \\
&=& \lambda (\phi(\frac{1}{\lambda} x) ) + (1-\lambda)(\phi(\frac{1}{1-\lambda} y'))  \\
&=& \lambda \frac{1}{\lambda} \phi( x)  + (1-\lambda)\frac{1}{1-\lambda} \phi( y')  \\
&=& \phi(x)+\phi(y)
\end{eqnarray*}
Hence $\phi$ is a linear map.
\end{proof} 



\section{Restriction obtained from preservation of connected cosets}
This section completes the proof of Theorem 2.1.1. 
Suppose that $\Phi: \G \longrightarrow \G$ is bijective and sends every coset of any Lie subgroup of $\G$ to some coset of some Lie subgroup of $\G$. Suppose $\phi$ is the corresponding induced map $\g \longrightarrow \g$. 

Now consider the map $A:\g \longrightarrow \g$ defined by 
\[
A(x):= \phi(x)-\phi(0)
\]
Then since $A(0)=)$, A must send subalgebras of $\g$ to subalgebras of $\g$ bijectively (since every subalgebra contains $0$). What are these subalgebras?

A subalgebra of $\g$ is a vector subspace which is closed under the Lie bracket. 
\subsection{one-dimensional subalgebras}
\begin{Proposition}
All one-dimensional subspaces of $\g$ are subalgebras.
\end{Proposition}

\begin{proof}
All one-dimensional subspaces are lines through the origin. Let $S$ be a one-dimensional subspace of $\g$. Then 
\[
S= \text{span}(X)
\]
for some $X=(x,y,z) \in \g$. Let $\lambda X$ and $\mu X$ be elements of $S$. Then 
\begin{eqnarray*}
[\lambda X,\mu X] &=& [(\lambda x, \lambda y, \lambda z), (\mu x, \mu y, \mu z)] \\
&=& (0,0, \lambda x \mu y - \lambda y \mu x) \\
&=& (0,0, \lambda  \mu xy - \lambda \mu xy) \\
&=& (0,0,0) \in S
\end{eqnarray*}
Hence every one-dimensional subspace of $S$ is closed under the Lie bracket and is thus also a one-dimensional subalgebra. 
\end{proof}

\subsection{two-dimensional subalgebras}
Let $Z$ denote the z-axis.
The question is now whether all two-dimensional subspaces of $\g$ are also subalgebras. Two-dimensional subspaces of $\g$ are planes through the origin. So any subspace is the span of two linearly independent vectors in $\g$.
Let $T= \text{span}(X ,Y)$ where $X=(x_{1},x_{2},x_{3})$ and $Y=(y_{1},y_{2},y_{3})$ are distinct elements of $\g$.

Then  
\begin{eqnarray*}
  [X,Y] & \in T \\
(0,0, x_{1}y_{2}-x_{2}y_{1}) & \in T
\end{eqnarray*}
   
This restriction can be analysed in the following two distinct cases:

\begin{enumerate}
 \item $x_{1}y_{2}=x_{2}y_{1}$
 \item $x_{1}y_{2}\neq x_{2}y_{1}$
\end{enumerate}

In case (1), 
\[
x_{1}y_{2}=x_{2}y_{1}
\]
First consider the sub-case in which $x_{1}=x_{2}=0$ Then it is not possible to have both $x_{2}=0$ and $y_{2}=0$ since this would make $X$ and $Y$ linearly dependent. So without loss of generality select $y_{2} \neq 0$. Then there exists $k \in \R$ such that 
\[
x_{2} = ky_{2}
\]
So 
\[
X-kY=(0,0,x_{3}-ky_{3}) \in T
\]
Note that $X-kY \neq 0$ since otherwise $X$ and $Y$ would be linearly dependent. So $T$ contains $\text{span}(0,0,x_{3}-ky_{3})=Z$, and hence $T$ is a vertical plane. 

Now suppose at least one of $x_{1}$ and $y_{1}$ is non-zero. Without loss of generality suppose $y_{1} \neq 0$.
Then there exists some $\lambda \in \R$ such that $x_{1} = \lambda y_{1}$. So

\begin{eqnarray*}
 \lambda y_{1}y_{2} &=& x_{2} y_{1}\\
 \lambda y_{2} &=& x_{2}
\end{eqnarray*}

Hence
\begin{eqnarray*}
 X-\lambda Y & = & (x_{1},x_{2},x_{3})-(\lambda y_{1},\lambda y_{2},\lambda y_{3}) \\
	     & = & (0,0,x_{3}-\lambda y_{3}) \\
	     & \neq & 0
\end{eqnarray*}
since $X$ and $Y$ are linearly independent. 
Therefore 
\[
Z=\text{span}(0,0,1)=\text{span}(0,0,x_{3}-\lambda y_{3}) \in T
\]

In the second case, $x_{1}y_{2}\neq x_{2}y_{1}$ so 
\[
\text{span}(0,0, x_{1}y_{2}- x_{2}y_{1})= Z \in T
\]

Hence any plane through the origin in $\g$ which is closed under the Lie bracket must contain the z-axis. In other words, all two-dimensional subalgebras of $\g$ are vertical planes through the origin.

%-find all the one and two dimensional subalgebras of g. 
%-describe the cosets
%-use the previous Proposition to show the map is linear + translation
%-use preservation of vertical planes to show that the linear portion of the %map takes on that slightly restricted form

\section{Restriction obtained from preservation of discrete cosets}

\section{Summary}

\chapter{Case of all upper triangular nilpotent Lie groups}

\end{document}
