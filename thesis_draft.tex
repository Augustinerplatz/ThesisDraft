\documentclass[honours]{UNSWthesis}
\linespread{1}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{latexsym,amsmath}
\usepackage{graphicx}

%% define some macros
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\G}{\mathcal{G}}
%\newcommand{\H}{\mathcal{H}}
\newcommand{\g}{\mathfrak{g}}
\newcommand{\1}{\mathbf{e}_{1}}
\newcommand{\2}{\mathbf{e}_{3}}
\newcommand{\3}{\mathbf{e}_{3}}


%% new environments

\newcounter{Item}[section]
%\newenvironment{proof}{\noindent {\bf Proof.}\ }{\qed}
\newenvironment{Definition}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Definition \thesection.\theItem.}\ }
                           {\medskip}
\newenvironment{Notation}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Notation \thesection.\theItem.}\ }
                           {\medskip}
\newenvironment{Theorem}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Theorem \thesection.\theItem.}\ %
                            \begingroup \sl}
                           {\endgroup\medskip}
\newenvironment{Proposition}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Proposition \thesection.\theItem.}\ %
                            \begingroup \sl}
                           {\endgroup\medskip}
\newenvironment{Corollary}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Corollary \thesection.\theItem.}\ %
                            \begingroup \sl}
                           {\endgroup\medskip}
\newenvironment{Lemma}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Lemma \thesection.\theItem.}\ %
                            \begingroup \sl}
                           {\endgroup\medskip}
\newenvironment{Conjecture}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Conjecture \thesection.\theItem.}\ %
                            \begingroup \sl}
                           {\endgroup\medskip}
\newenvironment{Example}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Example \thesection.\theItem.}\ }
                           {\qed}
\newenvironment{Remark}{\smallskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Remark \thesection.\theItem.}\ }
                           {\qed}
\newenvironment{Question}{\smallskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Question \thesection.\theItem.}\ }
                           {\par}
\newenvironment{theoremlist}{\begin{list}{}
                        {\setlength{\parsep}{0pt}
                        \setlength{\topsep}{\smallskipamount}} }
                        {\end{list}}

\title{Rigidity of Coset-preserving Maps on Upper-Triangular Nilpotent Lie Groups}

\authornameonly{Richard Tierney}

\author{\Authornameonly\\{\bigskip}Supervisor: Professor Michael Cowling}

\begin{document}
\maketitle

\prefacesection{Acknowledgements}
{\noindent}Many thanks go to Michael Cowling, Georgia Tsambos, Andrew Ardill...

\prefacesection{Introduction}

Lie Groups are to be found in many different applications of mathematics as well as being central to many of the ideas in quantum physics. It is therefore very useful to know whether different Lie groups linked by a certain map are similar in structure, or whether a particular map identifies two Lie groups in a useful or sensible way. This is the idea of a rigidity theorem. If there is a map between two nilpotent Lie groups that preserves the cosets of subgroups (structures arising from the group law on a Lie group), just how much structure of the respective Lie groups does the map preserve? In this paper, the extra assumption of bijectivity will help to show the kind of similarity that exists. If a map is bijective then no information is 'lost' about the structures of the domain and codomain, when either the map or its inverse is applied. 
This thesis is an attempt to demonstrate rigorously the extension of the rigidity theorem for the Heisenberg Group to all the nilpotent Lie groups. The proof involves arguments from a geometrical as well as algebraic perspective. 


%begin chapter 1
\chapter{Lie Groups, Lie Algebras and Homomorphisms}
This chapter outlines the basics of the theory of Lie Groups and their relationship to Lie Algebras. It gives some examples of matrix Lie groups including the Heisenberg Group, and explains the use of the Baker-Campbell-Hausdorff Formula in comparing operations in a Lie Group with the operations in its corresponding Lie Algebra. 

\section{Introduction}
The theory of Lie Groups has evolved from the combination of several different disciplines in mathematics. Lie Group Theory combines the following ingredients: Group Theory from Algebra, Manifold Theory from Differential Geometry, and basic ideas from Topology. All of the Lie Groups mentioned in this paper will be matrix Lie groups. That is, they are matrix groups that are also $C^2$-manifolds. The idea of a rigidity theory in this context is to use structures that arise from the group law within the manifold to show that a weak similarity of these matrix Lie groups implies a strong similarity between them. This paper will prove the theorem for upper-triangular nilpotent matrix Lie groups (and solvable matrix Lie groups??).

\section{Lie Groups}
\begin{Definition}\label{Lie Group}
A Lie group $\G $ is a $\mathrm{C}^{\infty}$ manifold which is also a group, for which the group operations of multiplication and taking inverses are continuous. ie the maps

\begin{eqnarray*}
\sigma : & \G \times \G & \longrightarrow \G  \\
& (g,h) & \longmapsto gh
\end{eqnarray*}
and

\begin{eqnarray*}
\tau : & \G  & \longrightarrow \G  \\
& g & \longmapsto g^{-1}
\end{eqnarray*}

are continuous with respect to the topology on $\G$.
\end{Definition}

\begin{Example}\label{Rn}
$(\R^{n}, +)$ is a Lie group with one single coordinate patch defined by the identity map on $\R^{n}$. The usual addition, and inversion given by multiplication by $(-1)$ are continuous maps.

\end{Example}

\begin{Example}\label{Circle}
The circle $\mathrm{S}^{1} = \{ e^{i\theta}\; \big| 0 \leq \theta < 2\pi \}$ is a Lie group. As a manifold, $\mathrm{S}^{1}$ has a single coordinate patch given by:

\begin{eqnarray*}
p_{1} : & \mathrm{S}^{1}  & \longrightarrow \R  \\
& e^{i\theta} & \longmapsto \theta
\end{eqnarray*}

The group multiplication on $\mathrm{S}^{1}$ is given by 
\[ (e^{i\theta}, e^{i\phi}) \longmapsto e^{i\theta}e^{i\phi} = e^{i(\theta + \phi)} \]
\end{Example}

An important aspect of this particular coordinate patch (even though in this case only one is required), is that it is 
also the tangent space of the manifold at the identity element of the group. [add picture]. In general this object is 
very useful for the study of Lie groups and takes on a structure of its own called a Lie algebra. 

%\begin{figure}
%\includegraphics{imagename.eps}
%\caption{Lovely caption}
%\label{fig:somethin-you-will-remember}
%\end{figure}

\begin{Definition}\label{Matrix Lie Group}
A matrix Lie group is any subgroup $\G$ of $\mathrm{GL}(n,\C)$ with the property that if there is a sequence $A_{m}$ of
matrices in $\G$ that converges to some matrix $A \in \mathrm{M}(n,\C)$ then either $A \in \G $ or $A$ is not invertible.
This is equivalent to $G$ being a closed subset of $\mathrm{GL}(n,\C)$.
\end{Definition}

\subsection*{Examples}
\[\mathrm{SL}(n,\R), \mathrm{O}(n), \mathrm{SO}(n), \mathrm{SU}(2) \]


\begin{Example}\label{Heisenberg Group}.
The Heisenberg group, defined as follows, is a Lie group. This example forms the cornerstone of the result in this thesis.
\paragraph
{\noindent}The Heisenberg group is the set of real three-tuples with non-commutative multiplication given by
\[
(x,y,z). (x', y', z') := (x+x', y+y', z+z' + xy')
\]
Inversion is given by 
\[ (x,y,z)^{-1}= (-x, -y, -z+xy) \]
As a matrix Lie group, the Heisenberg group can be represented as the group of all $ 3\times 3 $ upper triangular real matrices with $1$'s on the diagonal:
\[
 \G= \left\{ \begin{bmatrix} 1 & x & z \\ 0 & 1 & y \\ 0 & 0 & 1 \end{bmatrix} \bigg| x,y, z \in \R \right\} 
\]
It is topologically a closed subset of $\mathrm{GL}(n,\C)$ and is clearly closed under the operations of inversion and multiplication. Hence $\G$ is a matrix Lie group.
\end{Example}

Many of the important examples of matrix Lie groups are noncommutative groups. This makes the study of their structure in many 
cases fairly complex. However, as with the example of the circle, studying the Lie group can be made simpler by studying what is called its Lie algebra. The structure of the Lie algebra determines the local structure of its Lie group.


\section{Lie Algebras}
\begin{Definition}\label{Lie Algebra}
A Lie algebra is a vector space $\mathcal{A}$ endowed with a map
\[ [\cdotp,\cdotp]:\; \mathcal{A} \times \mathcal{A} \longrightarrow \mathcal{A} \]
with the following properties:
\begin{enumerate}
 \item $[\cdotp, \cdotp]$ is bilinear
 \item $[X,Y]=-[Y,X]$ for all $X$, $Y$ $\in \mathcal{A}$
 \item $[X,[Y,Z]]+[Y,[Z,X]]+[Z,[X,Y]]=0$ for all $X$, $Y$, $Z$ $\in \mathcal{A}$
\end{enumerate}
This map is often called the \emph{Lie bracket} or the \emph{bracket operation}.
\end{Definition}

The Lie algebra $\g$ of a Lie group $\G$ is the tangent space to $G$ at the identity $e$. This can be seen clearly in 
the example of the circle. The circle is parametrised as before by a the curve
\[ g: [0,2\pi) \longrightarrow \C \]
\[ g(\theta)= e^{i\theta}\]
Differentiating gives
\[g' (\theta)= ie^{i\theta}\]
At the identity, $1=e^{0}$,
\[ g' (0) = i \]
so the tangent space $s$ is the real vector space with basis $\{i\}$. 
In this case the operation $[\cdotp, \cdotp]$ is trivial, as with the Lie algebra of any abelian Lie group.
Without going into rigorous detail, it can be seen that the group operation on $S^{1}$ corresponds to addition of 
vectors in $s$. Also, the close we move towards the identity in $S^{1}$ the more it resembles flat space. Multiplying two elements together becomes almost equivalent to adding their imaginary components.
(picture required)

In the case of matrix Lie groups, tangent space at the identity takes a very clear form. The relationship between a 
matrix Lie group and its Lie algebra is determined by the exponential mapping on matrices. 

\begin{Definition}\label{Lie Algebra of a Matrix Lie Group}
The Lie algebra of an $n \times n$ matrix Lie group $\G$ is
\[
\g = \{ X \in M_{n}(\C) \big| \exp{X} \in \G \}
\]
The bracket operation on $\g$, given by $[X,Y]=XY-YX$, satisfies the axioms for $\g$ to be a Lie algebra. 
\end{Definition}

\begin{Proposition}
This definition corresponds to the definition of a Lie algebra as the tangent space to the manifold $\G$ at the identity. (proof?)
\end{Proposition}

Note that the exponential map from $\g$ to $\G$ is injective but not necessarily surjective. The image of $\g$ under the exponential map is precisely the connected component of $\G$ containing the identity $e$. (proof). In the case that $\G$ is connected, the inverse of the map $\exp : \g \longrightarrow \G $ is called the matrix logarithm map, $\log : \G \longrightarrow \g $.

\begin{Proposition}\label{Lie algebra of Heisenberg group}
The Lie algebra of the Heisenberg group, $$\G = \left\{ \begin{bmatrix} 1 & x & z \\ 0 & 1 & y \\ 0 & 0 & 1 \end{bmatrix} \bigg| x,y, z \in \R \right\}$$ is 
\[
\g= \left\{ \begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix} \bigg| x,y,z \in \R \right\}
\]
and the exponential map $\exp$: $\g \longrightarrow \G$ is a bijection. 
\end{Proposition}

\begin{proof}
It will be shown that $im(\exp) \subseteq \G$, $\G \subseteq im(\exp)$ and that $\exp$ is injective. 
Let $x$, $y$ and $z$ be real numbers and note the following: 
\[
\begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix}^2=\begin{bmatrix} 0 & 0 & xy \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}
\]
\[
\begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix}^3=\begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}
\]
Now,
\begin{eqnarray*}
 \exp \begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix} &=& \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} + \begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix}+ \frac{1}{2}\begin{bmatrix} 0 & 0 & xy \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} \\
&=& \begin{bmatrix} 1 & x & z+\frac{1}{2}xy \\ 0 & 1 & y \\ 0 & 0 & 1 \end{bmatrix} \\
&\in & \G
\end{eqnarray*} so $ im(\exp) \subseteq \G$.
Now suppose $a,b,c \in \R$ so $\begin{bmatrix} 1 & a & c \\ 0 & 1 & b \\ 0 & 0 & 1 \end{bmatrix} \in \G$. Then 
\[ 
\exp  \begin{bmatrix} 0 & a & c-\frac{1}{2}ab \\ 0 & 0 & b \\ 0 & 0 & 0 \end{bmatrix} = \begin{bmatrix} 1 & a & c \\ 0 & 1 & b \\ 0 & 0 & 1 \end{bmatrix}
\]
so $\G \subseteq im(\exp)$.

Suppose that $A=\begin{bmatrix} 0 & a & c \\ 0 & 0 & b \\ 0 & 0 & 0 \end{bmatrix}$ and $A'=\begin{bmatrix} 0 & a' & c' \\ 0 & 0 & b' \\ 0 & 0 & 0 \end{bmatrix}$ are in $\g$ and that $\exp{A}=\exp{A'}$. Then 
\[
\begin{bmatrix} 0 & a & c+\frac{1}{2}ab \\ 0 & 0 & b \\ 0 & 0 & 0 \end{bmatrix} =\begin{bmatrix} 0 & a' & c' +\frac{1}{2}a' b' \\ 0 & 0 & b' \\ 0 & 0 & 0 \end{bmatrix}
\] 
and hence
\begin{eqnarray*}
a &=& a' \\
b &=& b' \\
c &=& c'
\end{eqnarray*}
so $A=A'$.
\end{proof}

\begin{Definition}
The elements of $\g$ are $3 \times 3$ upper-triangular nilpotent matrices, where a matrix $A$ is called \emph{nilpotent} if 
\[
A^{n}=\mathbf{0}
\]
for some positive integer $n$. Since the elements of $\g$ are nilpotent, the corresponding Lie group $\G$ is called a \emph{nilpotent Lie group}.
\end{Definition}

\subsection*{Notation}
For the purpose of simplifying the notation of matrices in the Heisenberg group and in its Lie algebra, vector notation will sometimes be used. As defined above, an element of the Heisenberg group $\G$ will be denoted either by $ \begin{bmatrix} 1 & a & c \\ 0 & 1 & b \\ 0 & 0 & 1 \end{bmatrix} $ or equivalently $[a,b,c]$. Similarly, elements of the Lie algebra of the Heisenberg group will be denoted either by $\begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix}$ or $(x,y,z)$, or in some cases $\begin{bmatrix} x \\ y \\ z \end{bmatrix}$.

In this notation, the bracket operation on two elements of $\g$ is given by 
\begin{eqnarray*}
[(x,y,z), (x',y',z')] &=& \begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & x' & z' \\ 0 & 0 & y' \\ 0 & 0 & 0 \end{bmatrix} - \begin{bmatrix} 0 & x' & z' \\ 0 & 0 & y' \\ 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix} \\
&=& \begin{bmatrix} 0 & 0 & xy' \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} -\begin{bmatrix} 0 & 0 & x'y \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} \\
&=& (0,0,xy'-x'y)
\end{eqnarray*}

\section{Subgroups and Subalgebras}

\begin{Definition}\label{Lie subgroup}
Let $\G$ be a Lie group. A subset $\mathcal{H}$ of $\G$ is called a Lie subgroup if it is a subgroup and also a Lie group. So topologically $\mathcal{H}$ must be a closed subset of $\mathrm{GL}(n,\C)$, and algebraically $\mathcal{H}$ must be closed under operations of multiplication and taking inverses. 
\end{Definition}

\begin{Definition}\label{Lie subalgebra}
Let $\g$ be a Lie algebra. Then a vector subspace $\mathfrak{h}$ of $\g$ is called a Lie subalgebra of $\g$ if it is also closed under the Lie bracket, so if $X$ and $Y$ are in $\mathfrak{h}$
\[
[X,Y] \in \mathfrak{h}
\]

N.B. In this paper, the term "subalgebra" will be interchangeable with "Lie subalgebra" since no other types of algebras will be mentioned.
\end{Definition}

\section{Homomorphisms}

\begin{Definition}\label{Lie Group Homomorphism}
Let $\G$ and $\mathcal{H}$ be Lie groups and $f:\G \longrightarrow \mathcal{H} $ be a function. $f$ is called a \emph{Lie group homomorphism} if and only if 
\[ f(xy)=f(x)f(y) \; \; \; \text{(i.e. $f$ is a group homomorphism)} \]
and $f$ is continuous with respect to the topologies on $\G$ and $\mathcal{H}$. 
If moreover, $f$ is a bijection, $f$ is called a \emph{Lie group isomorphism}
\end{Definition}

\begin{Example}\label{canonical quotient of centre}
Let $\G$ be the Heisenberg group. Let 
\begin{eqnarray*}
\pi : &\G& \longrightarrow (\R^{2},+) \\
&[a,b,c]& \longmapsto (a,b)
\end{eqnarray*}

Then 

\begin{eqnarray*}
\pi([a,b,c][a',b',c']) &=& \pi[a+a',b+b',c+c'+ab'] \\
&=&(a+a',b+b') \\
&=& (a,b)+(a',b') \\
&=& \pi[a,b,c] + \pi[a',b',c']
\end{eqnarray*}

So $\pi$ is a group homomorphism. Also, since the topology on $\G$ is inherited from the topology on $\R^{3}$, $\pi$ acts topologically as a projection from $\R^{3}$ onto $\R^{2}$ so is continuous. Hence $\pi$ is a Lie group homomorphism.
\end{Example}

\begin{Definition}\label{Lie Algebra Homomorphism}
Let $\g$ and $\mathfrak{h}$ be Lie algebras. A linear transformation $T: \g \longrightarrow \mathfrak{h}$ is a Lie algebra homomorphism if in addition to the linearity properties, $T$ preserves the Lie bracket, i.e. 
\[
T[A,B] = [T(A), T(B)] \; \;\;\;\;\;\;\;\; \forall A,B \in \g
\]
\end{Definition}


\begin{Definition}\label{Lie group anti-homomorphism}
Let $\G$ and $\mathcal{H}$ be Lie groups. Then a function $f: \G \longrightarrow \mathcal{H}$ is called a \emph{Lie group anti-homomorphism} if it is continuous with respect to the topologies on $\G$ and $\mathcal{H}$ and
\[
f(xy)=f(y)f(x) \;\;\;\;\; \forall x,y \in \G
\]
If on top of this $f$ is a bijection, then it is called a \emph{Lie group anti-isomorphism}.
\end{Definition}

\begin{Definition}\label{Lie algebra anti-homomorphism}
Let $\g$ and $\mathfrak{h}$ be Lie algebras. A linear transformation $T: \g \longrightarrow \mathfrak{h}$ is a Lie algebra anti-homomorphism if in addition to the linearity properties,  
\[
T[A,B] = -[T(A), T(B)] \; \;\;\;\;\;\;\;\; \forall A,B \in \g
\]
\end{Definition}

\begin{Theorem}\label{Homomorphisms Correspondence}
Let $\G$ and $\mathcal{H}$ be matrix Lie groups with corresponding Lie algebras $\g$ and $\mathfrak{h}$ respectively. Suppose that $\Phi: \G \longrightarrow \mathcal{H}$ is a Lie group homomorphism. Then there exists a unique Lie algebra homomorphism $\phi: \g \longrightarrow \mathfrak{h}$ such that 
\[
\Phi(e^{X}) = e^{\phi(X)} \;\;\;\;\;\;\; \forall X \in \g
\]

Now let $\G$ and $\mathcal{H}$ be matrix Lie groups, $\g$ and $\mathfrak{h}$ be their Lie algebras and let $\G$ be connected. Then if $\phi: \g \longrightarrow \mathfrak{h}$ is a Lie algebra homomorphism, there exists a unique Lie group homomorphism $\Phi: \G \longrightarrow \mathcal{H}$ defined by
\[
\Phi(A) = e^{\phi(\log{A})} \;\;\;\;\;\;\; \forall A \in \G
\]
\end{Theorem}

\begin{Example}
Let $\G$ be the Heisenberg group and $\g$ its Lie algebra. Let $\mathcal{H}=\mathrm{GL}(1,\R)$. The Lie algebra of $\mathcal{H}$ is $\mathfrak{h}=\mathrm{M}_{1}(\R)$. Define 
\begin{eqnarray*}
\Phi :& \G &\longrightarrow  \mathcal{H} \\
&[x,y,z] &\longmapsto  e^{x}
\end{eqnarray*}
This is a Lie group homomorphism. It is continuous since the map $x \mapsto e^{x}$ is continuous and 

\begin{eqnarray*}
\Phi ([x,y,z][x',y',z']) &=& \Phi [x+x',y+y', z+z'+xy'] \\
&=& e^{x+x'} \\
&=& e^{x}e^{x'} \\
&=& \Phi [x,y,z] \Phi [x',y',z']
\end{eqnarray*}

Now
\[
\Phi [x,y,z] = \Phi (\exp{(x,y,z-\frac{1}{2}xy))}=e^{x}
\]
So there is a unique map $\phi: \g \longrightarrow \mathfrak{h}$ defined by $\phi(x,y,z)=x$ such that
\[
\Phi(e^{X}) = e^{\phi(X)} \;\;\;\;\;\;\;\; \forall X \in \g
\]
 It is a linear map of vector spaces since it projects onto one of the dimensions. Also for $(x,y,z)$ and $(x',y',z')$ in $\g$, 
\[ \phi [(x,y,z), (x',y',z')]=\phi (0,0,xy'-x'y)=0
\]
and
\begin{eqnarray*}
[\phi(x,y,z),\phi(x',y',z')]=[x,x']=xx'-x'x &=& 0 \\
&=&  \phi [(x,y,z), (x',y',z')]
\end{eqnarray*}
Hence $\phi$ is a Lie algebra homomorphism.
\end{Example}

%Baker Campbell Hausdorff Formula
\section{The Baker-Campbell-Hausdorff Formula}

Since we wish to simplify the study of Lie group homomorphisms by studying properties of the corresponding Lie algebra homomorphisms, we need a way to relate the operations on these objects directly. In the case of the unit circle $\mathrm{S}^{1}$, the properties of the exponential function on real numbers give the simple correspondence between addition on the real line, and multiplication of elements of the unit circle in the complex plane. For noncommutative groups this simple relationship does not hold. However, there is a more complicated way of relating the structure of Lie algebras to their corresponding Lie groups. 

\paragraph{Heisenberg Group}
The Lie algebra of the Heisenberg group has noncommutative multiplication, however for any $X$, $Y$ $\in \g$, $X$ and $Y$ each commutes with $[X,Y]$. In other words
\[
[X,[X,Y]]=[Y,[X,Y]]=0
\]
\begin{proof}
Let $X=(x,y,z)$ and $Y=(x',y',z')$ be elements of $\g$. 
\begin{eqnarray*}
[X,[X,Y]] &=& [X, (0,0,xy'-x'y)] \\
&=&[(x,y,z),(0,0,xy'-x'y)] \\
&=& 0
\end{eqnarray*}

and
\begin{eqnarray*}
[Y,[X,Y]] &=& [Y, (0,0,xy'-x'y)] \\
&=&[(x',y',z'),(0,0,xy'-x'y)] \\
&=& 0
\end{eqnarray*}
\end{proof}

We require some tools in order to explain the formula for the Heisenberg group. 

\begin{Definition}\label{Two Adjoint Mappings}
Let $X$ and $Y$ be elements of $\mathrm{M}_{n}(\C)$. Define the following maps: 
\begin{eqnarray*}
\mathrm{Ad}_{Y}:& \mathrm{M}_{n}(\C) & \longrightarrow \mathrm{M}_{n}(\C) \\
&X& \longmapsto YXY^{-1}
\end{eqnarray*}

and
\begin{eqnarray*}
\mathrm{ad}_{Y}:& \mathrm{M}_{n}(\C) & \longrightarrow \mathrm{M}_{n}(\C) \\
&X& \longmapsto [Y,X]=YX - XY
\end{eqnarray*}

\end{Definition}

\begin{Definition}
Define also the operator $e^{\mathrm{ad}_{Y}}$ given by the following power series:
\[
e^{\mathrm{ad}_{Y}}(X)= \mathrm{id}(X) + \mathrm{ad}_{Y}(X) + \frac{1}{2!}( \mathrm{ad}_{Y}^{2})(X) + \frac{1}{3!}( \mathrm{ad}_{Y}^{3})(X) + \cdots
\]
\end{Definition}

\begin{Lemma}
\[
\mathrm{Ad}_{e^{Y}}(X)=e^{\mathrm{ad}_{Y}}(X)
\]
\end{Lemma}

\begin{Theorem}
Let $\G$ be a matrix Lie group with Lie algebra $\g$. If the commutivity property above holds in $\g$ then for $X$ and $Y$ in $\g$, 
\[
e^{X}e^{Y}=e^{X+Y+\frac{1}{2}[X,Y]}
\]
This is a special case of the Baker-Campbell-Hausdorff Formula (which will sometimes be referred to as the BCF Formula in this project)
\end{Theorem}

\begin{proof}
To prove the formula, it will be shown that two apparently differential functions are both the unique solution to a certain differential equation, and therefore must be identical. 
Let $X$ and $Y$ be elements of $\g$. Consider the differential equations in the real variable $t$ defined by 
\begin{eqnarray*}
A(t)&=& e^{tX}e^{tY}e^{-\frac{t^{2}}{2}[X,Y]} \\
B(t) &=& e^{t(X+Y)}
\end{eqnarray*}

\begin{Lemma}\label{commute matrices}
Let $M$ and $N$ be $n \times n$ matrices. If $M$ commutes with $N$, then $M$ also commutes with $e^{N}$. 
\end{Lemma}
 

\begin{proof}
Suppose $MN^{k}=N^{k}M$ for some positive integer $k$. Then 
\begin{eqnarray*}
MN^{k+1} &=& MN^{k}N \\
&=& N^{k}MN \\
&=& N^{k}NM \\
&=& N^{k+1}M
\end{eqnarray*}
So since $MN=NM$, the mathematical induction hypothesis indicates that $MN^{K}=N^{K}M$ for all positive integers $K$.
Now, 
\begin{eqnarray*}
Me^{N} &=& M(I+N+\frac{N^{2}}{2!}+\frac{N^3}{3!}+\cdots) \\
&=& M+MN+\frac{1}{2!}MN^{2}+\frac{1}{3!}MN^{3} + \cdots \\
&=& M+NM+\frac{1}{2!}N^{2}M+\frac{1}{3!}N^{3}M + \cdots \\
&=& (I+N+\frac{N^{2}}{2!}+\frac{N^3}{3!}+\cdots)M \\
&=& e^{N}M
\end{eqnarray*}
\end{proof}
\begin{Lemma}
\[
Xe^{tY}=e^{tY}(X+t[X,Y])
\]
\end{Lemma}
\begin{proof}
\begin{eqnarray*}
Xe^{tY} &=& e^{tY}e^{-tY}Xe^{tY} \\
&=& e^{tY}\mathrm{Ad}_{e^{-tY}}(X) \\
&=& e^{tY}e^{-t\mathrm{ad}_{Y}}(X) \\
&=& e^{tY}(X-t[Y,X]+\frac{t^{2}}{2}[Y,[Y,X]]-\frac{t^{3}}{3!}[Y,[Y,[Y,X]]]+\cdots \\
&=& e^{tY}(X - t[Y,X])\\
&=& e^{tY}(X + t[X,Y])
\end{eqnarray*}
since $[Y,[Y,X]]=0$.
\end{proof}

$A(t)$ and $B(t)$ are both now differentiated as follows:

\begin{eqnarray*}
A'(t) &=& e^{tX}Xe^{tY}e^{-\frac{t^2}{2}[X,Y]}+e^{tX}e^{tY}Ye^{-\frac{t^2}{2}[X,Y]}+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(-t[X,Y]) \\
&=& e^{tX}Xe^{tY}e^{-\frac{t^2}{2}[X,Y]}+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}Y+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(-t[X,Y]) \\
& & (\text{since $Y$ commutes with $[X,Y]$ and by Lemma 1.5.5})\\
&=&e^{tX}e^{tY}(X+t[X,Y])e^{-\frac{t^2}{2}[X,Y]}+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}Y+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(-t[X,Y]) \\
&=& e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(X+t[X,Y])+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}Y+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(-t[X,Y]) \\
& & (\text{since $X$ and $[X,Y]$ each commute with $[X,Y]$})\\
&=& e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(X+t[X,Y]+Y-t[X,Y]) \\
&=& e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(X+Y) \\
&=& A(t)(X+Y)
\end{eqnarray*}

\begin{eqnarray*}
B'(t) &=& e^{t(X+Y)}(X+Y) \\
&=& B(t)(X+Y)
\end{eqnarray*}

It is also necessary for the above differential equations to have the same initial conditions if the uniqueness theorem is to be applied. 
\[
A(0)=e^{0}e^{0}e^{0}=I^{3}=I
\]
and 
\[
B(0)=e^{0}=I
\]
So $A(t)$ and $B(t)$ satisfy the same differential equation with the same initial conditions. By the basic uniqueness theorems for differential equations this implies $A(t)=B(t), \forall t \in \R$. Substituting the value $t=1$ gives 
\begin{eqnarray*}
e^{X}e^{Y}e^{-\frac{1}{2}[X,Y]} &=&e^{X+Y} \\
e^{X}e^{Y} &=& e^{X+Y}e^{\frac{1}{2}[X,Y]} \\
&=& e^{X+Y+\frac{1}{2}[X,Y]} \\
& & (\text{since $(X+Y)$ commutes with $\frac{1}{2}[X,Y]$})
\end{eqnarray*}
This completes the proof of the Baker-Campbell-Hausdorff Formula for the Heisenberg group. 
\end{proof}

There is a more general formula for any noncommutative Lie group $\G$, however there is an infinite series of commutator terms, say $S$, such that for all $X$ and $Y$ in $\G$,
\[
e^{X}e^{Y}=e^{X+Y+S}
\]
The series formula will be stated but not proved in this thesis. A full proof is given in ...
%find a proof for general BCH formula




%Begin chapter 2
\chapter{Case for the Heisenberg Group}
For this chapter the following notation will be used:
\begin{Notation}
$$\G=\text{the Heisenberg group}$$
$$\g=\text{the Lie algebra of the Heisenberg group}$$
\end{Notation}

\section{Aim}
The aim of this chapter is to prove a rigidity theorem for coset-preserving bijective maps on the Heisenberg group. The ideas and methods used in this theorem are foundational for the proof of the theorem for higher dimensional Lie groups. The first theorem will show that coset-preserving maps on the Heisenberg group have corresponding maps on the Lie algebra that are affine, and take on a particular form.\
The second theorem shows the stronger result if the map also preserves cosets of discrete subgroups of the Heisenberg group.





\section{Proof that it is an affine map}

\begin{Definition}\label{affine map}
A map $\varphi: \R^{n} \longrightarrow \R^{n}$ is \emph{affine} if for all $\mathbf{v_{1}},\mathbf{v_{2}} \in \R^{n}$ and all $\lambda \in \R$ 
\[
\varphi [ \lambda \mathbf{v_{1}}+(1-\lambda) \mathbf{v_{2}}]= \lambda \varphi (\mathbf{v_{1}})+(1-\lambda) \varphi(\mathbf{v_{2}})
\]
\end{Definition}

\begin{Lemma}
Suppose $\alpha: \R^{2} \longrightarrow \R^{2}$ sends $0$ to $0$, sends lines to lines and is bijective. Then $\alpha$ is linear. 
\end{Lemma}

\begin{proof}
Construct a map based on the following assumptions. Let $\alpha': \R^{2} \longrightarrow \R^{2}$ be linear and
\begin{eqnarray*}
\alpha'(\alpha(1,0))&=& (1,0)\\
\alpha'(\alpha(0,1)) &=& (0,1)
\end{eqnarray*}
This map $\alpha'$ is well-defined and unique since bijectivity of $\alpha$ implies $\alpha(1,0) \neq \alpha(0,1)$ so every element of $\R^{2}$ can be written as a linear combination of $\alpha(1,0)$ and $\alpha(0,1)$. 
Now define a map $\psi: \R^{2} \longrightarrow \R^{2}$ by $\psi= \alpha'_{o}\alpha$
Note that $\psi$ is bijective so sends parallel lines to parallel lines. In particular $\psi$ sends the x-axis to itself and also the y-axis to itself. Suppose $\psi(x_{1},0)=(x_{1}',y_{1}')$ and $\psi(x_{2},0)=(x_{2}',y_{2}')$
A geometrical construction will demonstrate that
\begin{equation}
\psi(x_{1}+x_{2},0)=\psi(x_{1},0)+\psi(x_{2},0)
\end{equation}

and 
\begin{equation}
\psi(x_{1}x_{2},0)=(x_{1}'x_{2}',0)
\end{equation}


where 
\begin{eqnarray*}
P: &R^{2}& \longrightarrow \R \\
&(x,y)& \longmapsto x
\end{eqnarray*}

Proof of the addition property:
Consider the diagram. Let $P=(1,0)$ and $Q=(0,1)$. $A$ is the point $(x_{1},0)$ and $B$ is the point $(x_{2},0)$. Then construct lines $L_{1}$, $L_{2}$, $L_{3}$ and $L_{4}$ as shown so that $$A'=(0,x_{1})$$ $$B'=(x_{2}, x_{1})$$ and $$C=(x_{1}+x_{2},0)$$ Now the map $\psi$ sends the x and y axes to themselves, sends parallel lines to parallel lines and $\psi(P)=P$ and $\psi(Q)=Q$. So the parallelogram $AA'B'C$ is mapped to a new parallelogram $\psi(A)\psi(A')\psi(B')\psi(C)$ where the sloping sides must have gradient $-1$, since they are parallel to $PQ$. So $\psi(A)=(x_{1}',0)$ and $\psi(B)=(x_{2}',0)$. Then by the properties of the parallelogram, 
\[
\psi(A')=(0,x_{1}')
\]
\[
\psi(B')=(x_{2}',x_{1}')
\]
and 
\[
\psi(C)=(x_{1}'+x_{2}',0)
\]
Hence $\psi(x_{1}+x_{2},0)=\psi(x_{1},0)+\psi(x_{2},0)$.

Now consider this diagram showing the construction of $C=(x_{1}x_{2},0)$ by starting with $A=(x_{1},0)$ and $B=(x_{2},0)$. Construct the line of gradient $-1$ through $A$ so that $A'=(0,x_{1})$. Then construct a horizontal through $A'$ and a vertical through $P$ to meet in $D=(1,x_{1})$. Construct a line $L$ through $O$ and $C$, $y=x_{1}x$ and a vertical line through $B$ to meet in $E=(x_{2}, x_{1}x_{2})$. Then construct a horizontal line through $E$ to meet the y-axis in $B'=(0,x_{1}x_{2})$. Construct a line of gradient $-1$ through $B'$ which meets the x-axis at $C$. 

Now consider the all these points and lines under the map $\psi$. Let $\psi(A)=(x_{1}',0)$ and $\psi(B)=(x_{2}',0) $. Lines of gradient $-1$ are mapped to lines of gradient $-1$ since $P$ and $Q$ are fixed. So 
\[
\psi(A')=(0,x_{1}')
\]
Horizontals are mapped to horizontals and verticals are mapped to verticals so
\[
\psi(D)=(1,x_{1}')
\]
Hence the line $L$ is mapped to the line $\psi(L)$ which is given by $y=x_{1}'x$. Again, the vertical line through $B$ is mapped to a vertical line through $\psi(B)$ so 
\[
E=(x_{2}',x_{1}'x_{2}')
\]
and the by the preservation of horizontals, 
\[
\psi(B)=(0,x_{1}'x_{2}')
\]
and by the preservation of lines of gradient $-1$


\[
\psi(C)=(x_{1}'x_{2}',0)
\]

Hence 
\begin{equation*}
\psi(x_{1}x_{2},0)= (x_{1}'x_{2}',0)
\end{equation*}

Define the projection map
\begin{eqnarray*}
P: &R^{2}& \longrightarrow \R \\
&(x,y)& \longmapsto x
\end{eqnarray*}

Consider the induced map $f:\R \longrightarrow \R$ defined by
\[
f(x)=P_{x}(\psi(x,0))
\]
Then by the equations (how do I refer to their labels???) above, $f$ is bijective and has the following properties:
\begin{eqnarray*}
f(0) &=& 0 \\
f(1) &=& 1 \\
f(x+y)&=& f(x)+f(y) \\
f(xy) &=& f(x)f(y)
\end{eqnarray*} 
Now let $n \in \Z$. Then 
\begin{eqnarray*}
f(n) &=& f(1+1+ \cdots +1) \\
&=&f(1)+f(1) + \cdots +f(1) \\
&=& 1+1+ \cdots +1 \\
&=& n
\end{eqnarray*}
Also 
\begin{eqnarray*}
1 &=&f(1) \\
 &=& f\left( n \cdot \frac{1}{n} \right) \\
&=& f(n)f\left( \frac{1}{n} \right) \\
&=& nf\left( \frac{1}{n} \right)
\end{eqnarray*}
Hence 
\[
f \left( \frac{1}{n} \right)=\frac{1}{n}
\]
Now let $m$ and $n$ be integers. So 
\begin{eqnarray*}
f \left(\frac{m}{n}\right)&=&f\left(m \cdot \frac{1}{n}\right)\\
&=& f(m)f\left(\frac{1}{n}\right) \\
&=& m\cdot \frac{1}{n} \\
&=& \frac{m}{n}
\end{eqnarray*}
So $f$ restricted to $\Q$ is the identity. \
In fact, $f$ is continuous.
\begin{proof}
$f$ sends positive numbers to positive numbers and negative numbers to negative numbers since if $a>0$,
\[
f(a)=f(\sqrt{a}\sqrt{a})=f(\sqrt{a})f(\sqrt{a})=f(\sqrt{a})^{2}>0
\]
and by bijectivity if $b<0$, $f(b)<0$.\
Let $\epsilon >0$ and fix an element $y \in \R$. Suppose $|x-y|<\epsilon$. Now if $x<y$ then $0<x-y<\epsilon$. Insert a rational number $z$ between $x-y$ and $\epsilon$ so that
\[
0<x-y<z<\epsilon
\]
Now
\[
f(z-(x-y))=f(z)-f(x-y)=z-f(x-y)>0
\]
since $z-(x-y)>0$ and $f(z)=z$. Also
\[
f(x-y)>0
\]
since $x-y>0$. So $0<f(x-y)<z<\epsilon$, so
\[
0<f(x)-f(y)<\epsilon
\]
On the other hand, if $x \geq y $ then $-\epsilon<x-y \leq 0$. Insert a rational number $z$ between $-\epsilon$ and $x-y$ so that $-\epsilon<z<x-y \leq 0$. Then 
\[
f(z-(x-y))=f(z)-f(x-y)=z-f(x-y)<0
\]
since $z-(x-y)<0$ and $f(z)=z$. Also
\[
f(x-y)\leq 0
\]
since $x-y \leq 0$. So $-\epsilon<z<f(x-y)\leq 0$, so
\[
-\epsilon < f(x)-f(y) \leq 0
\]
In both cases $|f(x)-f(y)|<\epsilon$. Hence the function $f$ is continuous
\end{proof}
Since every real number is a limit of a sequence of rational numbers, and $f$ is continuous, $f$ must be the identity map $\R \longrightarrow \R$. 

The same properties can be shown for the map $\psi$ on the y-axis. And then since every point in the plane is a projection of point on the x and y axes by horizontal and vertical lines, $\psi(x,y)=(x,y)$ for all $(x,y) \in \R^{2}$. Hence $\alpha'_{o}\alpha=\psi=\text{id}$ so $\alpha=\alpha'^{-1}$, which is a linear map since $\alpha'$ is a linear map. 
\end{proof}

\begin{Proposition}
Let $\varphi: \R^{3} \longrightarrow \R^{3}$ be such that $\varphi$ is bijective, sends lines to lines and vertical planes to vertical planes. Then $\varphi$ is an affine map. 
\end{Proposition}  

\begin{proof}
Let there be a line $L$ in $\R^{3}$ given by $$\lambda \mathbf{v_{1}} + (1-\lambda) \mathbf{v_{2}} $$ where the vector $\mathbf{v_{1}}-\mathbf{v_{2}} \notin \text{span} \{ \mathbf{{\bf e}_{3}} \}$. 
Now define a vertical plane $\Pi: \lambda \mathbf{v_{1}} + (1-\lambda) \mathbf{v_{2}} + \mu\mathbf{{\bf e}_{3}}$. $\varphi$ sends vertical planes to vertical planes so define the plane $\Sigma = \varphi(\Pi)$. Then 
\[
\mathbf{v_{1}} \in \Pi
\]
\[
\mathbf{v_{2}} \in \Pi
\]
so 
\[
\varphi(\mathbf{v_{1}}) \in \Sigma
\]
and 
\[
\varphi(\mathbf{v_{2}}) \in \Sigma
\]
Hence the line joining these to points is also in $\Sigma$, in other words
\[
\gamma \varphi(\mathbf{v_{1}}) + (1-\gamma)\varphi(\mathbf{v_{2}}) \in \Sigma
\]
Also $\Sigma$ is vertical, and since $\mathbf{v_{1}}-\mathbf{v_{2}} \notin \text{span}\{ \mathbf{{\bf e}_{3}}\}$, the plane $\Sigma$ is defined by 
\[
\Sigma = \{ \gamma \varphi(\mathbf{v_{1}}) + (1-\gamma)\varphi(\mathbf{v_{2}}) + \delta\mathbf{{\bf e}_{3}} \; \;|\;\; \gamma,\delta \in \R \}
\]
Now each plane $\Pi$ and $\Sigma$ can be given the structure of a two dimensional vector space in the following sense:
Let $O_{\Pi}=\mathbf{v_{2}}$ and $O_{\Sigma}= \varphi(\mathbf{v_{2}})$. Then elements of $\Pi$ can all be expressed as 
\[
(\alpha_{1}, \alpha_{2})_{\Pi}:= \alpha_{1}\mathbf{v_{1}}+(1-\alpha_{1})\mathbf{v_{2}} + \alpha_{2}\mathbf{{\bf e}_{3}}
\]
so that $O_{\Pi}=(0,0)$. 
Elements of $\Sigma$ can similarly be expressed as
\[
(\beta_{1}, \beta_{2})_{\Sigma}= \beta_{1}\varphi(\mathbf{v_{1}})+(1-\beta_{1})\varphi(\mathbf{v_{2}}) + \beta_{2}\mathbf{{\bf e}_{3}}
\]
Thus $\Pi$ and $\Sigma$ have the structure of two-dimensional vector spaces, isomorphic to $\R^2$. Then by the Lemma 2.2.1, since $\varphi(O_{\Pi}=O_{\Sigma}$, and $\varphi$ is bijective and sends lines to lines, $\varphi:\Pi \longrightarrow \Sigma$ is linear. In other words, 
\[
\varphi[(\alpha_{1},\alpha_{2})_{\Pi}+\lambda(\alpha_{1}',\alpha_{2}')_{\Pi}]=\varphi(\alpha_{1},\alpha_{2})_{\Pi} + \lambda \varphi(\alpha_{1}',\alpha_{2}')_{\Pi}
\]
Note also that 
\begin{eqnarray*}
\varphi(1,0)_{\Pi}&=&\varphi(\mathbf{v_{1}}) \\
&=& (1,0)_{\Sigma}
\end{eqnarray*}
So:
\begin{eqnarray*}
\lambda(\mathbf{v_{1}})+(1-\lambda)(\mathbf{v_{2}}) &=& (\lambda,0)_{\Pi} \\
&=&\lambda(1,0)_{\Pi} \\
&=& \varphi[\lambda(1,0)_{\Pi}] \\
&=& \lambda\varphi(1,0)_{\Pi} \\
&=& \lambda(1,0)_{\Sigma} \\
&=&(\lambda,0)_{\Sigma} \\
&=& \lambda\varphi(\mathbf{v_{1}})+(1-\lambda)\varphi(\mathbf{v_{2}})
\end{eqnarray*}
Hence $\varphi$ is affine on $\R^{3}$ (except for the case when v1-v2 is vertical).
\end{proof}

\begin{Proposition}
Let $\varphi$ be an affine map from $\R^{n}$ to $\R^{n}$. So $\varphi$ preserves affine combinations. Then the map $\phi$ defined by 
\[
\phi(x)=\varphi(x)-\varphi(0)
\]
is a linear map.
\end{Proposition}

\begin{proof}
Let $\lambda \in \R$ and $x$ and $y \in \R^{n}$. Then 
\begin{eqnarray*}
\phi(\lambda x) &=& \varphi(\lambda x) - \varphi(0) \\
&=& \varphi(\lambda x +(1-\lambda)0)-\varphi(0) \\
&=& \lambda \varphi(x)+(1-\lambda)\varphi(0) -\varphi(0) \\
&=& \lambda \varphi(x)-\lambda\varphi(0) \\
&=& \lambda (\varphi(x)-\varphi(0)) \\
&=& \lambda \phi(x) 
\end{eqnarray*}

and

\begin{eqnarray*}
\phi(x+y) &=& \varphi(x+y)-\varphi(0) \\
&=& \varphi(\lambda x' + (1-\lambda)y') -\varphi(0) \\
&=& \lambda \varphi( x') + (1-\lambda)\varphi(y') -\varphi(0) \\
&=& \lambda \varphi(\frac{1}{\lambda} x) + (1-\lambda)\varphi(\frac{1}{1-\lambda} y') -\varphi(0) \\
&=& \lambda (\varphi(\frac{1}{\lambda} x) -\varphi(0)) + (1-\lambda)(\varphi(\frac{1}{1-\lambda} y')-\varphi(0))  \\
&=& \lambda (\phi(\frac{1}{\lambda} x) ) + (1-\lambda)(\phi(\frac{1}{1-\lambda} y'))  \\
&=& \lambda \frac{1}{\lambda} \phi( x)  + (1-\lambda)\frac{1}{1-\lambda} \phi( y')  \\
&=& \phi(x)+\phi(y)
\end{eqnarray*}
Hence $\phi$ is a linear map.
\end{proof} 



\section{Restriction obtained from preservation of connected cosets}

\begin{Theorem}\label{preserve closed cosets}
Suppose that $\Psi: \G \longrightarrow \G$ is bijective and sends every coset of any Lie subgroup of $\G$ to some coset of some Lie subgroup of $\G$. Then $\Phi:\G \longrightarrow \G$ defined by $\Phi(X)=\Psi(0)^{-1}\Psi(X)$ also preserves these cosets and the induced map $\phi: \g \longrightarrow \g$ defined by $\phi(x)=\log \Phi(\exp(x))$ is a linear map of $\g$ given by a matrix of the form

$\phi =\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & g
\end{bmatrix}$,
$ \det(\phi) \neq 0 $

\end{Theorem}


\begin{proof}
Observe that 
\begin{eqnarray*}
\phi(0_{\g})&=&\log \Phi (\exp(0)) \\
&=& \log \Phi(0) \\
&=& \log \Psi(0)^{-1}\Psi(0) \\
&=& \log 0_{\G} \\
&=& 0_{\g}
\end{eqnarray*}

Then since $\phi(0)=0$, A must send subalgebras of $\g$ to subalgebras of $\g$ bijectively (since every subalgebra contains $0$). What are these subalgebras?

A subalgebra of $\g$ is a vector subspace which is closed under the Lie bracket. 

%What are the subalgebras and Subgroups and Cosets?
\subsection{One-dimensional Subalgebras}
\begin{Proposition}
All one-dimensional subspaces of $\g$ are subalgebras.
\end{Proposition}

\begin{proof}
All one-dimensional subspaces are lines through the origin. Let $S$ be a one-dimensional subspace of $\g$. Then 
\[
S= \text{span}(X)
\]
for some $X=(x,y,z) \in \g$. Let $\lambda X$ and $\mu X$ be elements of $S$. Then 
\begin{eqnarray*}
[\lambda X,\mu X] &=& [(\lambda x, \lambda y, \lambda z), (\mu x, \mu y, \mu z)] \\
&=& (0,0, \lambda x \mu y - \lambda y \mu x) \\
&=& (0,0, \lambda  \mu xy - \lambda \mu xy) \\
&=& (0,0,0) \in S
\end{eqnarray*}
Hence every one-dimensional subspace of $S$ is closed under the Lie bracket and is thus also a one-dimensional subalgebra. 
\end{proof}

\subsection{two-dimensional subalgebras}
Let $Z=\text{span}(0,0,1)$ denote the z-axis.
The question is now whether all two-dimensional subspaces of $\g$ are also subalgebras. Two-dimensional subspaces of $\g$ are planes through the origin. So any subspace is the span of two linearly independent vectors in $\g$.
Let $T= \text{span}(X ,Y)$ where $X=(x_{1},x_{2},x_{3})$ and $Y=(y_{1},y_{2},y_{3})$ are distinct elements of $\g$.

Then  
\begin{eqnarray*}
  [X,Y] & \in T \\
(0,0, x_{1}y_{2}-x_{2}y_{1}) & \in T
\end{eqnarray*}
   
This restriction can be analysed in the following two distinct cases:

\begin{enumerate}
 \item $x_{1}y_{2}=x_{2}y_{1}$
 \item $x_{1}y_{2}\neq x_{2}y_{1}$
\end{enumerate}

In case (1), 
\[
x_{1}y_{2}=x_{2}y_{1}
\]
First consider the sub-case in which $x_{1}=x_{2}=0$ Then it is not possible to have both $x_{2}=0$ and $y_{2}=0$ since this would make $X$ and $Y$ linearly dependent. So without loss of generality select $y_{2} \neq 0$. Then there exists $k \in \R$ such that 
\[
x_{2} = ky_{2}
\]
So 
\[
X-kY=(0,0,x_{3}-ky_{3}) \in T
\]
Note that $X-kY \neq 0$ since otherwise $X$ and $Y$ would be linearly dependent. So $T$ contains $\text{span}(0,0,x_{3}-ky_{3})=Z$, and hence $T$ is a vertical plane. 

Now suppose at least one of $x_{1}$ and $y_{1}$ is non-zero. Without loss of generality suppose $y_{1} \neq 0$.
Then there exists some $\lambda \in \R$ such that $x_{1} = \lambda y_{1}$. So

\begin{eqnarray*}
 \lambda y_{1}y_{2} &=& x_{2} y_{1}\\
 \lambda y_{2} &=& x_{2}
\end{eqnarray*}

Hence
\begin{eqnarray*}
 X-\lambda Y & = & (x_{1},x_{2},x_{3})-(\lambda y_{1},\lambda y_{2},\lambda y_{3}) \\
	     & = & (0,0,x_{3}-\lambda y_{3}) \\
	     & \neq & 0
\end{eqnarray*}
since $X$ and $Y$ are linearly independent. 
Therefore 
\[
Z=\text{span}(0,0,1)=\text{span}(0,0,x_{3}-\lambda y_{3}) \in T
\]

In the second case, $x_{1}y_{2}\neq x_{2}y_{1}$ so 
\[
\text{span}(0,0, x_{1}y_{2}- x_{2}y_{1})= Z \in T
\]

Hence any plane through the origin in $\g$ which is closed under the Lie bracket must contain the z-axis. In other words, all two-dimensional subalgebras of $\g$ are vertical planes through the origin.


%Using this propostion will mean changing the way the rest of this result is argued
%Need to explain why preservation of cosets in $\G$ implies preservation of cosets in $\g$.
\begin{Proposition}
Let $C$ be a coset of some subgroup $S$ of $\G$. Then $C$ is the image of a translate of a subalgebra of $\g$ under the exponential map. In other words, preserving cosets of Lie subgroups in $\G$ corresponds to preserving translates of subalgebras of $\g$.
\end{Proposition}

\begin{proof}
Let $X=(x,y,z) \in \g$ and so that $S=\{ \lambda X | \lambda \in \R \}$ is a one-dimensional subalgebra of $\g$. $e^{S}$ is the corresponding Lie subgroup of $\G$. i.e.
\begin{eqnarray*}
e^{S} &=& \{ e^{\lambda X}| \lambda \in \R \} \\
&=& \left\lbrace \begin{bmatrix}
1 & \lambda x & \lambda z + \frac{1}{2} \lambda^{2}xy \\
0 & 1 & \lambda y \\
0 & 0 & 1
\end{bmatrix} \bigg| \lambda \in \R \right\rbrace
\end{eqnarray*}
Let C be the coset of $e^{S}$ given by right translation by $e^{P}$, for a fixed element $P = \begin{bmatrix}
0 & p & r \\
0 & 0 & q \\
0 & 0 & 0
\end{bmatrix} \in \g$. Then 
\begin{eqnarray*}
C &=& \{e^{\lambda X}e^{P}|\lambda \in \R \} \\
&=& \{ \exp( \lambda X + P + \frac{1}{2}[\lambda X,P]) | \lambda \in \R \} \\
&=& \left\lbrace \exp \left( 
\begin{bmatrix}
0 & \lambda x & \lambda z \\
0 & 0 & \lambda y \\
0 & 0 & 0
\end{bmatrix} + 
\begin{bmatrix}
0 & p & r \\
0 & 0 & q \\
0 & 0 & 0
\end{bmatrix} + 
\frac{1}{2}
\begin{bmatrix}
0 & 0 & \lambda xq - \lambda yp \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}
\right)
 \bigg| \lambda \in \R \right\rbrace \\
&=& \left\lbrace \exp
\begin{bmatrix}
0 & \lambda x + p & \lambda (z+\frac{1}{2}(xq-yp))+r \\
0 & 0 & \lambda y +q \\
0 & 0 & 0
\end{bmatrix}
\bigg| \lambda \in \R \right\rbrace
\end{eqnarray*}
Now let $L \subseteq \g$ be the image of $C$ under the logarithm map.

\begin{eqnarray*}
L &=& \left\lbrace
\begin{bmatrix}
0 & \lambda x + p & \lambda (z+\frac{1}{2}(xq-yp))+r \\
0 & 0 & \lambda y +q \\
0 & 0 & 0
\end{bmatrix}
\bigg| \lambda \in \R \right\rbrace \\
&=& \left\lbrace \lambda
\begin{bmatrix}
0 &  x & z+\frac{1}{2}(xq-yp) \\
0 & 0 & y \\
0 & 0 & 0
\end{bmatrix} +
\begin{bmatrix}
0 & p & r \\
0 & 0 & q \\
0 & 0 & 0
\end{bmatrix}
\bigg| \lambda \in \R \right\rbrace \\
&=& \{ \lambda (x,y,z+\frac{1}{2}(xq-yp))+P | \lambda \in \R \}
\end{eqnarray*}

So the one-dimensional coset $C$ corresponds to a line $L$ in $\g$ which is a translate of a one-dimensional subalgebra, $\text{span}\{(x,y,z+\frac{1}{2}(xq-yp))\}$ by the element $P$. Note that this subalgebra is not the same as the subalgebra corresponding to the Lie subgroup in $\G$ from which the coset $C$ was originally generated. 

A similar argument will now demonstrate that two-dimensional cosets in $\G$ do indeed correspond to vertical planes in $\g$.
Suppose $X=(x,y,z) \in \g$ is not vertical. So there is a vertical plane $\mathbf{\pi}$ given by 
\[
\mathbf{\pi} = \{\lambda X + \mu \3 \;\;\;|\;\;\; \lambda,\mu \in \R \}
\]
Then the corresponding Lie subgroup of $\mathbf{\pi}$ is 
\[
\Pi = \{\exp(\lambda X + \mu \3) \;\;\;|\;\;\; \lambda,\mu \in \R \}
\]
A coset of this Lie subgroup is $C$ given by right translation by $e^{P}$ as before. So 
\begin{eqnarray*}
C &=& \{\exp(\lambda X + \mu \3)\exp (P) \;\;\;|\;\;\; \lambda,\mu \in \R \} \\
&=& \{ \exp(\lambda x, \lambda y, \lambda z +\mu)\exp(p,q,r)\;\;\;|\;\;\; \lambda,\mu \in \R \} \;\;\;\textit{applying the CBH Formula:}\\
&=&  \{ \exp((\lambda x, \lambda y, \lambda z +\mu)+(p,q,r)+\frac{1}{2}[(\lambda x, \lambda y, \lambda z +\mu),(p,q,r)])\;\;\;|\;\;\; \lambda,\mu \in \R \} \\
&=& \{ \exp ((\lambda x+p, \lambda y+q, \lambda z +\mu +r)+ \frac{1}{2}(0,0,\lambda xq -\lambda yp))\;\;\;|\;\;\; \lambda,\mu \in \R \} \\
&=& \{ \exp (\lambda x+p, \lambda y+q, \lambda (z+\frac{1}{2}(xq-yp)) +\mu +r)\;\;\;|\;\;\; \lambda,\mu \in \R \} \\
&=& \{ \exp [\lambda( x, y, z+\frac{1}{2}(xq-yp))+\mu(0,0,1)+(p,q,r)]\;\;\;|\;\;\; \lambda,\mu \in \R \}
\end{eqnarray*}

Now suppose that $\pi'$ is the image of this coset $C$ under the logarithm map. So 

\begin{eqnarray*}
\pi' &=& \{ \lambda( x, y, z+\frac{1}{2}(xq-yp))+\mu(0,0,1)+(p,q,r) \;\;\;|\;\;\; \lambda,\mu \in \R \} \\
&=& P+\text{span}\{Y,\3 \}
\end{eqnarray*}
where $Y=X+[X,P]=( x, y, z+\frac{1}{2}(xq-yp))$. \\
So $\pi'$ is yet another vertical plane in $\g$. Note however that unless $[X,P]=0$, $\pi'$ is not the same as $\pi$. Most importantly, if a bijective map from $\G$ to $\G$ preserves cosets, then the corresponding map from $\g$ to $\g$ preserves lines and vertical planes (translates of subalgebras).
\end{proof}

% may have to alter this to suit the real assumptions of the problem, ie that may need to get rid of translation bit in the group before moving to the algebra.
\subsection{Cosets}
The cosets in $\g$ are either lines or vertical planes. Hence the bijective map $\phi$ sends lines to lines and vertical planes to vertical planes. Therefore by Proposition [?] $\phi$ is an affine map on the vector space $\g$, and because $\phi$ maps the identity to the identity, Proposition 2.2.4 indicates it must be linear.


%-use preservation of vertical planes to show that the linear portion of the %map takes on that slightly restricted form
Now, suppose $S$ and $T$ are distinct vertical planes through the origin in $\g$. So by bijectivity $\phi(S) \neq \phi(T)$ and $\phi(S \cup T)=\phi(S) \cup \phi(T)$. Hence 
\[
\phi(Z)=Z
\]
If $\phi$ is given by the matrix 
$\begin{bmatrix}
a & b & h \\
c & d & i \\
e & f & g
\end{bmatrix}
$, with $\det(\phi) \neq 0$ then 

\[
\begin{bmatrix}
a & b & h \\
c & d & i \\
e & f & g
\end{bmatrix} 
\begin{bmatrix}
0 \\ 0 \\ 1
\end{bmatrix} =
\begin{bmatrix}
h \\ i \\ g
\end{bmatrix} 
\in Z
\]
Hence $h=i=0$ and 
\[
\phi=
\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & g
\end{bmatrix}
\]

\end{proof}



% Now to the integer lattice:

\section{Restriction obtained from preservation of discrete cosets}
Without assuming that the map preserves discrete cosets of $G$, this restriction may be the strongest one that can be ascertained. However, by assuming the map $\Phi$ also sends cosets of discrete subgroups to cosets of discrete subgroups, stronger restrictions may be obtained. Note that discrete subgroups are not Lie groups. They are not locally homeomorphic to $\R^{n}$. 

\begin{Theorem}
Suppose that $\Psi: \G \longrightarrow \G$ is bijective and sends every coset of \emph{any} subgroup of $\G$ to some coset of some subgroup of $\G$. Then $\Psi=L \circ \Phi$ where $\Phi:\G \longrightarrow \G$ is either an isomorphism or an anti-isomorphism and $L: \G \longrightarrow \G$ is a translation. Specifically, for any $x \in \G$
\[
L(x)=\Psi(0)^{-1}x
\]

\end{Theorem}

Consider the elements $E_{1}=[1,0,0]$, $E_{2}=[0,1,0]$ and $E_{3}=[0,0,1]$ of $\G$. Their images under the logarithm map are ${\bf e}_{1}=(1,0,0)$, ${\bf e}_{2}=(0,1,0)$ and ${\bf e}_{3}=(0,0,1)$ in $\g$.\
Now with $\phi$ given in the matrix form above,
\begin{eqnarray*}
\phi {\bf e}_{1} &=& (a,c,e) \\
\phi {\bf e}_{2} &=& (b,d,f) \\
\phi {\bf e}_{3} &=& (0,0,g)
\end{eqnarray*}
Since ${\bf e}_{1}$ and ${\bf e}_{2}$ lie in different two-dimensional subalgebras of $\g$, their images under $\phi$ must lie in different two-dimensional subalgebras. [picture]This means that the projected vectors $(a,c)$ and $(b,d)$ are linearly independent in $\R^{2}$ so 
\begin{eqnarray*}
\frac{a}{b} &\neq & \frac{c}{d} \\
ad &\neq & bc\\
ad-bc &\neq & 0 
\end{eqnarray*} Let $\Delta = ad-bc$.
Now consider the linear map $\beta:\g \longrightarrow \g$ given in matrix form by 
\begin{eqnarray*}
\beta &=& \begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & \Delta
\end{bmatrix}^{-1} \\
&=& \begin{bmatrix}
\frac{d}{\Delta} & \frac{-b}{\Delta} & 0 \\
\frac{-c}{\Delta} & \frac{a}{\Delta} & 0 \\
\frac{cf-ed}{\Delta^{2}} & \frac{eb-af}{\Delta^{2}} & \frac{1}{\Delta}
\end{bmatrix}
\end{eqnarray*}

Now define a new map $\tau: \g \longrightarrow \g$ as the composition of $\phi$ and $\beta$. So 
\begin{eqnarray*}
\tau &=& \beta \circ  \phi \\
&=& \begin{bmatrix}
\frac{d}{\Delta} & \frac{-b}{\Delta} & 0 \\
\frac{-c}{\Delta} & \frac{a}{\Delta} & 0 \\
\frac{cf-ed}{\Delta^{2}} & \frac{eb-af}{\Delta^{2}} & \frac{1}{\Delta}
\end{bmatrix}
\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & g
\end{bmatrix} \\
&=& 
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & g/\Delta
\end{bmatrix}
\end{eqnarray*}

Let the corresponding bijective maps, $B$ and $T$ from $\G$ to $\G$ be given by
\begin{eqnarray*}
B(e^{x}) &=& e^{\beta(x)} \\
T(e^{x}) &=& e^{\tau (x)} \\
& & \forall x \in \g
\end{eqnarray*}


\begin{Proposition}
If $g=\Delta$, $\phi$ is a Lie algebra homomorphism.
\end{Proposition}

\begin{proof}
\[
\phi = 
\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & \Delta
\end{bmatrix}
\]
Let $x=(x_{1},x_{2},x_{3})$ and $y=(y_{1},y_{2},y_{3})$ be any two elements of $\g$. Then $[x,y]=(0,0,x_{1}y_{2}-x_{2}y_{1})$. 
\begin{eqnarray*}
[\phi(x),\phi(y)] &=& \left[
\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & \Delta
\end{bmatrix} 
\begin{bmatrix}
x_{1} \\ x_{2} \\ x_{3}
\end{bmatrix}
,
\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & \Delta
\end{bmatrix} 
\begin{bmatrix}
y_{1} \\ y_{2} \\ y_{3}
\end{bmatrix}
\right] \\
&=& \left[
\begin{bmatrix}
ax_{1}+bx_{2} \\ cx_{1}+dx_{2} \\ ex_{1}+fx_{2} + \Delta x_{3}
\end{bmatrix}
,
\begin{bmatrix}
ay_{1}+by_{2} \\ cy_{1}+dy_{2} \\ ey_{1}+fy_{2} + \Delta y_{3}
\end{bmatrix}
\right] \\
&=& (0,0,(ax_{1}+bx_{2})(cy_{1}+dy_{2})-(ay_{1}+by_{2})(cx_{1}+dx_{2})) \\
&=& (0,0,acx_{1}y_{1} - acx_{1}y_{1} + bdx_{2}y_{2} - bdx_{2}y_{2} + adx_{1}y_{2} + bcx_{2}y_{1} -adx_{2}y_{1} -bcx_{1}y_{2} ) \\
&=& (0,0, adx_{1}y_{2}  -adx_{2}y_{1}+ bcx_{2}y_{1} -bcx_{1}y_{2} ) \\
&=& (0,0,ad(x_{1}y_{2}  -x_{2}y_{1})+ bc(x_{2}y_{1} -x_{1}y_{2})) \\
&=& (0,0,(ad-bc)(x_{1}y_{2}  -x_{2}y_{1})) \\
&=& (0,0,\Delta(x_{1}y_{2}-x_{2}y_{1})) \\
&=& \phi(0,0,x_{1}y_{2}-x_{2}y_{1}) \\
&=& \phi[x,y] 
\end{eqnarray*}
\end{proof}

\begin{Proposition}
If $g=-\Delta$, $\phi$ is a Lie algebra anti-homomorphism.
\end{Proposition}

\begin{proof}
\[
\phi = 
\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & -\Delta
\end{bmatrix}
\]
Let $x=(x_{1},x_{2},x_{3})$ and $y=(y_{1},y_{2},y_{3})$ be any two elements of $\g$. Then $[x,y]=(0,0,x_{1}y_{2}-x_{2}y_{1})$. 
\begin{eqnarray*}
[\phi(x),\phi(y)] &=& \left[
\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & -\Delta
\end{bmatrix} 
\begin{bmatrix}
x_{1} \\ x_{2} \\ x_{3}
\end{bmatrix}
,
\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & -\Delta
\end{bmatrix} 
\begin{bmatrix}
y_{1} \\ y_{2} \\ y_{3}
\end{bmatrix}
\right] \\
&=& \left[
\begin{bmatrix}
ax_{1}+bx_{2} \\ cx_{1}+dx_{2} \\ ex_{1}+fx_{2} - \Delta x_{3}
\end{bmatrix}
,
\begin{bmatrix}
ay_{1}+by_{2} \\ cy_{1}+dy_{2} \\ ey_{1}+fy_{2} - \Delta y_{3}
\end{bmatrix}
\right] \\
&=& (0,0,(ax_{1}+bx_{2})(cy_{1}+dy_{2})-(ay_{1}+by_{2})(cx_{1}+dx_{2})) \\
&=& (0,0,acx_{1}y_{1} - acx_{1}y_{1} + bdx_{2}y_{2} - bdx_{2}y_{2} + adx_{1}y_{2} + bcx_{2}y_{1} -adx_{2}y_{1} -bcx_{1}y_{2} ) \\
&=& (0,0, adx_{1}y_{2}  -adx_{2}y_{1}+ bcx_{2}y_{1} -bcx_{1}y_{2} ) \\
&=& (0,0,ad(x_{1}y_{2}  -x_{2}y_{1})+ bc(x_{2}y_{1} -x_{1}y_{2})) \\
&=& (0,0,(ad-bc)(x_{1}y_{2}  -x_{2}y_{1})) \\
&=& (0,0,\Delta(x_{1}y_{2}-x_{2}y_{1})) \\
&=& -\phi(0,0,x_{1}y_{2}-x_{2}y_{1}) \\
&=& -\phi[x,y] 
\end{eqnarray*}
\end{proof}

\begin{Proposition}
$\Phi$ is a Lie group homomorphism $\iff$ $\phi$ is a Lie algebra homomorphism, and $\Phi$ is a Lie group anti-homomorphism $\iff$ $\phi$ is a Lie algebra anti-homomorphism
\end{Proposition}

\begin{proof}
Suppose $\phi$ is a Lie algebra homomorphism. So $\phi [x,y]=[\phi(x),\phi(y)]$ for all $x$ and $y$ in $\g$. Suppose $X=\exp(x)$ and $Y=\exp(y)$. Then
\begin{eqnarray*}
\Phi(XY) &=& \Phi( \exp (x+y+\frac{1}{2}[x,y])) \\
&=& \exp( \phi(x+y+\frac{1}{2}[x,y])) \\
&=& \exp( \phi(x)+\phi(y) +\frac{1}{2} \phi[x,y])\;\;\; \text{(since $\phi$ is linear)} \\
&=& \exp ( \phi(x)+\phi(y) +\frac{1}{2} [\phi(x),\phi(y)]) \\
&=& \exp( \phi (x))\exp(\phi(y))\\
&=& \Phi(\exp(x))\Phi(\exp(y))\\
&=& \Phi(X)\Phi(Y)
\end{eqnarray*}

On the other hand, suppose $\phi$ is a Lie algebra anti-homomorphism. So $\phi [x,y]=-[\phi(x),\phi(y)]$. Then
\begin{eqnarray*}
\Phi(XY) &=& \Phi( \exp (x+y+\frac{1}{2}[x,y])) \\
&=& \exp( \phi(x+y+\frac{1}{2}[x,y])) \\
&=& \exp( \phi(x)+\phi(y) +\frac{1}{2} \phi[x,y])\;\;\; \text{(since $\phi$ is linear)} \\
&=& \exp ( \phi(x)+\phi(y) -\frac{1}{2} [\phi(x),\phi(y)]) \\
&=& \exp ( \phi(y)+\phi(x) +\frac{1}{2} [\phi(y),\phi(x)]) \\
&=& \exp( \phi (y))\exp(\phi(x))\\
&=& \Phi(\exp(y))\Phi(\exp(x))\\
&=& \Phi(Y)\Phi(X)
\end{eqnarray*}
\end{proof}

%Need to change order of the proof so that g=Delta -> isomorphism is done first. Then it can be shown that T maps subgroups to subgroups
\begin{Proposition}
$T$ maps subgroups of $\G$ to subgroups of $\G$.
\end{Proposition}
\begin{proof}
Since $\beta^{-1}$ is in the same form as $\phi$ but with $g=\Delta$, the previous two propositions show that $\beta^{-1}$ is a Lie algebra homomorphism. Since it is also bijective, it is a Lie algebra isomorphism. Therefore $\beta$ must also be a Lie algebra isomorphism. 

\begin{Lemma}
$B$ preserves subgroups. 
\end{Lemma}
\begin{proof}
Let $S$ be any subgroup of $\G$ and $\mathfrak{s}$ be its image under the logarithmic map. Let $Y_{1}$ and $Y_{2}$ be elements of $B(S)$. So there exist $X_{1}=\exp(x_{1})$ and $X_{2}=\exp(x_{2})$ in $S$ such that $Y_{1}=B(X_{1})$ and $Y_{2}=B(X_{2})$. Then 
\begin{align*}
Y_{1}Y_{2} &= B(X_{1})B(X_{2}) \\
&= B(X_{1}X_{2}) 
\end{align*}
which is in $B(S)$ because $S$ is a subgroup so $X_{1}X_{2} \in S$. Hence $B(S)$ is closed under the group operation. Moreover, $X_{1}^{-1} \in S$ so
\begin{align*}
Y_{1}^{-1} &=B(X_{1})^{-1} \\
&= B(X_{1}^{-1}) \\
&= \in B(S)
\end{align*}
Also,
$0_{\G} \in S$ so $B(0_{\G})=0_{\G} \in B(S)$. Hence $B(S)$ is a subgroup.

%&= B(\exp(x_{1}))B(\exp(x_{2})) \\
%&= \exp(\beta(x_{1}))\exp(\beta(x_{2}))\\
%&= \exp(\beta(x_{1})+\beta(x_{2})+\frac{1}{2}[\beta(x_{1}),\beta(x_{2})])\\
%&= \exp(\beta(x_{1})+\beta(x_{2})+\frac{1}{2}\beta[x_{1},x_{2}])\\
%&= \exp(\beta(x_{1}+x_{2}+\frac{1}{2}[x_{1},x_{2}]))\\
%&= B(\exp(x_{1}+x_{2}+\frac{1}{2}[x_{1},x_{2}]))\\
%&= B(\exp(x_{1})\exp(x_{2}))\\
\end{proof}

Now since $T$ is composed of $B$ and $\Phi$, which both preserve subgroups, $T$ must also preserve subgroups.
\end{proof}

\begin{Proposition}
If $\langle E_{1},E_{2} \rangle$ denotes the smallest subgroup of $\G$ containing the elements $E_{1}$ and $E_{2}$, then 
\[
\langle E_{1},E_{2} \rangle = 
\left\lbrace
\begin{bmatrix}
1 & x & z \\
0 & 1 & y \\
0 & 0 & 1
\end{bmatrix} \bigg|x,y,z \in \Z
\right\rbrace (\text{denote this set}\;\G_{int})
\]
\end{Proposition}

\begin{proof}
Note that 
\begin{eqnarray*}
E_{3}=[0,0,1]&=&
\begin{bmatrix}
1 & 1 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 1 \\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
1 & -1 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & -1 \\
0 & 0 & 1
\end{bmatrix}
 \\
&=& E_{1}E_{2}E_{1}^{-1}E_{2}^{-1}
\end{eqnarray*}
Take an element from $\G_{int}$, $[x,y,z]$, where $x$, $y$ and $z$ are integers. Then 
\begin{eqnarray*}
[x,y,z] &=&[x,y,0][0,0,z-xy] \\
&=& [x,0,0][0,y,0][0,0,z-xy] \\
&=& E_{1}^{x}E_{2}^{y}E_{3}^{z-xy}
\end{eqnarray*}
So $\G_{int} \in \langle E_{1},E_{2} \rangle $. Conversely any element of $\langle E_{1},E_{2} \rangle$ contains only integer entries since multiplying integers or taking their negatives gives integers. So $\langle E_{1},E_{2} \rangle \in \G_{int}$. Hence $\langle E_{1},E_{2} \rangle=\G_{int}$.
\end{proof}

\begin{Lemma}
$T(\G_{int})=\G_{int}$. 
\end{Lemma}

\begin{proof}
Note the following:
\begin{eqnarray*}
T(E_{1})= e^{\tau ({\bf e}_{1})} &=& e^{{\bf e}_{1}} = E_{1} \\
T(E_{2})= e^{\tau ({\bf e}_{2})} &=& e^{{\bf e}_{2}} = E_{2} \\
T^{-1}(E_{1})= e^{\tau^{-1} ({\bf e}_{1})} &=& e^{{\bf e}_{1}} = E_{1} \\
T^{-1}(E_{2})= e^{\tau^{-1} ({\bf e}_{2})} &=& e^{{\bf e}_{2}} = E_{2} 
\end{eqnarray*}

Consider also the following fact. For $X$ and $Y$ in $\G$, $T( \langle X,Y \rangle )$ is a subgroup, which contains $T(X)$ and $T(Y)$ so by minimality, $\langle T(X),T(Y) \rangle \subseteq T( \langle X,Y \rangle )$
Hence
\begin{eqnarray*}
\langle E_{1},E_{2} \rangle = \langle T(E_{1}),T(E_{2}) \rangle &\subseteq & T( \langle E_{1},E_{2} \rangle) \\
\langle E_{1},E_{2} \rangle = \langle T^{-1}(E_{1}),T^{-1}(E_{2}) \rangle & \subseteq & T^{-1}( \langle E_{1},E_{2} \rangle) \\
\implies T( \langle E_{1},E_{2} \rangle) & \subseteq & T(T^{-1}( \langle E_{1},E_{2} \rangle))\\
&=&\langle E_{1},E_{2} \rangle
\end{eqnarray*}
and hence 
\[
T( \langle E_{1},E_{2} \rangle)= \langle E_{1},E_{2}\rangle
\]
giving the result.
\end{proof}

We now need to consider what happens to the unit vector $(0,0,1)={\bf e}_{3}$ in $\g$ by considering what happens to $[0,0,1]=E_{3}=\exp({\bf e}_{3})$ in the group.

\[
T(E_{3})= \exp(\tau ({\bf e}_{3}))=\exp(0,0,\frac{g}{\Delta})=[0,0,\frac{g}{\Delta}]
\]
$T$ maps the centre of $\G$, $Z$ to itself (as do $B$ and $\Phi$) since 
\[
T[0,0,z]= \exp(\tau(0,0,z))= \exp(0,0,\frac{gz}{\Delta})=[0,0,\frac{gz}{\Delta}] \in Z
\]. So combining this result with the previous Lemma,
\[
T( Z \cap \G_{int})=Z \cap \G_{int}
\]
and hence 
\begin{eqnarray*}
T(E_{3}) &\in & Z \cap \G_{int} \\
\left[ 0,0,\frac{g}{\Delta} \right] &\in & Z \cap \G_{int} \\
\implies \frac{g}{\Delta} &\in & \Z
\end{eqnarray*}

Now we move into the Lie algebra to use the linearity properties of $\tau$.
Let 
\begin{eqnarray*}
D &=& \left\lbrace (0,0,m)\; \big| \;\; m \in \Z \right\rbrace \subseteq \g \\
\exp (D) &=& \left\lbrace [0,0,m] \; \big| \;\; m \in \Z \right\rbrace \subseteq \G \\
&=& Z \cap \G_{int}
\end{eqnarray*}

Now consider the image of this set of integer points on the z-axis under $\tau$:
\begin{eqnarray*}
\tau(D)&=& \log(\exp (\tau(D))) \\
&=& \log T(\exp (D)) \\
&=& \log T(Z \cap \G_{int}) \\
&=& \log (Z \cap \G_{int}) \\
&=& \log \exp (D) \\
&=& D
\end{eqnarray*}
So $\tau$ is a bijection when restricted to $D$. Linearity of $\tau$ gives the following for any integer $n$.
\begin{eqnarray*}
\tau (0,0,n) &=& n \tau (0,0,1) \\
&=& n\left(0,0, \frac{g}{\Delta}\right) \\
&=& \left(0,0, n \frac{g}{\Delta}\right)
\end{eqnarray*}
so $g/\Delta \in \Z$. But then considering the surjectivity of $\tau$ on $D$, ${\bf e}_{1}$ must be the image of some element of $D$ under $\tau$. In other words $(0,0,1)=\tau(0,0,n)$ for some $n \in \Z$ but from the previous result this gives 
\[
(0,0,1)=\left(0,0, n\left(\frac{g}{\Delta}\right)\right)
\]
where $n$ and $g/\Delta$ are both integers. Hence $n \left( \frac{g}{\Delta}\right)=1$ so 
\[
\frac{g}{\Delta}= \pm 1
\]
which gives
\[
g=\pm \Delta.
\]
Hence $\phi$ and $\Phi$ are either a pair of isomorphisms or anti-isomorphims and $\Psi$ is a composition of a translation and either an isomorphism or an anti-isomorphism.

\section{Summary}
The weaker assumption that $\Psi$ preserves cosets of Lie subgroups ensures $\phi$ is a linear map which sends the z-axis to itself. The stronger assumption that $\Psi$ preserves cosets of any subgroups ensures that $\Phi$ is either a Lie group isomorphism or a Lie group anti-isomorphism.\
The next chapter of this project will attempt to prove similar results for any upper-triangular nilpotent Lie group using induction. 

\chapter{Case of all upper triangular nilpotent Lie groups}


\end{document}
