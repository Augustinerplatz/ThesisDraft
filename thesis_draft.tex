\documentclass[honours]{UNSWthesis}
\linespread{1}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{latexsym,amsmath}
\usepackage{graphicx}

%% define some macros
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\G}{\mathcal{G}}
%\newcommand{\H}{\mathcal{H}}
\newcommand{\g}{\mathfrak{g}}
\newcommand{\1}{\mathbf{e}_{1}}
\newcommand{\2}{\mathbf{e}_{3}}
\newcommand{\3}{\mathbf{e}_{3}}

\DeclareMathOperator{\image}{image}
\DeclareMathOperator{\alg}{Alg}
\DeclareMathOperator{\spn}{span}


%% new environments

\newcounter{Item}[section]
%\newenvironment{proof}{\noindent {\bf Proof.}\ }{\qed}
\newenvironment{Definition}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Definition \thesection.\theItem.}\ }
                           {\medskip}
\newenvironment{Notation}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Notation \thesection.\theItem.}\ }
                           {\medskip}
\newenvironment{Theorem}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Theorem \thesection.\theItem.}\ %
                            \begingroup \sl}
                           {\endgroup\medskip}
\newenvironment{Proposition}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Proposition \thesection.\theItem.}\ %
                            \begingroup \sl}
                           {\endgroup\medskip}
\newenvironment{Corollary}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Corollary \thesection.\theItem.}\ %
                            \begingroup \sl}
                           {\endgroup\medskip}
\newenvironment{Lemma}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Lemma \thesection.\theItem.}\ %
                            \begingroup \sl}
                           {\endgroup\medskip}
\newenvironment{Conjecture}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Conjecture \thesection.\theItem.}\ %
                            \begingroup \sl}
                           {\endgroup\medskip}
\newenvironment{Example}{\medskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Example \thesection.\theItem.}\ }
                           {\qed}
\newenvironment{Remark}{\smallskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Remark \thesection.\theItem.}\ }
                           {\qed}
\newenvironment{Question}{\smallskip
                            \refstepcounter{Item}
                            \noindent
                           {\bf Question \thesection.\theItem.}\ }
                           {\par}
\newenvironment{theoremlist}{\begin{list}{}
                        {\setlength{\parsep}{0pt}
                        \setlength{\topsep}{\smallskipamount}} }
                        {\end{list}}

\title{Rigidity of Coset-Preserving Maps on Upper-Triangular Nilpotent Lie Groups}

\authornameonly{Richard Tierney}

\author{\Authornameonly\\{\bigskip}Supervisor: Professor Michael Cowling}

\begin{document}
\maketitle

\prefacesection{Acknowledgements}
{\noindent}Many thanks go to Michael Cowling, Georgia Tsambos, Andrew Ardill...

\prefacesection{Introduction}

Lie groups are to be found in many different applications of mathematics as well as being central to many of the ideas in quantum physics. It is therefore very useful to know whether different Lie groups linked by a certain map are similar in structure, or whether a particular map identifies two Lie groups in a useful or sensible way. This is the idea of a rigidity theorem. If there is a map between two nilpotent Lie groups that preserves the cosets of subgroups (structures arising from the group law on a Lie group), just how much structure of the respective Lie groups does the map preserve? In this paper, the extra assumption of bijectivity will help to show the kind of similarity that exists. If a map is bijective then no information is `lost' about the structures of the domain and codomain, when either the map or its inverse is applied. 
This thesis is an attempt to demonstrate rigorously the extension of the rigidity theorem for the Heisenberg Group to all the nilpotent Lie groups. The proof involves arguments from a geometrical as well as algebraic perspective. 


%begin chapter 1
\chapter{Lie Groups, Lie Algebras and Homomorphisms}
This chapter outlines the basics of the theory of Lie groups and their relationship to Lie algebras. It gives some examples of matrix Lie groups including the Heisenberg Group, and explains the use of the Baker-Campbell-Hausdorff Formula in comparing operations in a Lie group with the operations in its corresponding Lie algebra. 

\section{Introduction}
The theory of Lie groups has evolved from the combination of several different disciplines in mathematics. Lie group theory combines the following ingredients: Group Theory from Algebra, Manifold Theory from Differential Geometry, and basic ideas from Topology. This paper will be mainly concerned with matrix Lie groups. That is, matrix groups that are also $C^{\inf}$-manifolds. The idea of a rigidity theory in this context is to use structures that arise from the group law within the manifold to show that a weak similarity of these matrix Lie groups implies a strong similarity between them. This paper will prove the theorem for upper-triangular nilpotent matrix Lie groups (and solvable matrix Lie groups??).

\section{Lie Groups}
\begin{Definition}\label{Lie Group}
A Lie group $\G $ is a $\mathrm{C}^{\infty}$ manifold which is also a group, for which the group operations of multiplication and taking inverses are $\mathrm{C}^{\infty}$ maps. ie the maps

\begin{eqnarray*}
\sigma : & \G \times \G & \longrightarrow \G  \\
& (g,h) & \longmapsto gh
\end{eqnarray*}
and

\begin{eqnarray*}
\tau : & \G  & \longrightarrow \G  \\
& g & \longmapsto g^{-1}
\end{eqnarray*}

are smooth with respect to the topology on $\G$.
\end{Definition}

\begin{Example}\label{Rn}
$(\R^{n}, +)$ is a Lie group with one single coordinate patch defined by the identity map on $\R^{n}$. The usual addition, and inversion given by multiplication by $(-1)$ are smooth maps.

\end{Example}

\begin{Example}\label{Circle}
The circle $\mathrm{S}^{1} = \{ e^{i\theta} \mid 0 \leq \theta < 2\pi \}$ is a Lie group. As a manifold, $\mathrm{S}^{1}$ has a single coordinate patch given by:

\begin{eqnarray*}
p_{1} : & \mathrm{S}^{1}  & \longrightarrow \R  \\
& e^{i\theta} & \longmapsto \theta .
\end{eqnarray*}

The group multiplication on $\mathrm{S}^{1}$ is given by 
\[ (e^{i\theta}, e^{i\phi}) \longmapsto e^{i\theta}e^{i\phi} = e^{i(\theta + \phi)} .\] 
This map is smooth with respect to the topologies on $\mathrm{S}^{1} \times \mathrm{S}^{1}$ and $\mathrm{S}^{1}$.
\end{Example}

An important aspect of this particular coordinate patch (even though in this case only one is required), is that it is 
also the tangent space of the manifold at the identity element of the group. [add picture]. In general this object is 
very useful for the study of Lie groups and takes on a structure of its own called a Lie algebra. 

%\begin{figure}
%\includegraphics{imagename.eps}
%\caption{Lovely caption}
%\label{fig:somethin-you-will-remember}
%\end{figure}

\begin{Definition}\label{Matrix Lie Group}
A matrix Lie group is any subgroup $\G$ of $\mathrm{GL}(n,\C)$ with the property that if there is a sequence $A_{m}$ of
matrices in $\G$ that converges to some matrix $A \in \mathrm{M}(n,\C)$ then either $A \in \G $ or $A$ is not invertible.
This is equivalent to $G$ being a closed subset of $\mathrm{GL}(n,\C)$ that is also a subgroup.
\end{Definition}

\subsection*{Examples}
\[\mathrm{SL}(n,\R), \mathrm{O}(n), \mathrm{SO}(n), \mathrm{SU}(2) \]


\begin{Example}\label{Heisenberg Group}.
The Heisenberg group, defined as follows, is a Lie group. This example forms the cornerstone of the result in this thesis.
\paragraph
{\noindent}The Heisenberg group is the set of real three-tuples with non-commutative multiplication given by
\[
(x,y,z)\cdot(x', y', z') := (x+x', y+y', z+z' + xy').
\]
Inversion is given by 
\[ (x,y,z)^{-1}= (-x, -y, -z+xy) .\]
As a matrix Lie group, the Heisenberg group can be represented as the group of all $ 3\times 3 $ upper triangular real matrices with $1$'s on the diagonal:
\[
 \G= \left\{ \begin{bmatrix} 1 & x & z \\ 0 & 1 & y \\ 0 & 0 & 1 \end{bmatrix} \biggm| x,y, z \in \R \right\} 
\]
It is topologically a closed subset of $\mathrm{GL}(n,\C)$ and is clearly closed under the operations of inversion and multiplication. Hence $\G$ is a matrix Lie group.
\end{Example}

Many of the important examples of matrix Lie groups are noncommutative groups. This makes the study of their structure in many 
cases fairly complex. However, as with the example of the circle, studying the Lie group can be made simpler by studying what is called its Lie algebra. The structure of the Lie algebra determines the local structure of its Lie group.


\section{Lie Algebras}
\begin{Definition}\label{Lie Algebra}
A Lie algebra is a vector space $\mathfrak{a}$ endowed with a map
\[ [\cdotp,\cdotp]:\; \mathfrak{a} \times \mathfrak{a} \longrightarrow \mathfrak{a} \]
with the following properties:
\begin{enumerate}
 \item $[\cdotp, \cdotp]$ is bilinear
 \item $[X,Y]=-[Y,X]$ for all $X$, $Y$ $\in \mathfrak{a}$
 \item $[X,[Y,Z]]+[Y,[Z,X]]+[Z,[X,Y]]=0$ for all $X$, $Y$, $Z$ $\in \mathfrak{a}$
\end{enumerate}
This map is often called the \emph{Lie bracket} or the \emph{bracket operation}.
\end{Definition}

The Lie algebra $\g$ of a Lie group $\G$ is the tangent space to $G$ at the identity $e$. This can be seen clearly in 
the example of the circle. The circle is parametrised as before by a the curve
\[ g: [0,2\pi) \longrightarrow \C \]
\[ g(\theta)= e^{i\theta}.\]
Differentiating gives
\[g' (\theta)= ie^{i\theta}.\]
At the identity, $1=e^{0}$,
\[ g' (0) = i \]
so the tangent space $T_{1}\G$ is the real vector space with basis $\{i\}$. 
In this case the operation $[\cdotp, \cdotp]$ is trivial, as with the Lie algebra of any abelian Lie group.
Without going into rigorous detail, it can be seen that the group operation on $S^{1}$ corresponds to addition of 
vectors in $T_{1}\G$. Also, the closer we move towards the identity in $S^{1}$ the more it resembles flat space. Multiplying two elements together becomes almost equivalent to adding their imaginary components.
(picture required)

In the case of matrix Lie groups, the tangent space at the identity takes a very clear form. The relationship between a 
matrix Lie group and its Lie algebra is determined by the exponential mapping on matrices. 

\begin{Definition}\label{Lie Algebra of a Matrix Lie Group}
The Lie algebra $\g$ of an $n \times n$ matrix Lie group $\G$ is
\[
\g = \{ X \in M_{n}(\C) \big| \exp{X} \in \G \}
\]
The bracket operation on $\g$, given by $[X,Y]=XY-YX$, satisfies the axioms for $\g$ to be a Lie algebra. 
\end{Definition}

\begin{Proposition}
This definition corresponds to the definition of a Lie algebra as the tangent space to the manifold $\G$ at the identity. [give ref]
\end{Proposition}

Note that the exponential map from $\g$ to $\G$ is injective but not necessarily surjective. The image of $\g$ under the exponential map is precisely the connected component of $\G$ containing the identity $e$. (proof). In the case that $\G$ is connected, the inverse of the map $\exp : \g \longrightarrow \G $ is called the matrix logarithm map, \mbox{$\log : \G \longrightarrow \g $}.

\begin{Proposition}\label{Lie algebra of Heisenberg group}
The Lie algebra of the Heisenberg group, $$\G = \left\{ \begin{bmatrix} 1 & x & z \\ 0 & 1 & y \\ 0 & 0 & 1 \end{bmatrix} \bigg| x,y, z \in \R \right\}$$ is 
\[
\g= \left\{ \begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix} \bigg| x,y,z \in \R \right\}
\]
and the exponential map $\exp$: $\g \longrightarrow \G$ is a bijection. 
\end{Proposition}

\begin{proof}
It will be shown that $\image(\exp) \subseteq \G$, $\G \subseteq \image(\exp)$ and that $\exp$ is injective. 
Let $x$, $y$ and $z$ be real numbers and note the following: 
\[
\begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix}^2=\begin{bmatrix} 0 & 0 & xy \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}
\]
\[
\begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix}^3=\begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}
\]
Now,
\begin{eqnarray*}
 \exp \begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix} &=& \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} + \begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix}+ \frac{1}{2}\begin{bmatrix} 0 & 0 & xy \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} \\
&=& \begin{bmatrix} 1 & x & z+\frac{1}{2}xy \\ 0 & 1 & y \\ 0 & 0 & 1 \end{bmatrix} \\
&\in & \G
\end{eqnarray*} so $ \image(\exp) \subseteq \G$.
Now suppose $a,b,c \in \R$ so $\begin{bmatrix} 1 & a & c \\ 0 & 1 & b \\ 0 & 0 & 1 \end{bmatrix} \in \G$. Then 
\[ 
\exp  \begin{bmatrix} 0 & a & c-\frac{1}{2}ab \\ 0 & 0 & b \\ 0 & 0 & 0 \end{bmatrix} = \begin{bmatrix} 1 & a & c \\ 0 & 1 & b \\ 0 & 0 & 1 \end{bmatrix}
\]
so $\G \subseteq \image(\exp)$.

Suppose that $A=\begin{bmatrix} 0 & a & c \\ 0 & 0 & b \\ 0 & 0 & 0 \end{bmatrix}$ and $A'=\begin{bmatrix} 0 & a' & c' \\ 0 & 0 & b' \\ 0 & 0 & 0 \end{bmatrix}$ are in $\g$ and that $\exp{A}=\exp{A'}$. Then 
\[
\begin{bmatrix} 0 & a & c+\frac{1}{2}ab \\ 0 & 0 & b \\ 0 & 0 & 0 \end{bmatrix} =\begin{bmatrix} 0 & a' & c' +\frac{1}{2}a' b' \\ 0 & 0 & b' \\ 0 & 0 & 0 \end{bmatrix}
\] 
and hence
\begin{align*}
a &= a' \\
b &= b' \\
c &= c'
\end{align*}
so $A=A'$.
\end{proof}




\subsection*{Notation}
For the purpose of simplifying the notation of matrices in the Heisenberg group and in its Lie algebra, vector notation will sometimes be used. As defined above, an element of the Heisenberg group $\G$ will be denoted either by $ \begin{bmatrix} 1 & a & c \\ 0 & 1 & b \\ 0 & 0 & 1 \end{bmatrix} $ or equivalently $[a,b,c]$. Similarly, elements of the Lie algebra of the Heisenberg group will be denoted either by $\begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix}$ or $(x,y,z)$, or in some cases $\begin{bmatrix} x \\ y \\ z \end{bmatrix}$.

In this notation, the bracket operation on two elements of $\g$ is given by 
\begin{eqnarray*}
[(x,y,z), (x',y',z')] &=& \begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & x' & z' \\ 0 & 0 & y' \\ 0 & 0 & 0 \end{bmatrix} - \begin{bmatrix} 0 & x' & z' \\ 0 & 0 & y' \\ 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & x & z \\ 0 & 0 & y \\ 0 & 0 & 0 \end{bmatrix} \\
&=& \begin{bmatrix} 0 & 0 & xy' \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} -\begin{bmatrix} 0 & 0 & x'y \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} \\
&=& (0,0,xy'-x'y)
\end{eqnarray*}

\section{Subgroups and Subalgebras}

\begin{Definition}\label{Lie subgroup}
Let $\G$ be a Lie group. A subset $\mathcal{H}$ of $\G$ is called a Lie subgroup if it is a subgroup and also a Lie group. So topologically $\mathcal{H}$ must be a closed subset of $\mathrm{GL}(n,\C)$, and algebraically $\mathcal{H}$ must be closed under the operations of multiplication and taking inverses. 
\end{Definition}

\begin{Definition}\label{Lie subalgebra}
Let $\g$ be a Lie algebra. Then a vector subspace $\mathfrak{h}$ of $\g$ is called a Lie subalgebra of $\g$ if it is also closed under the Lie bracket, so if $X$ and $Y$ are in $\mathfrak{h}$, then
\[
[X,Y] \in \mathfrak{h}.
\]

N.B. In this paper, the term ``subalgebra" will be interchangeable with ``Lie subalgebra" since no other types of algebras will be mentioned.
\end{Definition}

\begin{Definition}\label{Nilpotent Group}
Every group $G$ has an \emph{upper central series} $Z_{0},Z_{1}, Z_{2} \ldots$  constructed in the following way:
\begin{itemize}
\item $Z_{0}={e}$
\item $Z_{1}=$the centre of the group $G$
\item For $n \geq 2$,  $Z_{n}$ is the unique subgroup of $G$ such that $Z_{n}/Z_{n-1}$ is the centre of $G/Z_{n-1}$.

A group $G$ is called nilpotent if its \emph{upper central series} $Z_{n}$ vanishes for some integer $n$.
\end{itemize}

\end{Definition}


\begin{Definition}\label{Nilpotent Lie Algebra}
In a similar way, every Lie algebra $\mathfrak{a}$ has a \emph{lower central series} constructed by applying the bracket operation to the Lie algebra repeatedly:
 The Lie algebra of the Heisenberg group is a three dimensional nilpotent Lie algebra.
\end{Definition}

\section{Homomorphisms}

\begin{Definition}\label{Lie Group Homomorphism}
Let $\G$ and $\mathcal{H}$ be Lie groups and $f:\G \longrightarrow \mathcal{H} $ be a function. Then $f$ is called a \emph{Lie group homomorphism} if 
\[ f(xy)=f(x)f(y) \; \; \; \text{(i.e. $f$ is a group homomorphism)} \]
and $f$ is continuous with respect to the topologies on $\G$ and $\mathcal{H}$. 
If moreover, $f$ is a bijection, $f$ is called a \emph{Lie group isomorphism}
\end{Definition}

\begin{Example}\label{canonical quotient of centre}
Let $\G$ be the Heisenberg group. Let 
\begin{eqnarray*}
\pi : &\G& \longrightarrow (\R^{2},+) \\
&[a,b,c]& \longmapsto (a,b)
\end{eqnarray*}

Then 

\begin{eqnarray*}
\pi([a,b,c][a',b',c']) &=& \pi[a+a',b+b',c+c'+ab'] \\
&=&(a+a',b+b') \\
&=& (a,b)+(a',b') \\
&=& \pi[a,b,c] + \pi[a',b',c']
\end{eqnarray*}

So $\pi$ is a group homomorphism. Also, since the topology on $\G$ is inherited from the topology on $\R^{3}$, $\pi$ acts topologically as a projection from $\R^{3}$ onto $\R^{2}$ so is continuous. Hence $\pi$ is a Lie group homomorphism.
\end{Example}

\begin{Definition}\label{Lie Algebra Homomorphism}
Let $\g$ and $\mathfrak{h}$ be Lie algebras. A linear transformation $T: \g \longrightarrow \mathfrak{h}$ is a Lie algebra homomorphism if in addition to the linearity properties, $T$ preserves the Lie bracket, i.e. 
\[
T[A,B] = [T(A), T(B)] \; \;\;\;\;\;\;\;\; \forall A,B \in \g
\]
\end{Definition}


\begin{Definition}\label{Lie group anti-homomorphism}
Let $\G$ and $\mathcal{H}$ be Lie groups. Then a function $f: \G \longrightarrow \mathcal{H}$ is called a \emph{Lie group anti-homomorphism} if it is continuous with respect to the topologies on $\G$ and $\mathcal{H}$ and
\[
f(xy)=f(y)f(x) \;\;\;\;\; \forall x,y \in \G
\]
If $f$ is also a bijection, then it is called a \emph{Lie group anti-isomorphism}.
\end{Definition}

\begin{Definition}\label{Lie algebra anti-homomorphism}
Let $\g$ and $\mathfrak{h}$ be Lie algebras. A linear transformation $T: \g \longrightarrow \mathfrak{h}$ is a Lie algebra anti-homomorphism if in addition to the linearity properties,  
\[
T[A,B] = -[T(A), T(B)] \; \;\;\;\;\;\;\;\; \forall A,B \in \g
\]
\end{Definition}

\begin{Theorem}\label{Homomorphisms Correspondence}
Let $\G$ and $\mathcal{H}$ be matrix Lie groups with corresponding Lie algebras $\g$ and $\mathfrak{h}$ respectively. Suppose that $\Phi: \G \longrightarrow \mathcal{H}$ is a Lie group homomorphism. Then there exists a unique Lie algebra homomorphism $\phi: \g \longrightarrow \mathfrak{h}$ such that 
\[
\Phi(e^{X}) = e^{\phi(X)} \;\;\;\;\;\;\; \forall X \in \g
\]

Now let $\G$ and $\mathcal{H}$ be matrix Lie groups, $\g$ and $\mathfrak{h}$ be their Lie algebras and let $\G$ be simply connected [need to define this somewhere- appendix]. Then if $\phi: \g \longrightarrow \mathfrak{h}$ is a Lie algebra homomorphism, there exists a unique Lie group homomorphism $\Phi: \G \longrightarrow \mathcal{H}$ defined by
\[
\Phi(A) = e^{\phi(\log{A})} \;\;\;\;\;\;\; \forall A \in \G
\]
\end{Theorem}

\begin{Example}
Let $\G$ be the Heisenberg group and let $\g$ be its Lie algebra. Let $\mathcal{H}=\mathrm{GL}(1,\R)$. The Lie algebra of $\mathcal{H}$ is $\mathfrak{h}=\mathrm{M}_{1}(\R)$. Define 
\begin{eqnarray*}
\Phi :& \G &\longrightarrow  \mathcal{H} \\
&[x,y,z] &\longmapsto  e^{x}
\end{eqnarray*}
This is a Lie group homomorphism. It is continuous since the map $x \mapsto e^{x}$ is continuous and 

\begin{eqnarray*}
\Phi ([x,y,z][x',y',z']) &=& \Phi [x+x',y+y', z+z'+xy'] \\
&=& e^{x+x'} \\
&=& e^{x}e^{x'} \\
&=& \Phi [x,y,z] \Phi [x',y',z']
\end{eqnarray*}

Now
\[
\Phi [x,y,z] = \Phi (\exp{(x,y,z-\frac{1}{2}xy))}=e^{x}
\]
So there is a unique map $\phi: \g \longrightarrow \mathfrak{h}$ defined by $\phi(x,y,z)=x$ such that
\[
\Phi(e^{X}) = e^{\phi(X)} \;\;\;\;\;\;\;\; \forall X \in \g
\]
 It is a linear map of vector spaces since it projects onto one of the dimensions. Also for $(x,y,z)$ and $(x',y',z')$ in $\g$, 
\[ \phi [(x,y,z), (x',y',z')]=\phi (0,0,xy'-x'y)=0
\]
and
\begin{eqnarray*}
[\phi(x,y,z),\phi(x',y',z')]=[x,x']=xx'-x'x &=& 0 \\
&=&  \phi [(x,y,z), (x',y',z')]
\end{eqnarray*}
Hence $\phi$ is a Lie algebra homomorphism.
\end{Example}

%Baker Campbell Hausdorff Formula
\section{The Baker-Campbell-Hausdorff Formula}

Since we wish to simplify the study of Lie group homomorphisms by studying properties of the corresponding Lie algebra homomorphisms, we need a way to relate the operations on these objects directly. In the case of the unit circle $\mathrm{S}^{1}$, the properties of the exponential function on real numbers give the simple correspondence between addition on the real line, and multiplication of elements of the unit circle in the complex plane. For noncommutative groups this simple relationship does not hold. However, there is a more complicated way of relating the structure of Lie algebras to their corresponding Lie groups. 

\paragraph{Heisenberg Group}
The Lie algebra of the Heisenberg group has noncommutative multiplication, however for any $X$, $Y$ $\in \g$, $X$ and $Y$ each commutes with $[X,Y]$. In other words
\[
[X,[X,Y]]=[Y,[X,Y]]=0
\]
\begin{proof}
Let $X=(x,y,z)$ and $Y=(x',y',z')$ be elements of $\g$. 
\begin{eqnarray*}
[X,[X,Y]] &=& [X, (0,0,xy'-x'y)] \\
&=&[(x,y,z),(0,0,xy'-x'y)] \\
&=& 0
\end{eqnarray*}

and
\begin{eqnarray*}
[Y,[X,Y]] &=& [Y, (0,0,xy'-x'y)] \\
&=&[(x',y',z'),(0,0,xy'-x'y)] \\
&=& 0
\end{eqnarray*}
\end{proof}

We require some tools in order to explain the formula for the Heisenberg group. 

\begin{Definition}\label{Two Adjoint Mappings}
Let $X$ and $Y$ be elements of $\mathrm{M}_{n}(\C)$. Define the following maps: 
\begin{eqnarray*}
\mathrm{Ad}_{Y}:& \mathrm{M}_{n}(\C) & \longrightarrow \mathrm{M}_{n}(\C) \\
&X& \longmapsto YXY^{-1}
\end{eqnarray*}

and
\begin{eqnarray*}
\mathrm{ad}_{Y}:& \mathrm{M}_{n}(\C) & \longrightarrow \mathrm{M}_{n}(\C) \\
&X& \longmapsto [Y,X]=YX - XY
\end{eqnarray*}

\end{Definition}

\begin{Definition}
Define also the operator $e^{\mathrm{ad}_{Y}}$ given by the following power series:
\[
e^{\mathrm{ad}_{Y}}(X)= \mathrm{id}(X) + \mathrm{ad}_{Y}(X) + \frac{1}{2!}( \mathrm{ad}_{Y}^{2})(X) + \frac{1}{3!}( \mathrm{ad}_{Y}^{3})(X) + \cdots
\]
\end{Definition}

\begin{Lemma}
\[
\mathrm{Ad}_{e^{Y}}(X)=e^{\mathrm{ad}_{Y}}(X)
\]
\end{Lemma}

\begin{Theorem}
Let $\G$ be a matrix Lie group with Lie algebra $\g$. If the commutivity property above holds in $\g$ then for $X$ and $Y$ in $\g$, 
\[
e^{X}e^{Y}=e^{X+Y+\frac{1}{2}[X,Y]}
\]
This is a special case of the Baker-Campbell-Hausdorff Formula (which will sometimes be referred to as the BCH Formula in this project)
\end{Theorem}

\begin{proof}
To prove the formula, it will be shown that two apparently different functions are both the unique solution to a particular differential equation, and therefore must be identical. 
Let $X$ and $Y$ be elements of $\g$. Consider the differential equations in the real variable $t$ defined by 
\begin{eqnarray*}
A(t)&=& e^{tX}e^{tY}e^{-\frac{t^{2}}{2}[X,Y]} \\
B(t) &=& e^{t(X+Y)}
\end{eqnarray*}

\begin{Lemma}\label{commute matrices}
Let $M$ and $N$ be $n \times n$ matrices. If $M$ commutes with $N$, then $M$ also commutes with $e^{N}$. 
\end{Lemma}
 

\begin{proof}
Suppose $MN^{k}=N^{k}M$ for some positive integer $k$. Then 
\begin{eqnarray*}
MN^{k+1} &=& MN^{k}N \\
&=& N^{k}MN \\
&=& N^{k}NM \\
&=& N^{k+1}M
\end{eqnarray*}
So since $MN=NM$, the mathematical induction hypothesis indicates that $MN^{K}=N^{K}M$ for all positive integers $K$.
Now, 
\begin{eqnarray*}
Me^{N} &=& M(I+N+\frac{N^{2}}{2!}+\frac{N^3}{3!}+\cdots) \\
&=& M+MN+\frac{1}{2!}MN^{2}+\frac{1}{3!}MN^{3} + \cdots \\
&=& M+NM+\frac{1}{2!}N^{2}M+\frac{1}{3!}N^{3}M + \cdots \\
&=& (I+N+\frac{N^{2}}{2!}+\frac{N^3}{3!}+\cdots)M \\
&=& e^{N}M
\end{eqnarray*}
\end{proof}
\begin{Lemma}
\[
Xe^{tY}=e^{tY}(X+t[X,Y])
\]
\end{Lemma}
\begin{proof}
\begin{eqnarray*}
Xe^{tY} &=& e^{tY}e^{-tY}Xe^{tY} \\
&=& e^{tY}\mathrm{Ad}_{e^{-tY}}(X) \\
&=& e^{tY}e^{-t\mathrm{ad}_{Y}}(X) \\
&=& e^{tY}(X-t[Y,X]+\frac{t^{2}}{2}[Y,[Y,X]]-\frac{t^{3}}{3!}[Y,[Y,[Y,X]]]+\cdots \\
&=& e^{tY}(X - t[Y,X])\\
&=& e^{tY}(X + t[X,Y])
\end{eqnarray*}
since $[Y,[Y,X]]=0$.
\end{proof}

$A(t)$ and $B(t)$ are both now differentiated as follows:

\begin{eqnarray*}
A'(t) &=& e^{tX}Xe^{tY}e^{-\frac{t^2}{2}[X,Y]}+e^{tX}e^{tY}Ye^{-\frac{t^2}{2}[X,Y]}+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(-t[X,Y]) \\
&=& e^{tX}Xe^{tY}e^{-\frac{t^2}{2}[X,Y]}+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}Y+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(-t[X,Y]) \\
& & (\text{since $Y$ commutes with $[X,Y]$ and by Lemma 1.5.5})\\
&=&e^{tX}e^{tY}(X+t[X,Y])e^{-\frac{t^2}{2}[X,Y]}+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}Y+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(-t[X,Y]) \\
&=& e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(X+t[X,Y])+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}Y+e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(-t[X,Y]) \\
& & (\text{since $X$ and $[X,Y]$ each commute with $[X,Y]$})\\
&=& e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(X+t[X,Y]+Y-t[X,Y]) \\
&=& e^{tX}e^{tY}e^{-\frac{t^2}{2}[X,Y]}(X+Y) \\
&=& A(t)(X+Y)
\end{eqnarray*}

\begin{eqnarray*}
B'(t) &=& e^{t(X+Y)}(X+Y) \\
&=& B(t)(X+Y)
\end{eqnarray*}

It is also necessary for the above differential equations to have the same initial conditions if the uniqueness theorem is to be applied. 
\[
A(0)=e^{0}e^{0}e^{0}=I^{3}=I
\]
and 
\[
B(0)=e^{0}=I
\]
So $A(t)$ and $B(t)$ satisfy the same differential equation with the same initial conditions. By the basic uniqueness theorems for differential equations this implies $A(t)=B(t), \forall t \in \R$. Substituting the value $t=1$ gives 
\begin{eqnarray*}
e^{X}e^{Y}e^{-\frac{1}{2}[X,Y]} &=&e^{X+Y} \\
e^{X}e^{Y} &=& e^{X+Y}e^{\frac{1}{2}[X,Y]} \\
&=& e^{X+Y+\frac{1}{2}[X,Y]} \\
& & (\text{since $(X+Y)$ commutes with $\frac{1}{2}[X,Y]$})
\end{eqnarray*}
This completes the proof of the Baker-Campbell-Hausdorff Formula for the Heisenberg group. 
\end{proof}

There is a more general formula for any noncommutative Lie group $\G$, however there is an infinite series of commutator terms, say $S$, such that for all $X$ and $Y$ in $\G$,
\[
e^{X}e^{Y}=e^{X+Y+S}
\]
The series formula will be stated but not proved in this thesis. A full proof is given in ...
%find a proof for general BCH formula




%Begin chapter 2
\chapter{Case of the Heisenberg Group}
For this chapter the following notation will be used:

$$\G=\text{the Heisenberg group}$$
$$\g=\text{the Lie algebra of the Heisenberg group}$$

\section{Aim}
The aim of this chapter is to prove a rigidity theorem for coset-preserving bijective maps on the Heisenberg group. The ideas and methods used in this theorem are foundational for the proof of the theorem for higher dimensional Lie groups. The first theorem will show that coset-preserving maps on the Heisenberg group have corresponding maps on the Lie algebra that are affine, and take on a particular form.\
The second theorem shows the stronger result if the map also preserves cosets of discrete subgroups of the Heisenberg group.





\section{Proof that coset-preserving maps are affine}

\begin{Definition}\label{affine map}
A map $\varphi: \R^{n} \longrightarrow \R^{n}$ is \emph{affine} if for all $\mathbf{v_{1}},\mathbf{v_{2}} \in \R^{n}$ and all $\lambda \in \R$ 
\[
\varphi [ \lambda \mathbf{v_{1}}+(1-\lambda) \mathbf{v_{2}}]= \lambda \varphi (\mathbf{v_{1}})+(1-\lambda) \varphi(\mathbf{v_{2}})
\]
\end{Definition}

\begin{Lemma}
Suppose $\alpha: \R^{2} \longrightarrow \R^{2}$ sends $\vec{0}$ to $\vec{0}$, sends lines to lines and is bijective. Then $\alpha$ is linear. 
\end{Lemma}

\begin{proof}
Construct a map based on the following assumptions. Let $\alpha': \R^{2} \longrightarrow \R^{2}$ be linear and
\begin{eqnarray*}
\alpha'(\alpha(1,0))&=& (1,0)\\
\alpha'(\alpha(0,1)) &=& (0,1)
\end{eqnarray*}
This map $\alpha'$ is well-defined and unique since bijectivity of $\alpha$ implies $\alpha(1,0) \neq \alpha(0,1)$ so every element of $\R^{2}$ can be written as a linear combination of $\alpha(1,0)$ and $\alpha(0,1)$. 
Now define a map $\psi: \R^{2} \longrightarrow \R^{2}$ by $\psi= \alpha' \circ \alpha$
Note that $\psi$ is bijective so sends parallel lines to parallel lines. In particular $\psi$ sends the $x$-axis to itself and also the $y$-axis to itself. Suppose $\psi(x_{1},0)=(x_{1}',0)$ and $\psi(x_{2},0)=(x_{2}',0)$
A geometrical construction will demonstrate that
\begin{equation}
\psi(x_{1}+x_{2},0)=\psi(x_{1},0)+\psi(x_{2},0)
\end{equation}

and 
\begin{equation}
\psi(x_{1}x_{2},0)=(x_{1}'x_{2}',0)
\end{equation}


where 
\begin{eqnarray*}
P: &R^{2}& \longrightarrow \R \\
&(x,y)& \longmapsto x
\end{eqnarray*}

Proof of the addition property:
Consider the diagram. Let $P=(1,0)$ and $Q=(0,1)$. $A$ is the point $(x_{1},0)$ and $B$ is the point $(x_{2},0)$. Then construct lines $L_{1}$, $L_{2}$, $L_{3}$ and $L_{4}$ as shown so that $$A'=(0,x_{1})$$ $$B'=(x_{2}, x_{1})$$ and $$C=(x_{1}+x_{2},0)$$ Now the map $\psi$ sends the $x$- and $y$- axes to themselves, sends parallel lines to parallel lines and $\psi(P)=P$ and $\psi(Q)=Q$. So the parallelogram $AA'B'C$ is mapped to a new parallelogram $\psi(A)\psi(A')\psi(B')\psi(C)$ where the sloping sides must have gradient $-1$, since they are parallel to $PQ$. So $\psi(A)=(x_{1}',0)$ and $\psi(B)=(x_{2}',0)$. Then by the properties of the parallelogram, 
\[
\psi(A')=(0,x_{1}')
\]
\[
\psi(B')=(x_{2}',x_{1}')
\]
and 
\[
\psi(C)=(x_{1}'+x_{2}',0)
\]
Hence 
\begin{equation}\label{axisaddnmap}
\psi(x_{1}+x_{2},0)=\psi(x_{1},0)+\psi(x_{2},0). 
\end{equation}

Now consider this diagram showing the construction of $C=(x_{1}x_{2},0)$ by starting with $A=(x_{1},0)$ and $B=(x_{2},0)$. Construct the line of gradient $-1$ through $A$ so that $A'=(0,x_{1})$. Then construct a horizontal through $A'$ and a vertical through $P$ to meet in $D=(1,x_{1})$. Construct a line $L$ through $O$ and $C$, $y=x_{1}x$ and a vertical line through $B$ to meet in $E=(x_{2}, x_{1}x_{2})$. Then construct a horizontal line through $E$ to meet the $y$-axis in $B'=(0,x_{1}x_{2})$. Construct a line of gradient $-1$ through $B'$ which meets the $x$-axis at $C$. 

Now consider the all these points and lines under the map $\psi$. Let $\psi(A)=(x_{1}',0)$ and $\psi(B)=(x_{2}',0) $. Lines of gradient $-1$ are mapped to lines of gradient $-1$ since $P$ and $Q$ are fixed. So 
\[
\psi(A')=(0,x_{1}')
\]
Horizontal lines are mapped to horizontal lines and vertical lines are mapped to vertical lines so
\[
\psi(D)=(1,x_{1}')
\]
Hence the line $L$ is mapped to the line $\psi(L)$ which is given by $y=x_{1}'x$. Again, the vertical line through $B$ is mapped to a vertical line through $\psi(B)$ so 
\[
E=(x_{2}',x_{1}'x_{2}')
\]
and the by the preservation of horizontal lines, 
\[
\psi(B)=(0,x_{1}'x_{2}')
\]
and by the preservation of lines of gradient $-1$


\[
\psi(C)=(x_{1}'x_{2}',0)
\]

Hence 
\begin{equation}\label{axismultmap}
\psi(x_{1}x_{2},0)= (x_{1}'x_{2}',0)
\end{equation}

Define the projection map
\begin{eqnarray*}
P: &R^{2}& \longrightarrow \R \\
&(x,y)& \longmapsto x
\end{eqnarray*}

Consider the induced map $f:\R \longrightarrow \R$ defined by
\[
f(x)=P_{x}(\psi(x,0))
\]
Then by the equations \ref{axisaddnmap} and \ref{axismultmap} above, $f$ is bijective and has the following properties:
\begin{eqnarray*}
f(0) &=& 0 \\
f(1) &=& 1 \\
f(x+y)&=& f(x)+f(y) \\
f(xy) &=& f(x)f(y)
\end{eqnarray*} 
Now let $n \in \Z$. Then 
\begin{eqnarray*}
f(n) &=& f(1+1+ \cdots +1) \\
&=&f(1)+f(1) + \cdots +f(1) \\
&=& 1+1+ \cdots +1 \\
&=& n
\end{eqnarray*}
Also 
\begin{eqnarray*}
1 &=&f(1) \\
 &=& f\left( n \cdot \frac{1}{n} \right) \\
&=& f(n)f\left( \frac{1}{n} \right) \\
&=& nf\left( \frac{1}{n} \right)
\end{eqnarray*}
Hence 
\[
f \left( \frac{1}{n} \right)=\frac{1}{n}
\]
Now let $m$ and $n$ be integers. So 
\begin{eqnarray*}
f \left(\frac{m}{n}\right)&=&f\left(m \cdot \frac{1}{n}\right)\\
&=& f(m)f\left(\frac{1}{n}\right) \\
&=& m\cdot \frac{1}{n} \\
&=& \frac{m}{n}
\end{eqnarray*}
So $f$ restricted to $\Q$ is the identity. \
Further, $f$ is continuous.
\begin{proof}
$f$ sends positive numbers to positive numbers and negative numbers to negative numbers since if $a>0$,
\[
f(a)=f(\sqrt{a}\sqrt{a})=f(\sqrt{a})f(\sqrt{a})=f(\sqrt{a})^{2}>0
\]
and by bijectivity if $b<0$ then $f(b)<0$.\
Let $\epsilon >0$ and fix an element $y \in \R$. Suppose $|x-y|<\epsilon$. Now if $x<y$ then $0<x-y<\epsilon$. Insert a rational number $z$ between $x-y$ and $\epsilon$ so that
\[
0<x-y<z<\epsilon
\]
Now
\[
f(z-(x-y))=f(z)-f(x-y)=z-f(x-y)>0
\]
since $z-(x-y)>0$ and $f(z)=z$. Also
\[
f(x-y)>0
\]
since $x-y>0$. So $0<f(x-y)<z<\epsilon$, so
\[
0<f(x)-f(y)<\epsilon
\]
On the other hand, if $x \geq y $ then $-\epsilon<x-y \leq 0$. Insert a rational number $z$ between $-\epsilon$ and $x-y$ so that $-\epsilon<z<x-y \leq 0$. Then 
\[
f(z-(x-y))=f(z)-f(x-y)=z-f(x-y)<0
\]
since $z-(x-y)<0$ and $f(z)=z$. Also
\[
f(x-y)\leq 0
\]
since $x-y \leq 0$. So $-\epsilon<z<f(x-y)\leq 0$, so
\[
-\epsilon < f(x)-f(y) \leq 0
\]
In both cases $|f(x)-f(y)|<\epsilon$. Hence the function $f$ is continuous
\end{proof}
Since every real number is a limit of a sequence of rational numbers, and $f$ is continuous, $f$ must be the identity map $\R \longrightarrow \R$. 

The same properties can be shown for the map $\psi$ on the $y$-axis. And then since every point in the plane is a projection of point on the x and y axes by horizontal and vertical lines, $\psi(x,y)=(x,y)$ for all $(x,y) \in \R^{2}$. Hence $\alpha'_{o}\alpha=\psi=\text{id}$ so $\alpha=\alpha'^{-1}$, which is a linear map since $\alpha'$ is a linear map. 
\end{proof}

\begin{Proposition}
Let $\varphi: \R^{3} \longrightarrow \R^{3}$ be such that $\varphi$ is bijective, sends lines to lines and vertical planes to vertical planes. Then $\varphi$ is an affine map. 
\end{Proposition}  

\begin{proof}
Let $\mathbf{v_{1}}$ and $\mathbf{v_{2}} $ be vectors in $\R^{3}$.[[ such that $\mathbf{v_{1}}-\mathbf{v_{2}} \notin \text{span} \{ \mathbf{{\bf e}_{3}} \}$.]] Define a line $L$ in $\R^{3}$ by 
$$ L:=\{ \lambda \mathbf{v_{1}} + (1-\lambda) \mathbf{v_{2}} \mid \lambda \in \R \}.$$

If $\mathbf{v_{1}}-\mathbf{v_{2}} \notin \text{span} \{ (0,0,1) \}$, then let $\mathbf{v_{3}}=(0,0,1)$. Whereas if $\mathbf{v_{1}}-\mathbf{v_{2}} \in \text{span} \{ (0,0,1) \}$ let $\mathbf{v_{3}}=(0,0,1)$. 
  
In either case it is possible to define a vertical plane $\Pi: \lambda \mathbf{v_{1}} + (1-\lambda) \mathbf{v_{2}} + \mu\mathbf{v_{3}}$. $\varphi$ sends vertical planes to vertical planes so define the plane $\Sigma = \varphi(\Pi)$. Then 
\[
\mathbf{v_{1}} \in \Pi
\]
\[
\mathbf{v_{2}} \in \Pi
\]
so 
\[
\varphi(\mathbf{v_{1}}) \in \Sigma
\]
and 
\[
\varphi(\mathbf{v_{2}}) \in \Sigma.
\]
Hence the line joining these to points is also in $\Sigma$, in other words
\[
\gamma \varphi(\mathbf{v_{1}}) + (1-\gamma)\varphi(\mathbf{v_{2}}) \in \Sigma
\]
Also $\Sigma$ is vertical, and since $\mathbf{v_{1}}-\mathbf{v_{2}} \notin \text{span}\{ \mathbf{v_{3}}\}$, the plane $\Sigma$ is defined by 
\[
\Sigma = \{ \gamma \varphi(\mathbf{v_{1}}) + (1-\gamma)\varphi(\mathbf{v_{2}}) + \delta \varphi(\mathbf{v_{3}}) \; \;|\;\; \gamma,\delta \in \R \}
\]
Now each plane $\Pi$ and $\Sigma$ can be given the structure of a two dimensional vector space in the following sense:
Let $O_{\Pi}=\mathbf{v_{2}}$ and $O_{\Sigma}= \varphi(\mathbf{v_{2}})$. Then elements of $\Pi$ can all be expressed as 
\[
(\alpha_{1}, \alpha_{2})_{\Pi}:= \alpha_{1}\mathbf{v_{1}}+(1-\alpha_{1})\mathbf{v_{2}} + \alpha_{2}\mathbf{v_{3}}
\]
so that $O_{\Pi}=(0,0)$. 
Elements of $\Sigma$ can similarly be expressed as
\[
(\beta_{1}, \beta_{2})_{\Sigma}= \beta_{1}\varphi(\mathbf{v_{1}})+(1-\beta_{1})\varphi(\mathbf{v_{2}}) + \beta_{2} \varphi (\mathbf{v_{3}})
\]
Thus $\Pi$ and $\Sigma$ have the structure of two-dimensional vector spaces, isomorphic to $\R^2$. Then by the Lemma 2.2.1, since $\varphi(O_{\Pi}=O_{\Sigma}$, and $\varphi$ is bijective and sends lines to lines, $\varphi:\Pi \longrightarrow \Sigma$ is linear. In other words, 
\[
\varphi[(\alpha_{1},\alpha_{2})_{\Pi}+\lambda(\alpha_{1}',\alpha_{2}')_{\Pi}]=\varphi(\alpha_{1},\alpha_{2})_{\Pi} + \lambda \varphi(\alpha_{1}',\alpha_{2}')_{\Pi}
\]
Note also that 
\begin{eqnarray*}
\varphi(1,0)_{\Pi}&=&\varphi(\mathbf{v_{1}}) \\
&=& (1,0)_{\Sigma}
\end{eqnarray*}
So:
\begin{eqnarray*}
\varphi[\lambda(\mathbf{v_{1}})+(1-\lambda)(\mathbf{v_{2}})] &=& \varphi(\lambda,0)_{\Pi} \\
&=& \varphi[\lambda(1,0)_{\Pi}] \\
&=& \lambda\varphi(1,0)_{\Pi} \\
&=& \lambda(1,0)_{\Sigma} \\
&=&(\lambda,0)_{\Sigma} \\
&=& \lambda\varphi(\mathbf{v_{1}})+(1-\lambda)\varphi(\mathbf{v_{2}})
\end{eqnarray*}
Hence $\varphi$ is affine on $\R^{3}$.
\end{proof}

\begin{Proposition}
Let $\varphi$ be an affine map from $\R^{n}$ to $\R^{n}$. So $\varphi$ preserves affine combinations. Then the map $\phi$ defined by 
\[
\phi(x)=\varphi(x)-\varphi(0)
\]
is a linear map.
\end{Proposition}

\begin{proof}
Let $\lambda \in \R$ and $x$ and $y \in \R^{n}$. Then 
\begin{eqnarray*}
\phi(\lambda x) &=& \varphi(\lambda x) - \varphi(0) \\
&=& \varphi(\lambda x +(1-\lambda)0)-\varphi(0) \\
&=& \lambda \varphi(x)+(1-\lambda)\varphi(0) -\varphi(0) \\
&=& \lambda \varphi(x)-\lambda\varphi(0) \\
&=& \lambda (\varphi(x)-\varphi(0)) \\
&=& \lambda \phi(x) 
\end{eqnarray*}

and

\begin{eqnarray*}
\phi(x+y) &=& \varphi(x+y)-\varphi(0) \\
&=& \varphi(\lambda x' + (1-\lambda)y') -\varphi(0) \\
&=& \lambda \varphi( x') + (1-\lambda)\varphi(y') -\varphi(0) \\
&=& \lambda \varphi(\frac{1}{\lambda} x) + (1-\lambda)\varphi(\frac{1}{1-\lambda} y') -\varphi(0) \\
&=& \lambda (\varphi(\frac{1}{\lambda} x) -\varphi(0)) + (1-\lambda)(\varphi(\frac{1}{1-\lambda} y')-\varphi(0))  \\
&=& \lambda (\phi(\frac{1}{\lambda} x) ) + (1-\lambda)(\phi(\frac{1}{1-\lambda} y'))  \\
&=& \lambda \frac{1}{\lambda} \phi( x)  + (1-\lambda)\frac{1}{1-\lambda} \phi( y')  \\
&=& \phi(x)+\phi(y)
\end{eqnarray*}
Hence $\phi$ is a linear map.
\end{proof} 



\section{Restriction obtained from preservation of connected cosets}

\begin{Theorem}\label{preserve closed cosets}
Suppose that $\Psi: \G \longrightarrow \G$ is bijective and sends every coset of any Lie subgroup of $\G$ to some coset of some Lie subgroup of $\G$. Then $\Phi:\G \longrightarrow \G$ defined by $\Phi(X)=\Psi(0)^{-1}\Psi(X)$ also preserves these cosets and the induced map $\phi: \g \longrightarrow \g$ defined by $\phi(x)=\log \Phi(\exp(x))$ is a linear map of $\g$ given by a matrix of the form

$\phi =\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & g
\end{bmatrix}$, where
$ \det(\phi) \neq 0 $. In other words
\[
\phi \begin{bmatrix}
x \\y \\z 
\end{bmatrix}= \begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & g
\end{bmatrix}
\begin{bmatrix}
x \\y \\z 
\end{bmatrix}
\]

\end{Theorem}


\begin{proof}
Observe that 
\begin{eqnarray*}
\phi(0_{\g})&=&\log \Phi (\exp(0)) \\
&=& \log \Phi(0) \\
&=& \log \Psi(0)^{-1}\Psi(0) \\
&=& \log 0_{\G} \\
&=& 0_{\g}.
\end{eqnarray*}

Then since $\phi(0_{\g})=0_{\g}$, $\phi$ must send subalgebras of $\g$ to subalgebras of $\g$ bijectively (since every subalgebra contains $0_{\g}$). What are these subalgebras?


%What are the subalgebras and Subgroups and Cosets?
\subsection{One-dimensional Subalgebras}
\begin{Proposition}
All one-dimensional subspaces of $\g$ are subalgebras.
\end{Proposition}

\begin{proof}
All one-dimensional subspaces are lines through the origin. Let $S$ be a one-dimensional subspace of $\g$. Then 
\[
S= \text{span}(X)
\]
for some $X=(x,y,z) \in \g$. Let $\lambda X$ and $\mu X$ be elements of $S$. Then 
\begin{eqnarray*}
[\lambda X,\mu X] &=& [(\lambda x, \lambda y, \lambda z), (\mu x, \mu y, \mu z)] \\
&=& (0,0, \lambda x \mu y - \lambda y \mu x) \\
&=& (0,0, \lambda  \mu xy - \lambda \mu xy) \\
&=& (0,0,0) \in S
\end{eqnarray*}
Hence every one-dimensional subspace of $S$ is closed under the Lie bracket and is thus also a one-dimensional subalgebra. 
\end{proof}

\subsection{Two-dimensional Subalgebras}
Let $\mathfrak{Z}=\text{span}(0,0,1)$ denote the $z$-axis.
The question is now whether all two-dimensional subspaces of $\g$ are also subalgebras. Two-dimensional subspaces of $\g$ are planes through the origin. So any subspace is the span of two linearly independent vectors in $\g$.
Let $\mathfrak{T}= \text{span}(X ,Y)$ where $X=(x_{1},x_{2},x_{3})$ and $Y=(y_{1},y_{2},y_{3})$ are distinct elements of $\g$.

Then  
\begin{eqnarray*}
  [X,Y] & \in \mathfrak{T} \\
\text{i.e.}\;\;\;\;\;(0,0, x_{1}y_{2}-x_{2}y_{1}) & \in \mathfrak{T}
\end{eqnarray*}
   
This restriction can be analysed in the following two distinct cases:

\begin{enumerate}
 \item $x_{1}y_{2}=x_{2}y_{1}$
 \item $x_{1}y_{2}\neq x_{2}y_{1}$
\end{enumerate}

In case (1), 
\[
x_{1}y_{2}=x_{2}y_{1}
\]
First consider the sub-case in which $x_{1}=x_{2}=0$ Then it is not possible to have both $x_{2}=0$ and $y_{2}=0$ since this would make $X$ and $Y$ linearly dependent. So without loss of generality select $y_{2} \neq 0$. Then there exists $k \in \R$ such that 
\[
x_{2} = ky_{2}
\]
So 
\[
X-kY=(0,0,x_{3}-ky_{3}) \in \mathfrak{T}
\]
Note that $X-kY \neq 0$ since otherwise $X$ and $Y$ would be linearly dependent. So $\mathfrak{T}$ contains $\text{span}(0,0,x_{3}-ky_{3})=\mathfrak{Z}$, and hence $\mathfrak{T}$ is a vertical plane. 

Now suppose at least one of $x_{1}$ and $y_{1}$ is non-zero. Without loss of generality suppose $y_{1} \neq 0$.
Then there exists some $\lambda \in \R$ such that $x_{1} = \lambda y_{1}$. So

\begin{eqnarray*}
 \lambda y_{1}y_{2} &=& x_{2} y_{1}\\
 \lambda y_{2} &=& x_{2}
\end{eqnarray*}

Hence
\begin{eqnarray*}
 X-\lambda Y & = & (x_{1},x_{2},x_{3})-(\lambda y_{1},\lambda y_{2},\lambda y_{3}) \\
	     & = & (0,0,x_{3}-\lambda y_{3}) \\
	     & \neq & 0
\end{eqnarray*}
since $X$ and $Y$ are linearly independent. 
Therefore 
\[
\mathfrak{Z}=\text{span}(0,0,1)=\text{span}(0,0,x_{3}-\lambda y_{3}) \in \mathfrak{T}
\]

In the second case, $x_{1}y_{2}\neq x_{2}y_{1}$ so 
\[
\text{span}(0,0, x_{1}y_{2}- x_{2}y_{1})= \mathfrak{Z} \in \mathfrak{T}
\]

Hence any plane through the origin in $\g$ which is closed under the Lie bracket must contain the $z$-axis. In other words, all two-dimensional subalgebras of $\g$ are vertical planes through the origin.


%Using this propostion will mean changing the way the rest of this result is argued
%Need to explain why preservation of cosets in $\G$ implies preservation of cosets in $\g$.
\begin{Proposition}
Let $\mathcal{C}$ be a coset of some subgroup $\mathcal{S}$ of $\G$. Then $\mathcal{C}$ is the image of a translate of a subalgebra of $\g$ under the exponential map. In other words, preserving cosets of Lie subgroups in $\G$ corresponds to preserving translates of subalgebras of $\g$.
\end{Proposition}

\begin{proof}
Let $X=(x,y,z) \in \g$ and so that $\mathcal{S}=\{ \lambda X | \lambda \in \R \}$ is a one-dimensional subalgebra of $\g$. $e^{\mathcal{S}}$ is the corresponding Lie subgroup of $\G$. i.e.
\begin{eqnarray*}
e^{\mathcal{S}} &=& \{ e^{\lambda X}| \lambda \in \R \} \\
&=& \left\lbrace \begin{bmatrix}
1 & \lambda x & \lambda z + \frac{1}{2} \lambda^{2}xy \\
0 & 1 & \lambda y \\
0 & 0 & 1
\end{bmatrix} \bigg| \lambda \in \R \right\rbrace
\end{eqnarray*}
Let $\mathcal{C}$ be the coset of $e^{\mathcal{S}}$ given by right translation by $e^{P}$, for a fixed element $P = \begin{bmatrix}
0 & p & r \\
0 & 0 & q \\
0 & 0 & 0
\end{bmatrix} \in \g$. Then 
\begin{eqnarray*}
\mathcal{C} &=& \{e^{\lambda X}e^{P}|\lambda \in \R \} \\
&=& \{ \exp( \lambda X + P + \frac{1}{2}[\lambda X,P]) | \lambda \in \R \} \\
&=& \left\lbrace \exp \left( 
\begin{bmatrix}
0 & \lambda x & \lambda z \\
0 & 0 & \lambda y \\
0 & 0 & 0
\end{bmatrix} + 
\begin{bmatrix}
0 & p & r \\
0 & 0 & q \\
0 & 0 & 0
\end{bmatrix} + 
\frac{1}{2}
\begin{bmatrix}
0 & 0 & \lambda xq - \lambda yp \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}
\right)
 \bigg| \lambda \in \R \right\rbrace \\
&=& \left\lbrace \exp
\begin{bmatrix}
0 & \lambda x + p & \lambda (z+\frac{1}{2}(xq-yp))+r \\
0 & 0 & \lambda y +q \\
0 & 0 & 0
\end{bmatrix}
\bigg| \lambda \in \R \right\rbrace
\end{eqnarray*}
Now let $L \subseteq \g$ be the image of $\mathcal{C}$ under the logarithm map.

\begin{eqnarray*}
L &=& \left\lbrace
\begin{bmatrix}
0 & \lambda x + p & \lambda (z+\frac{1}{2}(xq-yp))+r \\
0 & 0 & \lambda y +q \\
0 & 0 & 0
\end{bmatrix}
\bigg| \lambda \in \R \right\rbrace \\
&=& \left\lbrace \lambda
\begin{bmatrix}
0 &  x & z+\frac{1}{2}(xq-yp) \\
0 & 0 & y \\
0 & 0 & 0
\end{bmatrix} +
\begin{bmatrix}
0 & p & r \\
0 & 0 & q \\
0 & 0 & 0
\end{bmatrix}
\bigg| \lambda \in \R \right\rbrace \\
&=& \{ \lambda (x,y,z+\frac{1}{2}(xq-yp))+P | \lambda \in \R \}
\end{eqnarray*}

So the one-dimensional coset $\mathcal{C}$ corresponds to a line $L$ in $\g$ which is a translate of a one-dimensional subalgebra, $\text{span}\{(x,y,z+\frac{1}{2}(xq-yp))\}$ by the element $P$. Note that this subalgebra is not the same as the subalgebra corresponding to the Lie subgroup in $\G$ from which the coset $\mathcal{C}$ was originally generated. 

A similar argument will now demonstrate that two-dimensional cosets in $\G$ do indeed correspond to vertical planes in $\g$.
Suppose $X=(x,y,z) \in \g$ is not vertical. So there is a vertical plane $\mathbf{\pi}$ given by 
\[
\mathbf{\pi} = \{\lambda X + \mu \3 \;\;\;|\;\;\; \lambda,\mu \in \R \}
\]
Then the corresponding Lie subgroup of $\mathbf{\pi}$ is 
\[
\Pi = \{\exp(\lambda X + \mu \3) \;\;\;|\;\;\; \lambda,\mu \in \R \}
\]
A coset of this Lie subgroup is $\mathcal{C}$ given by right translation by $e^{P}$ as before. Let So 
\begin{eqnarray*}
\mathcal{C} &=& \{\exp(\lambda X + \mu \3)\exp (P) \;\;\;|\;\;\; \lambda,\mu \in \R \} \\
&=& \{ \exp(\lambda x, \lambda y, \lambda z +\mu)\exp(p,q,r)\;\;\;|\;\;\; \lambda,\mu \in \R \} 
\end{eqnarray*}
Then, applying the CBH Formula:
\begin{eqnarray*}
\mathcal{C}&=&  \{ \exp((\lambda x, \lambda y, \lambda z +\mu)+(p,q,r)+\frac{1}{2}[(\lambda x, \lambda y, \lambda z +\mu),(p,q,r)])\;\;\;|\;\;\; \lambda,\mu \in \R \} \\
&=& \{ \exp ((\lambda x+p, \lambda y+q, \lambda z +\mu +r)+ \frac{1}{2}(0,0,\lambda xq -\lambda yp))\;\;\;|\;\;\; \lambda,\mu \in \R \} \\
&=& \{ \exp (\lambda x+p, \lambda y+q, \lambda (z+\frac{1}{2}(xq-yp)) +\mu +r)\;\;\;|\;\;\; \lambda,\mu \in \R \} \\
&=& \{ \exp [\lambda( x, y, z+\frac{1}{2}(xq-yp))+\mu(0,0,1)+(p,q,r)]\;\;\;|\;\;\; \lambda,\mu \in \R \}
\end{eqnarray*}

Now suppose that $\pi'$ is the image of this coset $\mathcal{C}$ under the logarithm. So 

\begin{eqnarray*}
\pi' &=& \{ \lambda( x, y, z+\frac{1}{2}(xq-yp))+\mu(0,0,1)+(p,q,r) \;\;\;|\;\;\; \lambda,\mu \in \R \} \\
&=& P+\text{span}\{Y,\3 \}
\end{eqnarray*}
where $Y=X+[X,P]=( x, y, z+\frac{1}{2}(xq-yp))$. \\
So $\pi'$ is yet another vertical plane in $\g$. Note however that unless $[X,P]=0$, $\pi'$ is not the same as $\pi$. Most importantly, if a bijective map from $\G$ to $\G$ preserves cosets, then the corresponding map from $\g$ to $\g$ preserves lines and vertical planes (translates of subalgebras).
\end{proof}

% may have to alter this to suit the real assumptions of the problem, ie that may need to get rid of translation bit in the group before moving to the algebra.
\subsection{Preserving Connected Cosets}
The cosets in $\g$ are either lines or vertical planes. Hence the bijective map $\phi$ sends lines to lines and vertical planes to vertical planes. Therefore by Proposition [?] $\phi$ is an affine map on the vector space $\g$, and because $\phi$ maps the identity to the identity, Proposition 2.2.4 indicates it must be linear.


%-use preservation of vertical planes to show that the linear portion of the %map takes on that slightly restricted form
Now, suppose $S$ and $T$ are distinct vertical planes through the origin in $\g$. So by bijectivity $\phi(S) \neq \phi(T)$ and $\phi(S \cup T)=\phi(S) \cup \phi(T)$. Hence 
\[
\phi(Z)=Z
\]
If $\phi$ is given by the matrix 
$\begin{bmatrix}
a & b & h \\
c & d & i \\
e & f & g
\end{bmatrix}
$, with $\det(\phi) \neq 0$ then 

\[
\begin{bmatrix}
a & b & h \\
c & d & i \\
e & f & g
\end{bmatrix} 
\begin{bmatrix}
0 \\ 0 \\ 1
\end{bmatrix} =
\begin{bmatrix}
h \\ i \\ g
\end{bmatrix} 
\in Z
\]
Hence $h=i=0$ and 
\[
\phi=
\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & g
\end{bmatrix}
\]

\end{proof}



% Now to the integer lattice:

\section{Restriction obtained from preservation of discrete cosets}
Without assuming that the map preserves discrete cosets of $G$, this restriction may be the strongest one that can be ascertained. However, by assuming the map $\Phi$ also sends cosets of discrete subgroups to cosets of discrete subgroups, stronger restrictions may be obtained. Note that discrete subgroups are not Lie groups. They are not locally homeomorphic to $\R^{n}$. 

\begin{Theorem}
Suppose that $\Psi: \G \longrightarrow \G$ is bijective and sends every coset of \emph{any} subgroup of $\G$ to some coset of some subgroup of $\G$. Then $\Psi=L \circ \Phi$ where $\Phi:\G \longrightarrow \G$ is either an isomorphism or an anti-isomorphism and $L: \G \longrightarrow \G$ is a translation. Specifically, for any $x \in \G$
\[
L(x)=\Psi(0)^{-1}x
\]

\end{Theorem}

Consider the elements $E_{1}=[1,0,0]$, $E_{2}=[0,1,0]$ and $E_{3}=[0,0,1]$ of $\G$. Their images under the logarithm map are ${\bf e}_{1}=(1,0,0)$, ${\bf e}_{2}=(0,1,0)$ and ${\bf e}_{3}=(0,0,1)$ in $\g$.\
Now with $\phi$ given in the matrix form above,
\begin{eqnarray*}
\phi {\bf e}_{1} &=& (a,c,e) \\
\phi {\bf e}_{2} &=& (b,d,f) \\
\phi {\bf e}_{3} &=& (0,0,g)
\end{eqnarray*}
Since ${\bf e}_{1}$ and ${\bf e}_{2}$ lie in different two-dimensional subalgebras of $\g$, their images under $\phi$ must lie in different two-dimensional subalgebras. [picture]This means that the projected vectors $(a,c)$ and $(b,d)$ are linearly independent in $\R^{2}$ so 
\begin{eqnarray*}
\frac{a}{b} &\neq & \frac{c}{d} \\
ad &\neq & bc\\
ad-bc &\neq & 0 
\end{eqnarray*} Let $\Delta = ad-bc$.
Now consider the linear map $\beta:\g \longrightarrow \g$ given in matrix form by 
\begin{eqnarray*}
\beta &=& \begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & \Delta
\end{bmatrix}^{-1} \\
&=& \frac{1}{\Delta}\begin{bmatrix}
d & -b & 0 \\
-c & a & 0 \\
(cf-ed)/\Delta & (eb-af)/\Delta & 1
\end{bmatrix}
\end{eqnarray*}

Now define a new map $\tau: \g \longrightarrow \g$ as the composition of $\phi$ and $\beta$. So 
\begin{eqnarray*}
\tau &=& \beta \circ  \phi \\
&=& \frac{1}{\Delta}\begin{bmatrix}
d & -b & 0 \\
-c & a & 0 \\
(cf-ed)/\Delta & (eb-af)/\Delta & 1
\end{bmatrix}
\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & g
\end{bmatrix} \\
&=& 
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & g/\Delta
\end{bmatrix}.
\end{eqnarray*}

Let the corresponding bijective maps, $B$ and $T$ from $\G$ to $\G$ be given by
\begin{eqnarray*}
B(e^{x}) &=& e^{\beta(x)} \\
T(e^{x}) &=& e^{\tau (x)} \\
& & \forall x \in \g
\end{eqnarray*}


\begin{Proposition}
If $g=\Delta$, then $\phi$ is a Lie algebra homomorphism.
\end{Proposition}

\begin{proof}
\[
\phi = 
\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & \Delta
\end{bmatrix}
\]
Let $x=(x_{1},x_{2},x_{3})$ and $y=(y_{1},y_{2},y_{3})$ be any two elements of $\g$. Then $[x,y]=(0,0,x_{1}y_{2}-x_{2}y_{1})$. 
\begin{eqnarray*}
[\phi(x),\phi(y)] &=& \left[
\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & \Delta
\end{bmatrix} 
\begin{bmatrix}
x_{1} \\ x_{2} \\ x_{3}
\end{bmatrix}
,
\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & \Delta
\end{bmatrix} 
\begin{bmatrix}
y_{1} \\ y_{2} \\ y_{3}
\end{bmatrix}
\right] \\
&=& \left[
\begin{bmatrix}
ax_{1}+bx_{2} \\ cx_{1}+dx_{2} \\ ex_{1}+fx_{2} + \Delta x_{3}
\end{bmatrix}
,
\begin{bmatrix}
ay_{1}+by_{2} \\ cy_{1}+dy_{2} \\ ey_{1}+fy_{2} + \Delta y_{3}
\end{bmatrix}
\right] \\
&=& (0,0,(ax_{1}+bx_{2})(cy_{1}+dy_{2})-(ay_{1}+by_{2})(cx_{1}+dx_{2})) \\
&=& (0,0,acx_{1}y_{1} - acx_{1}y_{1} + bdx_{2}y_{2} - bdx_{2}y_{2} + adx_{1}y_{2} + bcx_{2}y_{1} -adx_{2}y_{1} -bcx_{1}y_{2} ) \\
&=& (0,0, adx_{1}y_{2}  -adx_{2}y_{1}+ bcx_{2}y_{1} -bcx_{1}y_{2} ) \\
&=& (0,0,ad(x_{1}y_{2}  -x_{2}y_{1})+ bc(x_{2}y_{1} -x_{1}y_{2})) \\
&=& (0,0,(ad-bc)(x_{1}y_{2}  -x_{2}y_{1})) \\
&=& (0,0,\Delta(x_{1}y_{2}-x_{2}y_{1})) \\
&=& \phi(0,0,x_{1}y_{2}-x_{2}y_{1}) \\
&=& \phi[x,y] 
\end{eqnarray*}
\end{proof}

\begin{Proposition}
If $g=-\Delta$, then $\phi$ is a Lie algebra anti-homomorphism.
\end{Proposition}

\begin{proof}
\[
\phi = 
\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & -\Delta
\end{bmatrix}
\]
Let $x=(x_{1},x_{2},x_{3})$ and $y=(y_{1},y_{2},y_{3})$ be any two elements of $\g$. Then $[x,y]=(0,0,x_{1}y_{2}-x_{2}y_{1})$. 
\begin{eqnarray*}
[\phi(x),\phi(y)] &=& \left[
\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & -\Delta
\end{bmatrix} 
\begin{bmatrix}
x_{1} \\ x_{2} \\ x_{3}
\end{bmatrix}
,
\begin{bmatrix}
a & b & 0 \\
c & d & 0 \\
e & f & -\Delta
\end{bmatrix} 
\begin{bmatrix}
y_{1} \\ y_{2} \\ y_{3}
\end{bmatrix}
\right] \\
&=& \left[
\begin{bmatrix}
ax_{1}+bx_{2} \\ cx_{1}+dx_{2} \\ ex_{1}+fx_{2} - \Delta x_{3}
\end{bmatrix}
,
\begin{bmatrix}
ay_{1}+by_{2} \\ cy_{1}+dy_{2} \\ ey_{1}+fy_{2} - \Delta y_{3}
\end{bmatrix}
\right] \\
&=& (0,0,(ax_{1}+bx_{2})(cy_{1}+dy_{2})-(ay_{1}+by_{2})(cx_{1}+dx_{2})) \\
%&=& (0,0,acx_{1}y_{1} - acx_{1}y_{1} + bdx_{2}y_{2} - bdx_{2}y_{2} + adx_{1}y_{2} + bcx_{2}y_{1} -adx_{2}y_{1} -bcx_{1}y_{2} ) \\
&=& (0,0, adx_{1}y_{2}  -adx_{2}y_{1}+ bcx_{2}y_{1} -bcx_{1}y_{2} ) \\
%&=& (0,0,ad(x_{1}y_{2}  -x_{2}y_{1})+ bc(x_{2}y_{1} -x_{1}y_{2})) \\
&=& (0,0,(ad-bc)(x_{1}y_{2}  -x_{2}y_{1})) \\
&=& (0,0,\Delta(x_{1}y_{2}-x_{2}y_{1})) \\
&=& -\phi(0,0,x_{1}y_{2}-x_{2}y_{1}) \\
&=& -\phi[x,y] 
\end{eqnarray*}
\end{proof}

\begin{Proposition}
$\Phi$ is a Lie group homomorphism $\iff$ $\phi$ is a Lie algebra homomorphism, and $\Phi$ is a Lie group anti-homomorphism $\iff$ $\phi$ is a Lie algebra anti-homomorphism
\end{Proposition}

\begin{proof}
Suppose $\phi$ is a Lie algebra homomorphism. So $\phi [x,y]=[\phi(x),\phi(y)]$ for all $x$ and $y$ in $\g$. Suppose $X=\exp(x)$ and $Y=\exp(y)$. Then
\begin{eqnarray*}
\Phi(XY) &=& \Phi( \exp (x+y+\frac{1}{2}[x,y])) \\
&=& \exp( \phi(x+y+\frac{1}{2}[x,y])) \\
&=& \exp( \phi(x)+\phi(y) +\frac{1}{2} \phi[x,y])\;\;\; \text{(since $\phi$ is linear)} \\
&=& \exp ( \phi(x)+\phi(y) +\frac{1}{2} [\phi(x),\phi(y)]) \\
&=& \exp( \phi (x))\exp(\phi(y))\\
&=& \Phi(\exp(x))\Phi(\exp(y))\\
&=& \Phi(X)\Phi(Y)
\end{eqnarray*}

On the other hand, suppose $\phi$ is a Lie algebra anti-homomorphism. So $\phi [x,y]=-[\phi(x),\phi(y)]$. Then
\begin{eqnarray*}
\Phi(XY) &=& \Phi( \exp (x+y+\frac{1}{2}[x,y])) \\
&=& \exp( \phi(x+y+\frac{1}{2}[x,y])) \\
&=& \exp( \phi(x)+\phi(y) +\frac{1}{2} \phi[x,y])\;\;\; \text{(since $\phi$ is linear)} \\
&=& \exp ( \phi(x)+\phi(y) -\frac{1}{2} [\phi(x),\phi(y)]) \\
&=& \exp ( \phi(y)+\phi(x) +\frac{1}{2} [\phi(y),\phi(x)]) \\
&=& \exp( \phi (y))\exp(\phi(x))\\
&=& \Phi(\exp(y))\Phi(\exp(x))\\
&=& \Phi(Y)\Phi(X)
\end{eqnarray*}
\end{proof}

%Need to change order of the proof so that g=Delta -> isomorphism is done first. Then it can be shown that T maps subgroups to subgroups
\begin{Proposition}
$T$ maps subgroups of $\G$ to subgroups of $\G$.
\end{Proposition}
\begin{proof}
Since $\beta^{-1}$ is in the same form as $\phi$ but with $g=\Delta$, the previous two propositions show that $\beta^{-1}$ is a Lie algebra homomorphism. Since it is also bijective, it is a Lie algebra isomorphism. Therefore $\beta$ must also be a Lie algebra isomorphism. 

\begin{Lemma}
$B$ preserves subgroups. 
\end{Lemma}
\begin{proof}
Let $S$ be any subgroup of $\G$ and $\mathfrak{s}$ be its image under the logarithmic map. Let $Y_{1}$ and $Y_{2}$ be elements of $B(S)$. So there exist $X_{1}=\exp(x_{1})$ and $X_{2}=\exp(x_{2})$ in $S$ such that $Y_{1}=B(X_{1})$ and $Y_{2}=B(X_{2})$. Then 
\begin{align*}
Y_{1}Y_{2} &= B(X_{1})B(X_{2}) \\
&= B(X_{1}X_{2}) 
\end{align*}
which is in $B(S)$ because $S$ is a subgroup so $X_{1}X_{2} \in S$. Hence $B(S)$ is closed under the group operation. Moreover, $X_{1}^{-1} \in S$ so
\begin{align*}
Y_{1}^{-1} &=B(X_{1})^{-1} \\
&= B(X_{1}^{-1}) \\
&= \in B(S)
\end{align*}
Also,
$0_{\G} \in S$ so $B(0_{\G})=0_{\G} \in B(S)$. Hence $B(S)$ is a subgroup.

%&= B(\exp(x_{1}))B(\exp(x_{2})) \\
%&= \exp(\beta(x_{1}))\exp(\beta(x_{2}))\\
%&= \exp(\beta(x_{1})+\beta(x_{2})+\frac{1}{2}[\beta(x_{1}),\beta(x_{2})])\\
%&= \exp(\beta(x_{1})+\beta(x_{2})+\frac{1}{2}\beta[x_{1},x_{2}])\\
%&= \exp(\beta(x_{1}+x_{2}+\frac{1}{2}[x_{1},x_{2}]))\\
%&= B(\exp(x_{1}+x_{2}+\frac{1}{2}[x_{1},x_{2}]))\\
%&= B(\exp(x_{1})\exp(x_{2}))\\
\end{proof}

Now since $T$ is composed of $B$ and $\Phi$, which both preserve subgroups, $T$ must also preserve subgroups.
\end{proof}

\begin{Proposition}
If $\langle E_{1},E_{2} \rangle$ denotes the smallest subgroup of $\G$ containing the elements $E_{1}$ and $E_{2}$, then 
\[
\langle E_{1},E_{2} \rangle = 
\left\lbrace
\begin{bmatrix}
1 & x & z \\
0 & 1 & y \\
0 & 0 & 1
\end{bmatrix} \bigg|x,y,z \in \Z
\right\rbrace .
\]
Denote this set $\G_{\Z}$.
\end{Proposition}

\begin{proof}
Note that 
\begin{eqnarray*}
E_{3}=[0,0,1]&=&
\begin{bmatrix}
1 & 1 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 1 \\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
1 & -1 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & -1 \\
0 & 0 & 1
\end{bmatrix}
 \\
&=& E_{1}E_{2}E_{1}^{-1}E_{2}^{-1}
\end{eqnarray*}
Take an element from $\G_{\Z}$, $[x,y,z]$, where $x$, $y$ and $z$ are integers. Then 
\begin{eqnarray*}
[x,y,z] &=&[x,y,0][0,0,z-xy] \\
&=& [x,0,0][0,y,0][0,0,z-xy] \\
&=& E_{1}^{x}E_{2}^{y}E_{3}^{z-xy}
\end{eqnarray*}
So $\G_{\Z} \in \langle E_{1},E_{2} \rangle $. Conversely any element of $\langle E_{1},E_{2} \rangle$ contains only integer entries since adding and multiplying integers or taking their negatives produces only integers. So $\langle E_{1},E_{2} \rangle \in \G_{\Z}$. Hence $\langle E_{1},E_{2} \rangle=\G_{\Z}$.
\end{proof}

\begin{Lemma}
$T(\G_{\Z})=\G_{\Z}$. 
\end{Lemma}

\begin{proof}
Note the following:
\begin{eqnarray*}
T(E_{1})= e^{\tau ({\bf e}_{1})} &=& e^{{\bf e}_{1}} = E_{1} \\
T(E_{2})= e^{\tau ({\bf e}_{2})} &=& e^{{\bf e}_{2}} = E_{2} \\
T^{-1}(E_{1})= e^{\tau^{-1} ({\bf e}_{1})} &=& e^{{\bf e}_{1}} = E_{1} \\
T^{-1}(E_{2})= e^{\tau^{-1} ({\bf e}_{2})} &=& e^{{\bf e}_{2}} = E_{2} 
\end{eqnarray*}

Consider also the following fact. For $X$ and $Y$ in $\G$, $T( \langle X,Y \rangle )$ is a subgroup, which contains $T(X)$ and $T(Y)$ so by minimality, $\langle T(X),T(Y) \rangle \subseteq T( \langle X,Y \rangle )$
Hence
\begin{eqnarray*}
\langle E_{1},E_{2} \rangle = \langle T(E_{1}),T(E_{2}) \rangle &\subseteq & T( \langle E_{1},E_{2} \rangle) \\
\langle E_{1},E_{2} \rangle = \langle T^{-1}(E_{1}),T^{-1}(E_{2}) \rangle & \subseteq & T^{-1}( \langle E_{1},E_{2} \rangle) \\
\implies T( \langle E_{1},E_{2} \rangle) & \subseteq & T(T^{-1}( \langle E_{1},E_{2} \rangle))\\
&=&\langle E_{1},E_{2} \rangle
\end{eqnarray*}
and hence 
\[
T( \langle E_{1},E_{2} \rangle)= \langle E_{1},E_{2}\rangle
\]
giving the result.
\end{proof}

We now need to consider what happens to the unit vector $(0,0,1)={\bf e}_{3}$ in $\g$ by considering what happens to $[0,0,1]=E_{3}=\exp({\bf e}_{3})$ in the group.

\[
T(E_{3})= \exp(\tau ({\bf e}_{3}))=\exp(0,0,\frac{g}{\Delta})=[0,0,\frac{g}{\Delta}].
\]
Now $T$ maps the centre of $\G$, $Z$ to itself (as do $B$ and $\Phi$) since 
\[
T[0,0,z]= \exp(\tau(0,0,z))= \exp(0,0,\frac{gz}{\Delta})=[0,0,\frac{gz}{\Delta}] \in Z
.\] So combining this result with the previous Lemma,
\[
T( Z \cap \G_{\Z})=Z \cap \G_{\Z}
.\]
Hence 
\begin{eqnarray*}
T(E_{3}) &\in & Z \cap \G_{\Z} \\
\left[ 0,0,\frac{g}{\Delta} \right] &\in & Z \cap \G_{\Z} \\
\implies \frac{g}{\Delta} &\in & \Z
\end{eqnarray*}

Now we move into the Lie algebra to use the linearity properties of $\tau$.
Let 
\begin{eqnarray*}
D &=& \left\lbrace (0,0,m)\; \big| \;\; m \in \Z \right\rbrace \subseteq \g \\
\exp (D) &=& \left\lbrace [0,0,m] \; \big| \;\; m \in \Z \right\rbrace \subseteq \G \\
&=& Z \cap \G_{int}
\end{eqnarray*}

Now consider the image of this set of integer points on the $z$-axis under $\tau$:
\begin{eqnarray*}
\tau(D)&=& \log(\exp (\tau(D))) \\
&=& \log T(\exp (D)) \\
&=& \log T(Z \cap \G_{int}) \\
&=& \log (Z \cap \G_{int}) \\
&=& \log \exp (D) \\
&=& D
\end{eqnarray*}
So $\tau$ is a bijection when restricted to $D$. Linearity of $\tau$ gives the following for any integer $n$.
\begin{eqnarray*}
\tau (0,0,n) &=& n \tau (0,0,1) \\
&=& n\left(0,0, \frac{g}{\Delta}\right) \\
&=& \left(0,0, n \frac{g}{\Delta}\right)
\end{eqnarray*}
so $g/\Delta \in \Z$. But then considering the surjectivity of $\tau$ on $D$, ${\bf e}_{1}$ must be the image of some element of $D$ under $\tau$. In other words $(0,0,1)=\tau(0,0,n)$ for some $n \in \Z$ but from the previous result this gives 
\[
(0,0,1)=\left(0,0, n\left(\frac{g}{\Delta}\right)\right)
\]
where $n$ and $g/\Delta$ are both integers. Hence $n \left( \frac{g}{\Delta}\right)=1$ so 
\[
\frac{g}{\Delta}= \pm 1
\]
which gives
\[
g=\pm \Delta.
\]
Hence $\phi$ and $\Phi$ are either a pair of isomorphisms or anti-isomorphims and $\Psi$ is a composition of a translation and either an isomorphism or an anti-isomorphism.

\section{Summary}
The weaker assumption that $\Psi$ preserves cosets of Lie subgroups ensures $\phi$ is a linear map which sends the $z$-axis to itself. The stronger assumption that $\Psi$ preserves cosets of any subgroups ensures that $\Phi$ is either a Lie group isomorphism or a Lie group anti-isomorphism.\
The next chapter of this project will attempt to prove similar results for any upper-triangular nilpotent Lie group using induction. 






\chapter{Case of all Nilpotent Lie Groups}
The aim now is to extend the two theorems proved in the previous section by an inductive step on any $n$-dimensional nilpotent Lie group. It will be shown that a bijective map between two nilpotent Lie algebras of any degree, for which the corresponding mapping between Lie groups is coset-preserving, is either a Lie algebra isomorphism or anti-isomorphism. We will be considering all cosets to be preserved (including the discrete ones that are not Lie subgroups). 



\section{Theory of Nilpotent Lie Groups and Lie Algebras}
%include all the background information for the proof and general ideas about the basic structure of these groups and algebras



Let $\mathfrak{a}$ be a Lie algebra and let $S$ and $T$ be subspaces. Then a new subspace is defined by 
\[
[S,T]=\left\lbrace \sum\limits_{i} [s_{i},t_{i}] \mid s_{i} \in S,\;\; t_{i} \in T \right\rbrace
\]

\begin{proof}
Let $x=\sum\limits_{i=1}^{k} [s_{i},t_{i}]$ and $y=\sum\limits_{i=k+1}^{l} [s_{i},t_{i}]$ be elements of $[S,T]$, and let $\lambda$ be a real number. Then
\begin{align*}
\lambda x &=\sum\limits_{i=1}^{k} [\lambda s_{i},t_{i}]\\
&=\sum\limits_{i=1}^{k} [s'_{i},t_{i}]
\end{align*}
where $s'_{i}=\lambda s_{i} \in S$, since $S$ is a subspace. Hence $\lambda x \in [S,T]$.
\newline Now
\begin{align*}
x+y&=\sum\limits_{i=1}^{k} [s_{i},t_{i}]+\sum\limits_{i=k+1}^{l} [s_{i},t_{i}]\\
&= \sum\limits_{i=1}^{l} [s_{i},t_{i}] \\
&\in [S,T]
\end{align*}
Hence $[S,T]$ is a vector subspace of $\mathfrak{a}$.
\newline
%Also, 
%\begin{align*}
%[x,y]&= [\sum\limits_{i=1}^{k} [s_{i},t_{i}], \sum\limits_{i=k+1}^{l} [s_{i},t_{i}]]\\
%&=\sum\limits_{
%\begin{array}{c} 0 \leq i \leq k \\ k+1 \leq j \leq l \end{array}}
%[[s_{i},t_{i}],[s_{j},t_{j}]] \\
%&= -\sum\limits_{
%\begin{array}{c} 0 \leq i \leq k \\ k+1 \leq j \leq l \end{array}}
%+[s_{j},[t_{j},[s_{i},t_{i}]]]+[t_{j},[[s_{i},t_{i}],s_{j}]]\\
%&=\sum\limits_{
%\begin{array}{c} 0 \leq i \leq k \\ k+1 \leq j \leq l \end{array}}

%\end{align*}

\end{proof}

\begin{Definition}
The \emph{lower central series} $\{A_{i}\}$ of a Lie algebra $\mathfrak{a}$ is 
\begin{align*}
A_{0} &=\mathfrak{a} \\
A_{1} &=[\mathfrak{a},\mathfrak{a}] \\
A_{2} &=[\mathfrak{a},A_{1}] \\
&\vdots \\
A_{n} &=[\mathfrak{a},A_{n-1}]\;\;\;\text{for all} \;\;n \geq 3 
\end{align*}
\end{Definition}

\begin{Proposition}
Let $k$ be a positive integer. For any $k+1$ elements $x_{i}$ of $\mathfrak{a}$, 
\[
[x_{k},[x_{k-1},[\ldots [x_{1},x_{0}]\ldots ]] \in A_{k}.
\]
\end{Proposition}
\begin{proof}
For $k=0$, this is true since $x_{0} \in \mathfrak{a}=A_{0}$. \newline
Suppose that the statement is true for $k=j$. Now let $k=j+1$. Then let $x_{i}$ be any elements of $\mathfrak{a}$ for $0 \leq i \leq k$. So 
\begin{align*}
[x_{k},[x_{k-1},[\ldots [x_{1},x_{0}]\ldots ]] &= [x_{k},[x_{j},[x_{j-1},[\ldots [x_{1},x_{0}]\ldots ]]\\
&=[x_{k},y]
\end{align*}
where $y \in A_{j}=A_{k-1}$. Hence
\[
[x_{k},[x_{k-1},[\ldots [x_{1},x_{0}]\ldots ]] \in A_{k}.
\]
\end{proof}

\begin{Proposition}
The entries $A_{i}$ in the lower central series for a Lie algebra $\mathfrak{a}$ are nested in the following way:
\[
A_{0}\supset A_{1} \supset A_{2} \supset \cdots \supset A_{j} \supset A_{j+1} \supset \cdots
\]
\end{Proposition}
 
\begin{proof}
Prove that $x \in A_{k}$ whenever $x \in A_{k+1}$. For $k=0$ this is trivial since $A_{0}$ is the whole Lie algebra. Suppose this statement is true for all $k \leq N$ for some positive integer $N$. Now let $k=N+1$.
Let $x \in A_{k+1}$. So 
\[
x= \sum\limits_{i}[g_{i},h_{i}]
\]
where $g_{i} \in \mathfrak{a}$ and $h_{i} \in A_{k}$. But then $h_{i} \in A_{N+1}$ and by the inductive hypothesis, $h_{i} \in A_{N}=A_{k-1}$. Hence 
\[
[g_{i},h_{i}] \in A_{k}
\]
and therefore
\[
x=\sum\limits_{i}[g_{i},h_{i}] \in A_{k}.
\]
\end{proof}

\begin{Proposition}
If $\mathfrak{a}$ is a Lie algebra with lower central series $A_{i}$, then each of the $A_{i}$ is a Lie subalgebra. 
\end{Proposition}

\begin{proof}
The $A_{i}$ are vector subspaces so it suffices to prove that $A_{i}$ is closed under the Lie bracket. Let $x$ and $y$ be elements of $A_{i}$. Then $y \in A_{i-1}$ and $x \in \mathfrak{a}$ so $[x,y] \in A_{i}$. Hence $A_{i}$ is a Lie subalgebra of $\mathfrak{a}$.
\end{proof}

\begin{Definition}\label{nilp lie alg}

A Lie Algebra $\mathfrak{a}$ is called \emph{nilpotent} if its lower central series $A_{n}$ vanishes for some $n$. \newline
Note that there is a specific $n$ for which $A_{n-1} \neq 0$ but $A_{n} =0$.
\end{Definition}

The idea behind the proof is to look at the linear portion of the map on the Lie algebra, say $\phi$. We will take any two elements $x$ and $y$ of a nilpotent Lie algebra and see what kind of subalgebra they \emph{generate}. If they do not generate the whole algebra, then $\phi$ can be restricted to this generated subalgebra, and by an inductive assumption, will act as either an isomorphism or anti-isomorphism on this subalgebra. So $\phi[x,y]=\pm[\phi(x),\phi(y)]$ will hold. Some pairs of elements may however generate the whole algebra, in which case the inductive assumption cannot be directly applied. These pairs of elements will require special attention and it is this part of the proof that occupies the bulk of this chapter. 

\section{Basis of the Induction}
%could include this section in the previous chapter

\begin{Proposition}
Up to isomorphism, the only three-dimensional nilpotent Lie groups are the Heisenberg group, and $(\R^{3},+)$. (Three-dimensional Lie groups are those which correspond to three-dimensional algebras).
\end{Proposition}

\begin{proof}
Let $\mathfrak{a}$ be any three-dimensional nilpotent Lie algebra.
Start by considering how Lie algebras could be built by starting with a basic three-dimensional vector space. The algebras then differ only in the definitions of their bracket operation, and the corresponding Lie groups are given their structures by the BCH Formula:
\begin{align*}
e^{x}e^{y} &= \exp\big( x+y+\frac{1}{2}[x,y]+\frac{1}{12}[x,[x,y]]-\frac{1}{12}[y,[x,y]] \\
&-\frac{1}{24}[y,[x,[x,y]]]+\cdots (\text{longer commutators}) \big)
\end{align*}


\begin{Lemma}
Suppose $x$ and $y$ are linearly independent elements of $\mathfrak{a}$ and that $[x,y] \neq 0$. Then $\{x,y,[x,y]\}$ forms a basis for $\mathfrak{a}$.
\begin{proof}
For a contradiction suppose the opposite, i.e. suppose there exist real numbers $a$, $b$ and $c$, not all zero, such that $ax+by+c[x,y]=0$. Suppose that $a \neq 0$. Then by the property of nilpotency there is some positive integer $k$ with $A_{k}=0$. Hence by recursively applying $x=-a^{-1}(by+c[x,y])$ on the right hand side in the following relation, the last term is forced to be zero:
\begin{align*}
ax &= -by-c[x,y]\\
&=-by-c[-\frac{1}{a}(by+c[x,y]),y] \\
&= -by+\frac{c^{2}}{a}[[x,y],y] \\
&= -by+\frac{c^{2}}{a}[[-\frac{c}{a}[x,y],y],y] \\
&= -by+\frac{c^{3}}{a^{2}}[[[x,y],y],y] \\
& \vdots \\
&= -by+\frac{c^{k}}{a^{k-1}}[[\ldots[x,\overbrace{y]\ldots ,y],y]}^{k} \\
&= -by.
\end{align*}
But this implies that $x$ and $y$ are linearly dependent, contrary to the assumption.

Supposing instead that $b \neq 0$, the same recursive argument can be applied using $y$ instead of $x$, giving the same contradiction. \newline
If neither of these is the case, so $a=0$, $b=0$ and $c \neq 0$, then this gives $[x,y]=0$ which is also a contradiction.

 Therefore $\{x,y,[x,y]\}$ forms a basis for $\mathfrak{a}$.
\end{proof}
\end{Lemma}

\begin{Lemma}
All the terms with commutators of length three or more in the BCH Formula for two elements of a three-dimensional nilpotent Lie algebra $\mathfrak{a}$ are zero.

\begin{proof} 
Let $\mathfrak{a}$ have lower central series $A_{n}$. Since $\mathfrak{a}$ is nilpotent, there is some integer $k$ such that $A_{k}=0$.\newline
Let $x$ and $y$ be elements of $\mathfrak{a}$.
Higher terms in the Baker-Campbell-Hausdorff Formula all contain either $[x,[x,y]]$ or $[y,[x,y]]$, for $x$, $y \in \mathfrak{a}$. If $x$ and $y$ are linearly dependent then $[x,[x,y]]$ and $[y,[x,y]]$ are both zero and the result holds. So suppose that $x$ and $y$ are linearly independent elements of $\mathfrak{a}$. Then $\{x,y,[x,y]\}$ forms a basis for $\mathfrak{a}$ and so
\[
[x,[x,y]]=ax+by+c[x,y], \;\;\;\;\; \text{some $a$, $b$, $c \in \R$}
\]
By repeatedly taking commutators with $x$, we obtain
\[
[x,[x,[x,y]]]=b[x,y]+c[x,[x,y]]
\]
Further, let $D(n)=\underbrace{[x,[x,\ldots[x}_{n},y]] \ldots ]$, then for $n \geq 3$,
\begin{equation}\label{recurrence}
D(n)=bD(n-2)+cD(n-1).
\end{equation}

Since $\mathfrak{a}$ is nilpotent, there must exist some $k \geq 2$ such that $D(k)=0$ but $D(k-1) \neq 0$. If this $k=2$ the result holds. If $k \geq 3$ then from \ref{recurrence}, substituting $n=k+1$,
\[
bD(k-1)=0
.\] Hence $b=0$.
Then substituting $n=k$,
\[
bD(k-2)+cD(k-1)=0
.\] So
\[
cD(k-1)=0
\] and so
\[
c=0.
\]

This leaves us with 
\[
[x,[x,y]]=ax.
\]
If a=0 then the result holds. Suppose $a \neq 0$. Then $x=\frac{1}{a}[x,[x,y]]$. So

\begin{align*}
x &= \frac{1}{a}[x,[x,y]] \\
&= \frac{1}{a^{2}}[x,[[x,[x,y]],y]] \\
&=\frac{1}{a^{3}}[x,[[x,[[x,[x,y]],y]],y]] \\
&=\frac{1}{a^{n}}\underbrace{[x,[[x,[[x,}_{n}[x,\underbrace{y]],y]],y]]}_{n}\\
&=\frac{(-1)^{n}}{a^{n}}[[\ldots[x,\underbrace{y],x],y],x]\ldots,y],x]}_{2n\; \text{brackets}} .
\end{align*}
%Need to define precisely what the A_{n} are, ie the entries in the upper central series. Define at the beginning of the chapter. 
By continuing this argument it can be seen that $x \in A_{2n}$ for arbitrarily large $n$. Thus, making sure $n$ is sufficiently large, the nilpotent property of $\mathfrak{a}$ gives $x=0$, which is a contradiction. Hence $a=0$.

So $[x,[x,y]]=0$ and by symmetry $[y,[y,x]]=0$ so $[y,[x,y]=0$ as required.
\end{proof}

\end{Lemma}
There are two cases. Either there exist linearly independent elements $x$ and $y$ of $\mathfrak{a}$ such that $[x,y] \neq 0$, in which case $\{x,y,[x,y]$ is a basis for $\mathfrak{a}$ or not. In the first case, $\mathfrak{a}$ is isomorphic to $\g$ via the linear transformation $\varphi$, defined by:
\begin{align*}
\varphi(x)&=\begin{bmatrix}
0 & 1 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0 
\end{bmatrix}\\
\varphi(y)&=\begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 1 \\
0 & 0 & 0 
\end{bmatrix}\\
\varphi([x,y])&=\begin{bmatrix}
0 & 0 & 1 \\
0 & 0 & 0 \\
0 & 0 & 0 
\end{bmatrix}\\
\end{align*}

\begin{proof}
The map $\varphi$ is defined to be a linear transformation so it suffices to prove that it preserves the bracket operation.
Let $u$ and $v$ be elements of $\mathfrak{a}$. These can each be written as linear combinations of the basis elements $x$, $y$ and $[x,y]$. So suppose
\begin{align*}
u &=ax+by+c[x,y] \\
v &=a'x+b'y+c'[x,y].
\end{align*}
Then
\begin{align*}
\varphi[u,v] &= \varphi [ax+by+c[x,y],a'x+b'y+c'[x,y]] \\
&=\varphi(aa'[x,x]+ab'[x,y]+ac'[x,[x,y]]+ba'[y,x]+bb'[y,y]+bc'[y,[x,y]]+ca'[[x,y],x]+cb'[[x,y],y]+cc'[[x,y],[x,y]])\\
&=\varphi(ab'[x,y]+ba'[y,x]) \\
&=(ab'-a'b)\varphi[x,y] \\
&=(0,0,ab'-a'b).
\end{align*}
And,
\begin{align*}
[\varphi(u),\varphi(v)] &=[a\varphi(x)+b\varphi(y)+c\varphi[x,y],a'\varphi(x)+b'\varphi(y)+c'\varphi[x,y]]\\
&=[(a,b,c),(a',b',c')]\\
&=(0,0,ab'-ba')\\
&=\varphi[u,v] 
\end{align*}
%Could add subscripts to the bracket operations to distinguish between bracket in a and g!
Hence $\varphi$ is a Lie algebra homomorphism. Since $\varphi$ sends each of the basis vectors in $\mathfrak{a}$ to each of the basis vectors in $\g$, it is the identity transformation with respect to these bases and so is a bijection. Hence $\varphi$ is a Lie algebra isomorphism.
\end{proof}

In the second case, $\mathfrak{a}$ has no non-zero commutators and is isomorphic to $\R^{3}$. 
\begin{proof}
Let $\varphi$ be the identity transformation with respect to any two bases of $\mathfrak{a}$ and $\g$. So $\varphi$ is a bijective linear transformation and 
\[
\varphi[x,y]=0=[\varphi(x),\varphi(y)], \;\;\;\; \forall x,y \in \mathfrak{a}
\]
Hence $\varphi$ is a Lie algebra isomorphism.
\end{proof}
Hence there are only two three-dimensional nilpotent Lie algebras up to isomorphism: $\R^{3}$ and the Lie algebra of the Heisenberg group.
\end{proof}

\begin{Proposition}
A bijective map between $(\R^{3},+)$ and the Heisenberg Group cannot preserve the cosets.
This is obvious. Two-dimensional cosets in $\R^{3}$ are all the planes, whereas two-dimensional cosets in the Heisenberg group are only the vertical planes. So the set of two-dimensional cosets in $\R^{3}$ has greater cardinality than the set of two-dimensional cosets in the Heisenberg group and there cannot be a bijection between them.
\end{Proposition}

%explain now how this works as a basis for induction

Note that a bijective coset-preserving map on $\R^{3}$ is just a bijective affine map, so is composed of a linear map and a translation. This linear map is a bijection so is an isomorphism. Hence the theorem holds trivially when considering $(\R^{3},+)$ as a Lie group.
Therefore a bijective coset-preserving map between \emph{any} three-dimensional Lie groups is composed of a translation and either an isomorphism or anti-isomorphism.
%need to word this more carefully


When considering nilpotent Lie groups and Lie algebras of higher dimension, the essential structure can be studied mainly by looking at the bracket operation and the lower central series.

\begin{Proposition}\label{adddim}
Let $\mathfrak{a}$ be any Lie algebra and let $ A_{0},A_{1},A_{2}\ldots$ be its lower central series. Then if $x \in A_{m}$ and $y \in A_{n}$, $[x,y] \in A_{m+n+1}$.
\end{Proposition}

\begin{proof}
Proof is by induction on $m$, with arbitrary $n$. Suppose $x=\sum\limits_{i} U_{i}$ with $U_{i}=[x_{i1},[x_{i2},[x_{i3},\ldots [x_{im},x_{i0}]\ldots]]]$ and $y=\sum\limits_{i} V_{i}$, with $V_{i}=[y_{i1},[y_{i2},[y_{i3},\ldots [y_{im},y_{i0}]\ldots]]]$.
\newline
Suppose $m=0$, so $x$ is any element of $\mathfrak{a}$. Then $[x,y]=\sum\limits_{i}[x,V_{i}]=\Sigma_{i}W_{i}$, where $W_{i} \in A_{n+1}=A_{m+n+1}$ as required. The statement is true for $m=0$. \newline
Now suppose the statement is true for $m \leq k$, for some integer $k \geq 0$. Now suppose that $m=k+1$. So each $U_{i}$ can be written as $[x_{i1},T_{i}]$, where $T_{i} \in A_{k}$. So
\begin{align*}
[x,y] &=\sum\limits_{i}[[x_{i1},T_{i1}],y] \\
&= \sum\limits_{i}([x_{i1},[T_{i},y]]-[T_{i},[x_{i1},y]]).
\end{align*}

Now $T_{i}$ is in $A_{k}$ so $[T_{i},y]$ is in $A_{k+n+1}$ by the inductive hypothesis, and since $x_{i1}$ is any element of $\mathfrak{a}$, $[x_{i1},[T_{i},y]]$ must be in $A_{k+n+2}=A_{m+n+1}$. \newline
Also, $[x_{i1},y] \in A_{n+1}$ since $x_{i1}$ is any element of $\mathfrak{a}$, and then by the inductive hypothesis $[T_{i},[x_{i1},y]]$ must be in $A_{k+n+1+1}=A_{m+n+1}$. \newline
Hence $\sum\limits_{i}([x_{i1},[T_{i},y]]-[T_{i},[x_{i1},y]])$ is in $A_{m+n+1}$ as required. By induction the statement is true for all integers $m \geq 0$.
\end{proof}




\section{Reducing the Dimension of an Algebra}
For the rest of the thesis, $\g$ will denote a nilpotent Lie algebra of dimension $n$ with corresponding Lie group $\G$. Suppose the lower central series for $\g$ is $A_{i}$ and the last non-zero term is $A_{N}$. Let $\phi: \g \longrightarrow \g$ denote a linear map that preserves structures corresponding to cosets in the group. \newline
Let $x$ and $y$ be elements of $\g$. If $\alg(x,y)\subsetneq \g$ then by the inductive assumption $\phi|_{\alg(x,y)}$ is either an isomorphism or anti-isomorphism, so $\phi[x,y]= \pm [\phi(x),\phi(y)]$. \newline
Whereas if $\alg(x,y)=\g$, the inductive assumption cannot be directly applied. Therefore we attempt to reduce the dimension of the algebra $\alg(x,y)$ and look at the restrictions obtained by using the inductive assumption on the new algebra. 
\newline Suppose that $\g=\alg(x,y)$.

\begin{Proposition}
Suppose that the terms in the BCH Formula for $x$ and $y$ are labelled $C_{i}$. (So $x=C_{1}$, $y=C_{2}$, $[x,y]=C_{3}$ etc.), and that $C_{k}$ is the last non-zero term in the sequence. 
Then $\spn\{C_{1},\ldots,C_{k}\}=\alg(x,y)=\g $.

\end{Proposition}

\begin{proof}

Clearly $\{ C_{1},\ldots,C_{k} \}$ is a spanning set for $\g=\alg(x,y)$ since the $C_{j}$ are all the non-zero commutators involving $x$ and $y$, and by definition $\alg(x,y)$ is anything in the linear span of this set.

\end{proof}

Now that we have this spanning set for $\alg(x,y)$ it is possible to reduce its dimension by deleting particular elements. We will consider the subalgebras that arise by deleting $x$ ($C_{1}$) or $y$ ($C_{2}$). \newline
Let 
\[
\g_{-x}=\spn \{ C_{2},C_{3},\ldots,C_{k} \}
\]
and
\[
\g_{-y}=\spn \{C_{1},C_{3},C_{4},\ldots,C_{k} \}
\]

%do this lemma differently by looking at something is the span of all the Ci and commuting it with x or y to get something in g-x or g-y.
\begin{Lemma}\label{gohigher}
If $C_{p}$ and $C_{q}$ are distinct non-zero terms in the sequence $\{C_{i}\}$, then $[C_{p},C_{q}]= C_{p'}-C_{q'}$ where $C_{p'}$ and $C_{q'}$ are also terms in the sequence $\{C_{i}\}$ (they are possibly zero), for which  $p',q' > p$.
\end{Lemma}
\begin{proof}
The proof is by induction, very similar to the proof of \ref{adddim}. Clearly
\begin{align*}
[C_{1},C_{q}]&=[x,C_{q}]\\
&=C_{p'}
\end{align*}
for some $p'>p$. So the result holds if $p=1$. Now suppose the result holds for all distinct $p$ and $q$ with $p \leq K$ ($q$ can be anything distinct from $p$). Then suppose $r \leq K+1$ and that $r \neq q$. Then
\[
C_{r}=[\mu,C_{p}]
\]
for some $\mu \in \{x,y\}$ and some $C_{p}$ with $p < r$ (hence $p\leq K$). 
\newline
So then 
\begin{align*}
[C_{r},C_{q}]&=[[\mu,C_{p}],C_{q}]\\
&=[\mu,[C_{p},C_{q}]]-[C_{p},[\mu,C_{q}]] \\
&=[\mu,C_{p'}]-[C_{p},C_{s}]\\
&=C_{t_{1}}-C_{t_{2}}
\end{align*}
for some $t_{1}>p'>p$ and some $t_{2}>p$ as required. Hence by induction the statement is true for all positive integers $p$ and $q \neq p$.
\end{proof}


\begin{Proposition} The subspaces $\g_{-x}$ and $\g_{-y}$ are Lie subalgebras.
\end{Proposition} 
\begin{proof}
Let $u$ and $v$ be elements of $\g_{-x}$. Then there are real numbers $a_{2}\ldots a_{k}$ and $b_{2} \ldots b_{k}$ such that 
\begin{align*}
u &= a_{2}C_{2}+ a_{3}C_{3} + \ldots a_{k}C_{k} \\
v &= b_{2}C_{2}+ b_{3}C_{3} + \ldots b_{k}C_{k}
\end{align*}


\begin{align*}
[u,v]&=[a_{2}C_{2}+ a_{3}C_{3} + \ldots a_{k}C_{k},b_{2}C_{2}+ b_{3}C_{3} + \ldots b_{k}C_{k}] \\
&=a_{2}b_{3}[C_{2},C_{3}]+a_{2}b_{4}[C_{2},C_{4}]+\ldots a_{k}b_{k-1}[C_{k},C_{k-1}]\\
&=\gamma_{d(1)} C_{d(1)} +\gamma_{d(2)} C_{d(2)}+ \cdots \gamma_{d(P)} C_{d(P)}
\end{align*}
% give better explanation of where this expression comes from in terms of the new version of the previous lemma
Where $d(i) \geq 3$ for all $1 \leq i \leq P$, by \ref{gohigher}. Hence $[u,v] \in \spn \{ C_{2},C_{3}, \ldots C_{k} = \g_{-x}\}$ and so $\g_{-x}$ is closed under the bracket operation, making it a Lie subalgebra. By symmetry it can be seen that $\g_{-y}$ is also a Lie subalgebra.
\end{proof}

\begin{Proposition}
The subalgebras $\g_{-x}$ and $\g_{-y}$ are proper subalgebras of $\g$. 
\end{Proposition}

\begin{proof}

\end{proof}


\section{The Central Error Term}

\begin{Definition}
The \emph{centre} of a Lie algebra $\mathfrak{a}$ is 
\[
Z(\mathfrak{a})=\{x \in \mathfrak{a} \mid [x,a]=0, \;\; \forall a \in \mathfrak{a}\}.
\]
Note that the centre of the corresponding Lie group is the image of $Z(\mathfrak{a})$ under the exponential map.
\end{Definition}

\begin{Lemma}
If $\mathfrak{a}$ is a nilpotent Lie algebra, then it has a non-trivial centre. 
\end{Lemma}

\begin{proof}
Let $A_{i}$ be the lower central series for the nilpotent Lie algebra $\mathfrak{a}$, and suppose that $A_{n}$ is the last non-trivial term in the series. So there is some non-trivial element $x \in A_{n}$. But 
\[
[x,a] \in A_{n+1} \;\;\; \forall a \in \mathfrak{a} 
\]
Hence 
\[
[x,a] =0 \;\;\; \forall a \in \mathfrak{a}
\] which is the requirement for $x$ to be in the centre of $\mathfrak{a}$. Hence the centre in non-trivial.
\end{proof}

\begin{Proposition}\label{miracle}
Let $\phi:\g \longrightarrow \g $ be bijective and linear. Then 
\[
\phi[x,[x,y]]=[\phi(x),[\phi(x),\phi(y)]]
\]
\end{Proposition}

\begin{proof}
\end{proof}


We wish to show that $\phi[x,y]= \pm[\phi(x),\phi(y)]$. So we will define an element of $\g$ which is the difference between these two elements and aim to prove that it is zero.

\begin{Proposition}
 Let $z$ be the `error' term 
\[
z = \phi [x,y] -[\phi(x),\phi(y)].
\]
Then $z \in Z(\g)$.
\end{Proposition}

\begin{proof}
Let $a$ be any element of $\g$. Recall that $\g = \alg(x,y)$ and that a basis is $C_{i}$, the commutator terms in the BCH Formula for $x$ and $y$. We have 
\begin{align*}
x&=C_{1}\\
y&=C_{2}\\
[x,y]&=C_{3}
\end{align*}
and by the inductive assumption on $\g_{-x}$ and $\g_{-y}$, we know that 
\[
[\phi(C_{i}),\phi(C_{j})]=\phi[C_{i},C_{j}],\;\;\text{whenever}\;\; (i,j) \notin \{(1,2),(2,1)\}.
\]
Since $\phi$ is bijective, $a$ is the image of some element of $\g$ under $\phi$. So we can write 
\begin{align*}
a &=\phi(\sum\limits_{i=1}^{k}a_{i}C_{i})\\
&=\sum\limits_{i=1}^{k} a_{i}\phi(C_{i})
\end{align*}
for some real numbers $a_{i}$.\newline
So
\begin{align*}
[a,z] &= [a,\phi(C_{3})-[\phi(C_{1}),\phi(C_{2})]] \\
&=[a,\phi(C_{3})]-[a,[\phi(C_{1}),\phi(C_{2})]] \\
&=\left[\sum\limits_{i=1}^{k} a_{i}\phi(C_{i}),\phi(C_{3}) \right]-\left[ \sum\limits_{i=1}^{k} a_{i}\phi(C_{i}),[\phi(C_{1}),\phi(C_{2})]\right] \\
&=\sum\limits_{i=1}^{k} a_{i}[\phi(C_{i}),\phi(C_{3})]-\sum\limits_{i=1}^{k} a_{i}[\phi(C_{i}),[\phi(C_{1}),\phi(C_{2})]] \\
&=\sum\limits_{i=1}^{k} a_{i}[\phi(C_{i}),\phi(C_{3})]+ \sum\limits_{i=1}^{k} a_{i}\big(\; [\phi(C_{1}),[\phi(C_{2}),\phi(C_{i})]]+[\phi(C_{2}),[\phi(C_{i}),\phi(C_{1})]]\;\big) \\
&=\sum\limits_{i=1}^{k} a_{i}[\phi(C_{i}),\phi(C_{3})]+a_{1}[\phi(C_{1}),[\phi(C_{2}),\phi(C_{1})]]+a_{2}[\phi(C_{2}),[\phi(C_{2}),\phi(C_{1})]] \\
&\;\;\;+ \sum\limits_{i=3}^{k} a_{i}\big(\; [\phi(C_{1}),[\phi(C_{2}),\phi(C_{i})]]+[\phi(C_{2}),[\phi(C_{i}),\phi(C_{1})]] \;\big) \\
&=\sum\limits_{i=1}^{k} a_{i}\phi[C_{i},C_{3}]+a_{1}\phi[C_{1},[C_{2},C_{1}]]+a_{2}\phi[C_{2},[C_{2},C_{1}]] \\
&\;\;\;+ \sum\limits_{i=3}^{k} a_{i}\big(\; \phi[C_{1},[C_{2},C_{i}]]+\phi[C_{2},[C_{i},C_{1}]] \;\big) \\
& \;\;\;\;\;\; \text{(using Proposition \ref{miracle} and the inductive assumption) } \\
&=\sum\limits_{i=1}^{k} a_{i}\phi[C_{i},C_{3}]+ \sum\limits_{i=1}^{k} a_{i}\big(\; \phi[C_{1},[C_{2},C_{i}]]+\phi[C_{2},[C_{i},C_{1}]] \;\big) \\
&=\sum\limits_{i=1}^{k} \big( a_{i}\phi[C_{i},C_{3}]+ a_{i}\phi\big(\; [C_{1},[C_{2},C_{i}]]+[C_{2},[C_{i},C_{1}]] \;\big)\big) \\
&=\sum\limits_{i=1}^{k} a_{i}\big(\phi[C_{i},C_{3}]- \phi\big(\; [C_{i},[C_{1},C_{2}]] \;\big)\big) \\
&=\sum\limits_{i=1}^{k} a_{i}\big(\phi[C_{i},C_{3}]- \phi[C_{i},C_{3}] \big) \\
&=0
\end{align*}

\end{proof}

\section{Proof that the Error Term is Zero}



\bibliography{bibliography.bib}

\end{document}
